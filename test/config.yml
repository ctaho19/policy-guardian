pipeline:
  name: pl_automated_monitoring_CTRL_1077231
  description: Calculates Tier 1 and Tier 2 monitoring metrics for CTRL-1077231 (EC2 IMDSv2 enforcement).
  # Add other top-level pipeline settings from reference if applicable
  # use_test_data_on_nonprod: false 
  # dq_strict_mode: false 
  tags:
    - automated_monitoring
    - controls
    - ctrl_1077231
    - cloud

stages:
  # Define the Extract stage
  extract:
    threshold_df: # Name for the extracted DataFrame
      connector: snowflake # Use the standard snowflake connector
      options:
        # Reference the SQL file correctly using @text:
        sql: "@text:sql/monitoring_thresholds.sql"
        # Snowflake connection details should be implicitly handled by the framework
        # or defined centrally, not typically repeated here per extract.
        # Remove explicit connection block from here if framework handles it.

    # Add an extract step for the API call, if this is the standard pattern.
    # Alternatively, API calls are often done within the transform stage.
    # Based on the cloud_custodian example, API seems called later.
    # Let's assume API call happens in transform for now, like the original script.

  # Define Ingress Validation (Optional but recommended)
  # Add checks similar to the reference if needed for threshold_df
  ingress_validation:
    threshold_df:
      - type: count_check
        fatal: true
        options:
          threshold: 1 # Ensure at least one threshold row is loaded
      # Add schema check if dataset_id for thresholds is known
      # - type: schema_check
      #   fatal: true
      #   options:
      #     dataset_id: 'YOUR_THRESHOLD_DATASET_ID' # Replace with actual ID

  # Define the Transform stage
  transform:
    # Step 1: Perform the main transformation logic (API calls, filtering, calculation)
    # This replaces the two separate transform steps I had before.
    # The output of this transform should be the final DataFrame(s) ready for loading.
    monitoring_metrics_df: # Name of the final output DataFrame
      # Use a custom function defined in transform.py
      - function: custom/calculate_ctrl1077231_metrics # Assuming we rename/refactor the main function
        options:
          # Pass the necessary inputs from the extract stage
          threshold_df: $threshold_df
          # Pass any required context/secrets if needed (e.g., API credentials)
          # api_auth_token: "${secrets.cloud_tooling_api_token}" # Example

  # Define Consistency Checks (Optional)
  # Add checks comparing input/output counts/schemas if applicable
  # consistency_checks:
  #   - ingress: ...
  #     egress: monitoring_metrics_df
  #     type: ...

  # Define the Load stage
  load:
    monitoring_metrics_df: # Name of the DataFrame to load
      - connector: onestream # Use the standard onestream connector
        options:
          # Target table defined in the reference avro schema
          table_name: etip_controls_monitoring_metrics
          # Business application from reference avro schema
          business_application: BAENTERPRISETECHINSIGHTS
          file_type: AVRO
          # Reference the avro schema correctly using @json:
          avro_schema: "@json:avro_schema.json"
          # Add write mode if needed (e.g., append, overwrite)
          # write_mode: append

  # Define Test Data Preparation/Export (Optional)
  # prepare_test_data:
  #   threshold_df:
  #     - function: head
  #       options:
  #         n: 10
  # export_test_data:
  #   threshold_df:

connections:
  # Define the Snowflake connection used for thresholds and potentially the final load
  # Ensure this alias matches the connection details provided by the ETIP environment
  snowflake_connection:
    type: snowflake
    # Connection details (account, user, password/auth, role, warehouse, database, schema)
    # should be sourced securely from the ETIP environment/secrets, not hardcoded.
    # Example placeholders:
    account: "prod.us-east-1.capitalone.snowflakecomputing.com"
    user: "${secrets.etip_service_user}" # Example using secret management
    authenticator: "${secrets.etip_service_auth_type}"
    role: "ETIP_CONTROLS_MONITORING_ROLE"
    warehouse: "ETIP_CONTROLS_MONITORING_WH"
    database: "CYBR_DB_COLLAB" # Database for thresholds
    schema: "LAB_ESRA_TCRD" # Schema for thresholds

  # Define connection/authentication details for the Cloud Tooling API if needed separately
  # Often, secrets like API keys are accessed directly within transform functions

extracts:
  - name: extract_thresholds
    type: sql
    connection: snowflake_connection # Use the defined Snowflake connection
    # Fetch thresholds using the dedicated SQL file
    file_path: ./sql/monitoring_thresholds.sql
    # The result of this extract will be available as a DataFrame/object named 'extract_thresholds'

transforms:
  # Step 1: Fetch all relevant resources from the Cloud Tooling API
  - name: fetch_ec2_resources
    type: python
    # Call the function defined in transform.py
    function: transform.fetch_all_resources
    # Define the payload required by the API function
    kwargs:
      payload:
        searchParameters:
          - resourceType: AWS::EC2::Instance
      # Optionally pass timeout/retries if they need to differ from defaults in transform.py
      # timeout: 60
      # max_retries: 3
    # The result (List[Dict], int) will be available as 'fetch_ec2_resources'

  # Step 2: Filter resources, calculate metrics, and determine compliance
  - name: calculate_metrics
    type: python
    # Call the main processing function in transform.py
    function: transform.filter_resources_and_calculate_metrics
    # Pass the results from the previous steps as arguments
    kwargs:
      # The fetch_all_resources function returns a tuple (resources_list, count)
      # We need the list, which is the first element (index 0)
      resources:
        expr: "fetch_ec2_resources[0]" # Access the list of resources
      # Pass the DataFrame loaded from the thresholds extract step
      thresholds_df:
        expr: "extract_thresholds"
    # The result (monitoring_df, t1_non_compliant_df, t2_non_compliant_df)
    # will be available as 'calculate_metrics'

loads:
  - name: load_monitoring_metrics
    type: snowflake # Specify Snowflake loader type
    connection: snowflake_connection # Use the defined Snowflake connection
    input_name:
      expr: "calculate_metrics[0]" # Access the first element (monitoring_df) from calculate_metrics
    target_database: CYBR_DB_COLLAB # Database for the target monitoring table
    target_schema: LAB_ESRA_TCRD # Schema for the target monitoring table
    target_table: CYBER_CONTROLS_MONITORING
    target_write_mode: append # Append new metrics to the table
    avro_schema_path: ./avro_schema.json # Specify the schema file
    options:
      # Add any specific Snowflake write options if needed
      # e.g., privileges: "USAGE on schema LAB_ESRA_TCRD, INSERT on table CYBER_CONTROLS_MONITORING"
      # Ensure the service account has the necessary permissions

  # Optional: Load non-compliant resource details to a separate table/location if required
  # - name: load_tier1_non_compliant
  #   type: snowflake # or other type like s3_csv
  #   connection: snowflake_connection
  #   input_name:
  #     expr: "calculate_metrics[1]" # Access the t1_non_compliant_df
  #   target_database: ...
  #   target_schema: ...
  #   target_table: NON_COMPLIANT_CTRL_1077231_T1
  #   target_write_mode: overwrite # Or append

  # - name: load_tier2_non_compliant
  #   type: snowflake
  #   connection: snowflake_connection
  #   input_name:
  #     expr: "calculate_metrics[2]" # Access the t2_non_compliant_df
  #   target_database: ...
  #   target_schema: ...
  #   target_table: NON_COMPLIANT_CTRL_1077231_T2
  #   target_write_mode: overwrite # Or append 