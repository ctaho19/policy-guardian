from pyspark.sql.functions import current_date, lit, round, when
from pyspark.sql.types import StructType, StructField, DateType, StringType, DoubleType, LongType

# Get the KEYS_EVALUATED value
keys_evaluated = df_ozone_value.collect()[0][0]

# Create initial DataFrame with the numeric values
base_df = spark.createDataFrame([(keys_evaluated, level1_resource_count)], ["numerator", "denominator"])

# Create the final monitoring metric DataFrame with all calculations as column operations
monitoring_metric_df = base_df.select(
    current_date().alias("DATE"),
    lit("MNTR-1071824-T1").alias("MONITORING_METRIC_NUMBER"),
    round((col("numerator") * 100.0) / col("denominator"), 2).alias("MONITORING_METRIC"),
    when((col("numerator") * 100.0) / col("denominator") >= 98, "GREEN")
    .when((col("numerator") * 100.0) / col("denominator") >= 95, "YELLOW")
    .otherwise("RED").alias("COMPLIANCE_STATUS"),
    col("numerator").alias("NUMERATOR"),
    col("denominator").alias("DENOMINATOR")
)

# Display the result
display(monitoring_metric_df)




monitoring_metric_df.write \
    .format(SNOWFLAKE_SOURCE_NAME) \
    .options(**sfOptions) \
    .option("dbtable", "YOUR_TARGET_TABLE") \
    .mode("append") \
    .save()
