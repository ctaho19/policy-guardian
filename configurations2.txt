
"The main function orchestrates the pipeline by initializing a Spark session with SparkSession.builder.appName('MNTR-XXXXXXX').getOrCreate() to enable Snowflake queries, then sets up a resource_config dictionary with specifics like 'resource_type': 'AWS::KMS::Key', 'config_key': 'origin', and 'max_retries': 3 to control API behavior. It defines two payloads—summary_payload_all and summary_payload_filtered—using this config to query total and AWS_KMS-specific key counts via get_summary_count, which returns integers like total_summary_count and filtered_summary_count for validation. Next, it tests the pipeline with fetch_all_resources on a validation_limit of 1,000 keys, passing a config_payload with 'searchParameters': [{'resourceType': 'AWS::KMS::Key'}], and runs filter_tier1_resources and filter_tier2_resources to extract and count keys with non-empty origins and AWS_KMS origins, respectively, logging sample values for debugging. After validation, it fetches the full dataset with fetch_all_resources (no limit unless specified), applies both filters again to get tier1_numerator and tier2_numerator, and uses load_thresholds to pull alert and warning thresholds from Snowflake into a DataFrame. It then computes compliance metrics—tier1_metric as tier1_numerator / config_total_count and tier2_metric as tier2_numerator / tier1_numerator—and calls get_compliance_status with these against thresholds like 95% to assign ‘GREEN’ or ‘RED’. Finally, it constructs a monitoring_df DataFrame with these metrics and prints detailed reports of non-compliant keys from tier1_non_compliant_df and tier2_non_compliant_df, ensuring all steps are logged for traceability."
