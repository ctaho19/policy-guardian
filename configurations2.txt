from pyspark.sql.functions import current_date, lit, round
from pyspark.sql.types import StructType, StructField, DateType, StringType, DoubleType, LongType

# Assuming df_ozone_value is your DataFrame with KEYS_EVALUATED
# and level1_resource_count is your Python variable

# First, get the KEYS_EVALUATED value from the DataFrame
keys_evaluated = df_ozone_value.collect()[0][0]  # Gets the single value from the DataFrame

# Create the monitoring metric DataFrame
monitoring_metric_df = spark.createDataFrame([
    (
        current_date(),  # DATE
        "MNTR-1071824-T1",  # MONITORING_METRIC_NUMBER (adjust as needed)
        round((100.0 * keys_evaluated) / level1_resource_count, 2),  # MONITORING_METRIC
        # Adjust threshold conditions as needed
        "GREEN" if ((100.0 * keys_evaluated) / level1_resource_count) >= 98
        else "YELLOW" if ((100.0 * keys_evaluated) / level1_resource_count) >= 95
        else "RED",
        keys_evaluated,  # NUMERATOR
        level1_resource_count  # DENOMINATOR
    )
], 
    schema=StructType([
        StructField("DATE", DateType(), False),
        StructField("MONITORING_METRIC_NUMBER", StringType(), False),
        StructField("MONITORING_METRIC", DoubleType(), False),
        StructField("COMPLIANCE_STATUS", StringType(), False),
        StructField("NUMERATOR", LongType(), False),
        StructField("DENOMINATOR", LongType(), False)
    ])
)

# Display the result
display(monitoring_metric_df)




monitoring_metric_df.write \
    .format(SNOWFLAKE_SOURCE_NAME) \
    .options(**sfOptions) \
    .option("dbtable", "YOUR_TARGET_TABLE") \
    .mode("append") \
    .save()
