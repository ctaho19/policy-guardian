import logging
from pyspark.sql.functions import col, count
from pyspark.sql import SparkSession

# Configure logging
logging.basicConfig(level=logging.INFO, 
                    format='%(asctime)s - %(levelname)s - %(message)s',
                    filename='databricks_debug.log')  # Logs to a file; use console by removing filename

logger = logging.getLogger(__name__)

# Function to log DataFrame details safely, checking for emptiness
def log_dataframe_details_safely(df, name):
    logger.info(f"Inspecting DataFrame: {name}")
    logger.info(f"Schema: {df.schema}")
    try:
        # Use take(1) to check for rows without triggering count()
        sample_rows = df.take(1)  # Safely fetch up to 1 row
        is_empty = len(sample_rows) == 0
        logger.info(f"Is empty: {is_empty}")
        if not is_empty:
            logger.info(f"Sample row: {sample_rows}")
        else:
            logger.info("No rows found in DataFrame")
    except Exception as e:
        logger.error(f"Error fetching sample row: {str(e)}")
    logger.info(f"Is cached: {df.is_cached}")
    logger.info(f"Storage level: {df.storageLevel}")
    logger.info(f"Number of partitions: {df.rdd.getNumPartitions()}")

try:
    # Log initial state
    logger.info("Starting analysis of mapped roles DataFrames")

    # Log schemas and emptiness of input DataFrames before union
    logger.info("Inspecting mapped_roles_simplified:")
    log_dataframe_details_safely(mapped_roles_simplified, "mapped_roles_simplified")
    logger.info("Inspecting additional_mapped_roles:")
    log_dataframe_details_safely(additional_mapped_roles, "additional_mapped_roles")

    # Perform union and log immediately
    logger.info("Performing union of mapped roles DataFrames")
    all_mapped_roles = mapped_roles_simplified.union(additional_mapped_roles)
    log_dataframe_details_safely(all_mapped_roles, "all_mapped_roles")

    # Process further if not empty
    if len(all_mapped_roles.take(1)) > 0:  # Safe check for emptiness
        logger.info("Filtering and aggregating Machine roles")
        machine_roles_count = all_mapped_roles.filter(
            col("Role_Type") == "MACHINE"
        ).agg(
            count("*").alias("Machine_Roles_Count")
        )
        log_dataframe_details_safely(machine_roles_count, "machine_roles_count")

        # Test output with show (safer than collect)
        logger.info("Attempting to display machine_roles_count with show")
        machine_roles_count.show()
    else:
        logger.warning("all_mapped_roles is empty, skipping further processing")

    # Log Spark and Databricks runtime versions
    spark = SparkSession.builder.getOrCreate()
    logger.info(f"Spark Version: {spark.version}")
    logger.info(f"Databricks Runtime Version: {spark.conf.get('spark.databricks.clusterUsageTags.databricksRuntimeVersion', 'Unknown')}")

except Exception as e:
    logger.error(f"Unexpected error in pipeline: {str(e)}")
    raise
