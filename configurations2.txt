import logging
import json
from datetime import datetime
from typing import Dict, List, Tuple

import pandas as pd
import requests
from requests.adapters import HTTPAdapter
from requests.packages.urllib3.util.retry import Retry
from pyspark.sql import DataFrame, SparkSession
from pyspark.sql.functions import col, lit, when, regexp_replace
from pyspark.sql.types import StructType, StructField, StringType, DoubleType, LongType, TimestampType, ArrayType

logger = logging.getLogger(__name__)

CONTROL_CONFIGS = [
    {
        "cloud_control_id": "AC-3.AWS.39.v02",
        "ctrl_id": "CTRL-1074653",
        "metric_ids": {
            "tier1": "MNTR-1074653-T1",
            "tier2": "MNTR-1074653-T2",
            "tier3": "MNTR-1074653-T3"
        },
        "requires_tier3": True
    },
    {
        "cloud_control_id": "AC-6.AWS.13.v01",
        "ctrl_id": "CTRL-1105806",
        "metric_ids": {
            "tier1": "MNTR-1105806-T1",
            "tier2": "MNTR-1105806-T2"
        },
        "requires_tier3": False
    },
    {
        "cloud_control_id": "AC-6.AWS.35.v02",
        "ctrl_id": "CTRL-1077124",
        "metric_ids": {
            "tier1": "MNTR-1077124-T1",
            "tier2": "MNTR-1077124-T2"
        },
        "requires_tier3": False
    }
]

API_URL = "https://api.cloud.capitalone.com/internal-operations/cloud-service/aws-tooling/accounts"
API_HEADERS = {
    'X-Cloud-Accounts-Business-Application': 'BACyberProcessAutomation',
    'Authorization': 'Bearer eyJwY2siOjEsImFsZyI6ImRpciIsImVuYyI6IkExMjhDQkMtSFMyNTYiLCJraWQiOiJyNHEiLCJ0diI6Mn0..GlTvA5yFSD0i7cjWt3ZlIA.ejG2I8LPSefBMsPVcR0jTad7c1CCBpZdVF-7-UGuF1vMjLmdaHFJXFQwH4tbeTgznB2ffk606dW7FqqVtr5G_FH_ceQCC-oVUX4qqlJCaBOAGtxkIWu1U_nbnaqp21XVuaPzW8FbjZbT6PHUGC1LJu9rEMejcWypxsaRxEDAAMrNICIub3vmCJE-V1ADo3EDj2oSSIAY89l1Xgj-1xkDRprVH-2lZEMAaLOsvg67oyplLeMdQliwYv07yI-uDAhEdVqteIPTJFLGIZAb2SvbTeA3e2H_LPSUbBumIv3TA00xdwLjwKfHqDKxEOtyY1j0.gnhTfrRDNr3AIQ4hH-9Oww',
    'Accept': 'application/json;v=2.0',
    'Content-Type': 'application/json'
}
API_PARAMS = {
    'accountStatus': 'Active',
    'region': ['us-east-1', 'us-east-2', 'us-west-2', 'eu-west-1', 'eu-west-2', 'ca-central-1']
}

def get_approved_accounts(spark: SparkSession) -> DataFrame:
    """Fetch approved AWS accounts from API and create DataFrame."""
    retry_strategy = Retry(
        total=3,
        backoff_factor=1,
        status_forcelist=[500, 502, 503, 504]
    )
    http = requests.Session()
    http.mount("https://", HTTPAdapter(max_retries=retry_strategy))
    
    try:
        response = http.get(API_URL, headers=API_HEADERS, params=API_PARAMS, verify=False)
        response.raise_for_status()
        
        data = response.json()
        account_numbers = [acc['accountNumber'] for acc in data['accounts'] 
                         if acc.get('accountNumber') and acc['accountNumber'].strip()]
        
        if not account_numbers:
            raise ValueError("No valid account numbers received from API")
            
        return spark.createDataFrame([(acc,) for acc in account_numbers], ['ACCOUNT'])
        
    except requests.RequestException as e:
        logger.error(f"API call failed: {str(e)}", exc_info=True)
        raise
    except Exception as e:
        logger.error(f"Error processing approved accounts: {str(e)}", exc_info=True)
        raise

def calculate_compliance_status(metric_value: float, warning_threshold: float, alert_threshold: float) -> str:
    if metric_value < alert_threshold:
        return "RED"
    elif metric_value < warning_threshold:
        return "YELLOW"
    else:
        return "GREEN"

def get_sla_status(open_date: datetime, control_risk: str) -> Tuple[str, int]:
    """Calculate SLA status based on risk level and open date."""
    sla_thresholds = {
        "Critical": 0,
        "High": 30,
        "Medium": 60,
        "Low": 90
    }
    
    days_open = (datetime.now() - open_date).days
    sla_limit = sla_thresholds.get(control_risk, 90)
    
    if days_open > sla_limit:
        return "Past SLA", days_open
    return "Within SLA", days_open

def collect_non_compliant_resources(
    non_compliant_df: DataFrame,
    control_id: str,
    spark: SparkSession
) -> List[str]:
    """Collect details of non-compliant resources for evidence."""
    if non_compliant_df.isEmpty():
        return []
        
    # Get SLA data for non-compliant resources
    resource_ids = [row["RESOURCE_ID"] for row in non_compliant_df.collect()]
    resource_id_list = ",".join([f"'{rid}'" for rid in resource_ids])
    
    sla_query = f"""
    SELECT 
        RESOURCE_ID,
        CONTROL_RISK,
        OPEN_DATE_UTC_TIMESTAMP
    FROM CLCN_DB.PHDP_CLOUD.OZONE_NON_COMPLIANT_RESOURCES_TCRD_VIEW_V01
    WHERE CONTROL_ID = '{control_id}'
      AND RESOURCE_ID IN ({resource_id_list})
      AND ID NOT IN (SELECT ID FROM CLCN_DB.PHDP_CLOUD.OZONE_CLOSED_NON_COMPLIANT_RESOURCES_V04)
    """
    
    sla_data = spark.sql(sla_query).collect()
    evidence = []
    
    for row in non_compliant_df.collect():
        resource_id = row["RESOURCE_ID"]
        sla_info = next((s for s in sla_data if s["RESOURCE_ID"] == resource_id), None)
        
        if sla_info:
            status, days_open = get_sla_status(
                sla_info["OPEN_DATE_UTC_TIMESTAMP"],
                sla_info["CONTROL_RISK"]
            )
            evidence.append(json.dumps({
                "resource_id": resource_id,
                "resource_name": row["AMAZON_RESOURCE_NAME"],
                "account": row["ACCOUNT"],
                "ba": row["BA"],
                "control_risk": sla_info["CONTROL_RISK"],
                "open_date": sla_info["OPEN_DATE_UTC_TIMESTAMP"].isoformat(),
                "days_open": days_open,
                "sla_status": status
            }))
    
    return evidence

def calculate_metrics(
    thresholds_raw: DataFrame,
    iam_roles: DataFrame,
    evaluated_roles: DataFrame,
    spark: SparkSession
) -> DataFrame:
    logger.info("Starting metric calculations")
    
    # Get approved accounts
    approved_accounts = get_approved_accounts(spark)
    approved_accounts.createOrReplaceTempView("approved_accounts")
    
    # Filter IAM roles to only approved accounts
    iam_roles = iam_roles.join(
        approved_accounts,
        iam_roles.ACCOUNT == approved_accounts.ACCOUNT,
        "inner"
    )
    
    # Create schema matching avro_schema.json
    schema = StructType([
        StructField("date", TimestampType(), False),
        StructField("control_id", StringType(), False),
        StructField("monitoring_metric_id", StringType(), False),
        StructField("monitoring_metric_value", DoubleType(), False),
        StructField("compliance_status", StringType(), False),
        StructField("numerator", LongType(), False),
        StructField("denominator", LongType(), False),
        StructField("non_compliant_resources", ArrayType(StringType()), True)
    ])
    
    results = []
    
    for control in CONTROL_CONFIGS:
        logger.info(f"Processing control: {control['cloud_control_id']}")
        
        # Get thresholds for this control
        control_thresholds = thresholds_raw.filter(
            col("control_id") == control["ctrl_id"]
        ).collect()
        
        if not control_thresholds:
            logger.warning(f"No thresholds found for control {control['ctrl_id']}")
            continue
            
        # Process each tier
        for tier, metric_id in control["metric_ids"].items():
            logger.info(f"Processing {tier} for {control['ctrl_id']}")
            
            # Get threshold values for this metric
            threshold = next(
                (t for t in control_thresholds if t["monitoring_metric_id"] == metric_id),
                None
            )
            
            if not threshold:
                logger.warning(f"No threshold found for metric {metric_id}")
                continue
                
            # Calculate metrics based on tier
            if tier == "tier1":
                # Tier 1: Coverage metric (% of roles evaluated)
                total_roles = iam_roles.count()
                evaluated_count = evaluated_roles.filter(
                    col("control_id") == control["cloud_control_id"]
                ).count()
                
                metric_value = (evaluated_count / total_roles * 100) if total_roles > 0 else 0
                
            elif tier == "tier2":
                # Tier 2: Compliance metric (% of evaluated roles that are compliant)
                evaluated_for_control = evaluated_roles.filter(
                    col("control_id") == control["cloud_control_id"]
                )
                total_evaluated = evaluated_for_control.count()
                
                if total_evaluated == 0:
                    metric_value = 0
                else:
                    compliant_count = evaluated_for_control.filter(
                        col("compliance_status") == "Compliant"
                    ).count()
                    metric_value = (compliant_count / total_evaluated * 100)
                    
            elif tier == "tier3":
                # Tier 3: SLA compliance metric (% of non-compliant roles within SLA)
                if not control["requires_tier3"]:
                    continue
                    
                non_compliant = evaluated_roles.filter(
                    (col("control_id") == control["cloud_control_id"]) &
                    (col("compliance_status") == "NonCompliant")
                )
                total_non_compliant = non_compliant.count()
                
                if total_non_compliant == 0:
                    metric_value = 100
                    within_sla_count = 0
                else:
                    # Get non-compliant resources with their details
                    non_compliant_details = iam_roles.join(
                        non_compliant,
                        iam_roles.AMAZON_RESOURCE_NAME == non_compliant.RESOURCE_NAME,
                        "inner"
                    )
                    
                    # Collect evidence for non-compliant resources
                    evidence = collect_non_compliant_resources(
                        non_compliant_details,
                        control["cloud_control_id"],
                        spark
                    )
                    
                    # Count resources within SLA
                    within_sla_count = len([e for e in evidence if "Within SLA" in e])
                    metric_value = (within_sla_count / total_non_compliant * 100)
            
            # Calculate compliance status
            status = calculate_compliance_status(
                metric_value,
                float(threshold["warning_threshold"]),
                float(threshold["alerting_threshold"])
            )
            
            # Add result
            results.append({
                "date": datetime.now(),
                "control_id": control["ctrl_id"],
                "monitoring_metric_id": metric_id,
                "monitoring_metric_value": metric_value,
                "compliance_status": status,
                "numerator": evaluated_count if tier == "tier1" else compliant_count if tier == "tier2" else within_sla_count,
                "denominator": total_roles if tier == "tier1" else total_evaluated if tier == "tier2" else total_non_compliant,
                "non_compliant_resources": evidence if tier == "tier3" else None
            })
    
    # Create final DataFrame
    return spark.createDataFrame(results, schema)

def calculate_machine_iam_metrics(
    thresholds_raw: DataFrame,
    iam_roles: DataFrame,
    evaluated_roles: DataFrame,
    spark: SparkSession
) -> DataFrame:
    logger.info("Starting machine IAM metrics calculation")
    return calculate_metrics(thresholds_raw, iam_roles, evaluated_roles, spark) 
