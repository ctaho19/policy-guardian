try:
    # Get approved accounts
    df_approved_accounts = get_approved_accounts()

    # Step 1: Load all IAM roles with their full information
    log_step("Loading IAM roles")
    df_all_iam_roles = spark.read.format(SNOWFLAKE_SOURCE_NAME) \
        .options(**sfOptions) \
        .option("query", all_iam_roles_query) \
        .load()
    
    # Validate account numbers and basic role information
    df_all_iam_roles = df_all_iam_roles.filter(
        (col("ACCOUNT").isNotNull()) &  # Check for null
        (col("ACCOUNT") != "") &        # Check for empty string
        (col("ACCOUNT").rlike("^[0-9]+$"))  # Check for only digits using regex pattern
    )
    
    validate_dataframe(
        df_all_iam_roles,
        expected_cols=['RESOURCE_ID', 'AMAZON_RESOURCE_NAME', 'ACCOUNT'],
        min_count=1
    )
    log_step("IAM roles loaded", f"Total Valid IAM Roles: {df_all_iam_roles.count()}")

    # Step 2: Load violation roles mapping
    log_step("Loading violation roles mapping")
    df_violation_roles = spark.read.format(SNOWFLAKE_SOURCE_NAME) \
        .options(**sfOptions) \
        .option("query", violation_roles_query) \
        .load()
    
    validate_dataframe(
        df_violation_roles,
        expected_cols=['RESOURCE_NAME', 'ROLE_TYPE'],
        min_count=1
    )
    log_step("Violation roles loaded", f"Total Violation Roles: {df_violation_roles.count()}")

    # Step 3: Load evaluated roles
    log_step("Loading evaluated roles")
    df_evaluated_roles = spark.read.format(SNOWFLAKE_SOURCE_NAME) \
        .options(**sfOptions) \
        .option("query", evaluated_roles_query) \
        .load()
    
    validate_dataframe(
        df_evaluated_roles,
        expected_cols=['RESOURCE_NAME'],
        min_count=1
    )
    log_step("Evaluated roles loaded", f"Total Evaluated Roles: {df_evaluated_roles.count()}")

    # Step 4: Create initial mapping with full role information
    log_step("Creating initial role mapping")
    df_initial_mapping = df_all_iam_roles.alias("iam") \
        .join(
            df_violation_roles.alias("viol"),
            df_all_iam_roles.AMAZON_RESOURCE_NAME == df_violation_roles.RESOURCE_NAME,
            "left"
        ) \
        .join(
            df_evaluated_roles.alias("eval"),
            df_all_iam_roles.AMAZON_RESOURCE_NAME == df_evaluated_roles.RESOURCE_NAME,
            "left"
        ) \
        .select(
            col("iam.RESOURCE_ID"),
            col("iam.AMAZON_RESOURCE_NAME"),
            col("iam.BA"),
            col("iam.ACCOUNT"),
            col("iam.CREATE_DATE"),
            col("iam.LOAD_TIMESTAMP"),
            coalesce(col("viol.ROLE_TYPE"), lit("NOT FOUND")).alias("ROLE_TYPE"),
            when(col("eval.RESOURCE_NAME").isNotNull(), 1).otherwise(0).alias("IS_EVALUATED"),
            lit("INITIAL_MAPPING").alias("MAPPING_SOURCE")
        ) \
        .dropDuplicates(["RESOURCE_ID"])  # Ensure no duplicates after joins
    
    log_step("Initial mapping complete", f"Initially Mapped Roles: {df_initial_mapping.count()}")

    # Step 5: Find unmapped resources
    log_step("Finding unmapped resources")
    df_unmapped = df_all_iam_roles.alias("iam") \
        .join(
            df_initial_mapping.alias("init"),
            col("iam.RESOURCE_ID") == col("init.RESOURCE_ID"),
            "left_anti"
        )
    log_step("Unmapped resources identified", f"Unmapped Roles: {df_unmapped.count()}")

    # Step 6: Check unmapped resources in violations dataset
    log_step("Validating unmapped resources")
    df_unmapped_validation = df_unmapped.alias("unmap") \
        .join(
            df_violation_roles.alias("viol"),
            col("unmap.AMAZON_RESOURCE_NAME") == col("viol.RESOURCE_NAME"),
            "left"
        ) \
        .select(
            col("unmap.RESOURCE_ID"),
            col("unmap.AMAZON_RESOURCE_NAME"),
            col("unmap.BA"),
            col("unmap.ACCOUNT"),
            col("unmap.CREATE_DATE"),
            col("unmap.LOAD_TIMESTAMP"),
            coalesce(col("viol.ROLE_TYPE"), lit("NOT FOUND")).alias("ROLE_TYPE"),
            lit(0).alias("IS_EVALUATED"),
            lit("VALIDATION_MAPPING").alias("MAPPING_SOURCE")
        )
    log_step("Unmapped validation complete", 
             f"Validated Unmapped Roles: {df_unmapped_validation.count()}")

    # Step 7: Combine all roles and deduplicate
    log_step("Combining and deduplicating roles")
    df_all_roles = df_initial_mapping.union(df_unmapped_validation) \
        .dropDuplicates(["RESOURCE_ID"]) \
        .orderBy(col("LOAD_TIMESTAMP").desc())

    # Step 8: Filter for machine roles only
    log_step("Filtering for machine roles")
    df_machine_roles = df_all_roles.filter(col("ROLE_TYPE") == "MACHINE")
    log_step("Machine roles filtered", f"Total Machine Roles Found: {df_machine_roles.count()}")

    # Step 9: Join with approved accounts
    log_step("Filtering for approved accounts")
    df_filtered = df_machine_roles.join(
        df_approved_accounts,
        upper(df_machine_roles.ACCOUNT) == upper(df_approved_accounts.accountNumber),
        "inner"
    )
    log_step("Account filtering complete", 
             f"Machine Roles in Approved Accounts: {df_filtered.count()}")

    # Cache the filtered results for metrics calculation
    df_filtered.cache()

except Exception as e:
    log_step("ERROR in data processing", f"Pipeline execution failed during data processing: {str(e)}")
    raise
