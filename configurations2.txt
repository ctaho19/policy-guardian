import os
import time
import json
import pandas as pd
import requests
import snowflake.connector
from datetime import datetime
from typing import Dict, List, Set, Tuple, Optional, Any

# API details
API_URL = "https://api.cloud.capitalone.com/internal-operations/cloud-service/aws-tooling/search-resource-configurations"
API_HEADERS = {
    'Accept': 'application/json;v=1.0',
    'Authorization': 'Bearer YOUR_API_TOKEN',  # Replace with actual token
    'Content-Type': 'application/json'
}
API_CALL_DELAY = 0.15  # Delay between API calls to avoid rate limits
API_TIMEOUT = 60  # Timeout for API calls

# Snowflake connection parameters using dbutils.secrets
# Connecting to snowflake, necessary for querying metric thresholds
try:
    # Try to use databricks secrets if running in Databricks
    import dbutils
    username = dbutils.secrets.get(scope="sdd802_scope", key="eid") #NOTE: Using Aditya's scope
    password = dbutils.secrets.get(scope="sdd802_scope", key="password")
except:
    # Fall back to environment variables if not in Databricks
    username = os.environ.get("SNOWFLAKE_USER", "YOUR_USERNAME")
    password = os.environ.get("SNOWFLAKE_PASSWORD", "YOUR_PASSWORD")

# Define Snowflake connection options
SNOWFLAKE_SOURCE_NAME = "net.snowflake.spark.snowflake"
SNOWFLAKE_USER = username
SNOWFLAKE_PASSWORD = password
SNOWFLAKE_ACCOUNT = "cptlone-sfprod"
SNOWFLAKE_DATABASE = "SB"
SNOWFLAKE_SCHEMA = "USER_SDD802"  # Using Aditya's credentials
SNOWFLAKE_WAREHOUSE = "CYBR_Q_DI_WH"

# Define Snowflake options dict for PySpark
sfOptions = dict(
    sfUrl= SNOWFLAKE_ACCOUNT, #REGION-SPECIFIC CONNECTION: prod.us-east-1.capitalone.snowflakecomputing.com",
    sfUser=username,
    sfPassword=password,
    sfDatabase=SNOWFLAKE_DATABASE,
    sfSchema=SNOWFLAKE_SCHEMA,
    sfWarehouse=SNOWFLAKE_WAREHOUSE
)

# Configure logging
import logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def get_snowflake_connection():
    """Establish a connection to Snowflake."""
    try:
        conn = snowflake.connector.connect(
            user=SNOWFLAKE_USER,
            password=SNOWFLAKE_PASSWORD,
            account=SNOWFLAKE_ACCOUNT,
            warehouse=SNOWFLAKE_WAREHOUSE,
            database=SNOWFLAKE_DATABASE,
            schema=SNOWFLAKE_SCHEMA
        )
        logger.info(f"Successfully connected to Snowflake: {SNOWFLAKE_DATABASE}.{SNOWFLAKE_SCHEMA}")
        return conn
    except Exception as e:
        logger.error(f"Failed to connect to Snowflake: {e}")
        raise

def fetch_certificates_from_snowflake() -> pd.DataFrame:
    """Fetch certificates from Snowflake based on the provided query."""
    conn = get_snowflake_connection()
    try:
        query = """
        SELECT DISTINCT certificate_arn, certificate_id, nominal_issuer, 
               not_valid_before_utc_timestamp, not_valid_after_utc_timestamp,
               last_usage_observation_utc_timestamp
        FROM CYBR_DB.PHDP_CYBR.certificate_catalog_certificate_usage 
        WHERE certificate_arn LIKE '%arn:aws:acm%' 
          AND nominal_issuer = 'Amazon' 
          AND not_valid_after_utc_timestamp >= DATEADD('DAY', -365, CURRENT_TIMESTAMP)
        """
        logger.info("Fetching certificates from Snowflake...")
        df = pd.read_sql(query, conn)
        logger.info(f"Retrieved {len(df)} certificates from Snowflake")
        return df
    except Exception as e:
        logger.error(f"Error fetching certificates from Snowflake: {e}")
        raise
    finally:
        conn.close()

def fetch_certificates_from_api(payload: Dict) -> List[Dict]:
    """Fetch certificates from the AWS Tooling API with pagination."""
    all_certificates = []
    next_record_key = None
    page_count = 0
    
    logger.info(f"Fetching certificates from API with payload: {json.dumps(payload, indent=2)}")
    
    while True:
        page_count += 1
        current_url = API_URL
        params = {'limit': 10000}
        
        if next_record_key:
            params['nextRecordKey'] = next_record_key
        
        try:
            logger.info(f"Fetching page {page_count} from API...")
            response = requests.post(
                current_url, 
                headers=API_HEADERS, 
                json=payload,
                params=params,
                timeout=API_TIMEOUT, 
                verify=False
            )
            response.raise_for_status()
            data = response.json()
            
            certificates = data.get('resourceConfigurations', [])
            logger.info(f"Retrieved {len(certificates)} certificates from page {page_count}")
            all_certificates.extend(certificates)
            
            next_record_key = data.get('nextRecordKey')
            if not next_record_key:
                logger.info("No more pages to fetch")
                break
                
            # Sleep to avoid rate limits
            time.sleep(API_CALL_DELAY)
            
        except requests.exceptions.RequestException as e:
            logger.error(f"API request failed on page {page_count}: {e}")
            # Continue with certificates fetched so far
            break
    
    logger.info(f"Total certificates fetched from API: {len(all_certificates)}")
    return all_certificates

def extract_certificate_data(certificates: List[Dict]) -> pd.DataFrame:
    """Extract relevant data from the certificate configurations."""
    extracted_data = []
    
    for cert in certificates:
        cert_data = {
            'amazonResourceName': cert.get('amazonResourceName', ''),
            'resourceId': cert.get('resourceId', ''),
            'awsAccountId': cert.get('awsAccountId', ''),
            'awsRegion': cert.get('awsRegion', ''),
            'businessApplicationName': cert.get('businessApplicationName', ''),
            'environment': cert.get('environment', ''),
            'source': cert.get('source', '')
        }
        
        # Extract values from configuration lists
        config_list = cert.get('configurationList', [])
        for config in config_list:
            name = config.get('configurationName', '')
            value = config.get('configurationValue', '')
            # Create a flattened column for each configuration
            cert_data[name] = value
            
        # Extract values from supplementary configuration lists
        supp_config = cert.get('supplementaryConfiguration', [])
        for config in supp_config:
            name = config.get('supplementaryConfigurationName', '')
            value = config.get('supplementaryConfigurationValue', '')
            # Create a flattened column for each supplementary configuration
            cert_data[name] = value
            
        extracted_data.append(cert_data)
    
    # Convert to DataFrame
    df = pd.DataFrame(extracted_data)
    return df

def identify_common_attributes(df_snowflake: pd.DataFrame, df_api: pd.DataFrame) -> Dict:
    """Identify common attributes between dataset and API certificates."""
    # Extract just the ARNs for comparison
    snowflake_arns = set(df_snowflake['certificate_arn'].str.lower())
    api_arns = set(df_api['amazonResourceName'].str.lower())
    
    # Find matching and non-matching ARNs
    matching_arns = snowflake_arns.intersection(api_arns)
    api_only_arns = api_arns - matching_arns
    snowflake_only_arns = snowflake_arns - matching_arns
    
    logger.info(f"Matching ARNs: {len(matching_arns)}")
    logger.info(f"API-only ARNs: {len(api_only_arns)}")
    logger.info(f"Snowflake-only ARNs: {len(snowflake_only_arns)}")
    
    # Filter DataFrames to get only matching and non-matching records
    df_api_matching = df_api[df_api['amazonResourceName'].str.lower().isin(matching_arns)]
    df_api_non_matching = df_api[df_api['amazonResourceName'].str.lower().isin(api_only_arns)]
    
    # Analyze configurations that might differentiate between matching and non-matching
    differences = {}
    common_configurations = {}
    
    # Only look at columns that have more than 5% non-null values
    relevant_columns = [col for col in df_api.columns if df_api[col].notna().mean() > 0.05]
    
    for col in relevant_columns:
        if col in ['amazonResourceName', 'resourceId']:  # Skip identifier columns
            continue
            
        # Get unique values for matching certificates
        if len(df_api_matching) > 0:
            matching_values = set(df_api_matching[col].dropna().unique())
        else:
            matching_values = set()
            
        # Get unique values for non-matching certificates
        if len(df_api_non_matching) > 0:
            non_matching_values = set(df_api_non_matching[col].dropna().unique())
        else:
            non_matching_values = set()
            
        # Check if there are differences in values
        if matching_values and (matching_values != non_matching_values):
            # Find values that are common in the matching set but rare in the non-matching set
            distinctive_values = [v for v in matching_values if v and 
                                 ((v not in non_matching_values) or 
                                  (df_api_matching[col] == v).mean() > (df_api_non_matching[col] == v).mean() * 3)]
            
            if distinctive_values:
                differences[col] = {
                    "matching_values": list(matching_values),
                    "non_matching_values": list(non_matching_values),
                    "distinctive_values": distinctive_values
                }
                
                # Look for common values in the matching set
                common_value = None
                if len(set(df_api_matching[col].dropna())) == 1:
                    common_value = df_api_matching[col].dropna().iloc[0]
                elif df_api_matching[col].dropna().value_counts().index[0] == df_api_matching[col].dropna().value_counts().index[0]:
                    # If most common value appears in >80% of matching records
                    most_common = df_api_matching[col].value_counts(normalize=True).index[0]
                    if df_api_matching[col].value_counts(normalize=True).iloc[0] > 0.8:
                        common_value = most_common
                
                if common_value:
                    common_configurations[col] = common_value
    
    return {
        "matching_count": len(matching_arns),
        "api_only_count": len(api_only_arns),
        "snowflake_only_count": len(snowflake_only_arns),
        "differences": differences,
        "common_configurations": common_configurations
    }

def suggest_api_payload(analysis_results: Dict) -> Dict:
    """Suggest an improved API payload based on analysis results."""
    # Start with the basic payload
    suggested_payload = {
        "searchParameters": [
            {
                "resourceType": "AWS::ACM::Certificate",
                "configurationItems": [
                    {
                        "configurationName": "issuer",
                        "configurationValue": "Amazon"
                    }
                ]
            }
        ]
    }
    
    # Add additional configurations that seem to be common across dataset certificates
    common_configs = analysis_results.get("common_configurations", {})
    for config_name, config_value in common_configs.items():
        # Skip if already present
        if config_name in ['issuer']:
            continue
            
        # Check if it's a configuration item or a top-level attribute
        if config_name.startswith("configuration."):
            # Add as a configuration item
            suggested_payload["searchParameters"][0]["configurationItems"].append({
                "configurationName": config_name,
                "configurationValue": config_value
            })
        elif config_name.startswith("supplementaryConfiguration."):
            # Add as a supplementary configuration item
            if "supplementaryConfigurationItems" not in suggested_payload["searchParameters"][0]:
                suggested_payload["searchParameters"][0]["supplementaryConfigurationItems"] = []
            
            suggested_payload["searchParameters"][0]["supplementaryConfigurationItems"].append({
                "supplementaryConfigurationName": config_name,
                "supplementaryConfigurationValue": config_value
            })
        else:
            # Add as a top-level attribute (like businessApplicationName)
            suggested_payload["searchParameters"][0][config_name] = config_value
    
    return suggested_payload

def main():
    # 1. Fetch certificates from Snowflake
    try:
        df_snowflake = fetch_certificates_from_snowflake()
    except Exception as e:
        logger.error(f"Failed to fetch Snowflake data: {e}")
        df_snowflake = pd.DataFrame()
    
    # 2. Fetch certificates from API with initial payload
    initial_payload = {
        "searchParameters": [
            {
                "resourceType": "AWS::ACM::Certificate",
                "configurationItems": [
                    {
                        "configurationName": "issuer",
                        "configurationValue": "Amazon"
                    }
                ]
            }
        ]
    }
    
    try:
        api_certificates = fetch_certificates_from_api(initial_payload)
        df_api = extract_certificate_data(api_certificates)
    except Exception as e:
        logger.error(f"Failed to fetch API data: {e}")
        df_api = pd.DataFrame()
    
    # 3. Compare and analyze certificates
    if not df_snowflake.empty and not df_api.empty:
        analysis_results = identify_common_attributes(df_snowflake, df_api)
        
        # 4. Suggest improved API payload
        suggested_payload = suggest_api_payload(analysis_results)
        
        # 5. Print results
        logger.info("\n=== Analysis Results ===")
        logger.info(f"Matching certificates: {analysis_results['matching_count']}")
        logger.info(f"API-only certificates: {analysis_results['api_only_count']}")
        logger.info(f"Snowflake-only certificates: {analysis_results['snowflake_only_count']}")
        
        logger.info("\n=== Common Configurations ===")
        for config, value in analysis_results['common_configurations'].items():
            logger.info(f"{config}: {value}")
        
        logger.info("\n=== Suggested API Payload ===")
        logger.info(json.dumps(suggested_payload, indent=2))
        
        # 6. Save results to files
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Save suggested payload
        with open(f"suggested_payload_{timestamp}.json", "w") as f:
            json.dump(suggested_payload, indent=2, fp=f)
            
        # Save analysis results
        with open(f"analysis_results_{timestamp}.json", "w") as f:
            # Convert sets to lists for JSON serialization
            serializable_results = {
                k: v if not isinstance(v, set) else list(v) 
                for k, v in analysis_results.items()
            }
            json.dump(serializable_results, indent=2, fp=f)
        
        logger.info(f"\nResults saved to suggested_payload_{timestamp}.json and analysis_results_{timestamp}.json")
    else:
        logger.error("Cannot perform analysis because one or both datasets are empty")

if __name__ == "__main__":
    main()
