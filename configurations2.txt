# Define control configurations (what metrics to calculate)
CONTROL_CONFIGS = [
    {
        "cloud_control_id": "CM-2.AWS.12.v02",  # Unique ID from the cloud system
        "ctrl_id": "CTRL-1234567",             # Control ID mapping to monitoring metrics
        "metric_ids": {                        # Metric IDs derived from ctrl_id
            "tier1": "MNTR-1234567-T1",
            "tier2": "MNTR-1234567-T2",
            "tier3": "MNTR-1234567-T3"
        },
        "requires_tier3": True
    },
    {
        "cloud_control_id": "CM-3.AWS.1.v03",
        "ctrl_id": "CTRL-1234568",
        "metric_ids": {
            "tier1": "MNTR-1234568-T1",
            "tier2": "MNTR-1234568-T2"
        },
        "requires_tier3": False
    }
    # Replace CTRL-1234567 and CTRL-1234568 with your actual Control IDs
]

# Define Snowflake table where results will be stored
SNOWFLAKE_TABLE = "CYBR_DB_COLLAB.LAB_ESRA_TCRD.CYBER_CONTROLS_MONITORING"
SNOWFLAKE_SOURCE_NAME = "snowflake"

# Snowflake connection settings
sfOptions = {
    "sfUrl": "https://abc123.snowflakecomputing.com",
    "sfUser": "METRICS_INGEST",
    "sfPassword": "${SNOWFLAKE_INGEST_PWD}",
    "sfRole": "METRICS_READER",
    "sfDatabase": "CYBR_DB_COLLAB",
    "sfSchema": "LAB_ESRA_TCRD",
    "sfWarehouse": "METRICS_WH",
    "sfClientSessionKeepAlive": "true"
}

# Define API settings for fetching approved accounts
API_URL = "https://api.cloud.capitalone.com/internal-operations/cloud-service/aws-tooling/accounts"
API_HEADERS = {
    'X-Cloud-Accounts-Business-Application': 'BACyberProcessAutomation',
    'Authorization': 'Bearer ${AUTH_TOKEN}',
    'Accept': 'application/json;v=2.0',
    'Content-Type': 'application/json'
}
API_PARAMS = {
    'accountStatus': 'Active',
    'region': ['us-east-1', 'us-east-2', 'us-west-2', 'eu-west-1', 'eu-west-2', 'ca-central-1']
}

logger.info("Configuration defined for controls, Snowflake, and API")


-----
def validate_configs():
    """Check that our control configurations are valid before starting."""
    for config in CONTROL_CONFIGS:
        control_id = config["cloud_control_id"]
        ctrl_id = config.get("ctrl_id")  # Get ctrl_id, expect it to exist
        if not ctrl_id or not re.match(r'^CTRL-\d+$', ctrl_id):  # Basic validation (e.g., CTRL- followed by numbers)
            logger.error(f"Invalid ctrl_id for {control_id}: {ctrl_id}")
            raise ValueError(f"Invalid ctrl_id: {ctrl_id}")
        tiers = ['tier1', 'tier2'] + (['tier3'] if config.get("requires_tier3", False) else [])
        for tier in tiers:
            metric_id = config["metric_ids"].get(tier)
            if not metric_id or not re.match(r'^MNTR-\w+-T\d$', metric_id):
                logger.error(f"Invalid metric ID for {control_id} in {tier}: {metric_id}")
                raise ValueError(f"Invalid metric ID: {metric_id}")
    logger.info("Control configurations validated successfully")

def calculate_metrics(alert_val, warning_val, numerator, denominator):
    """Calculate a percentage metric and assign a status (GREEN, YELLOW, RED)."""
    alert = float(alert_val) if alert_val is not None else None
    warning = float(warning_val) if warning_val is not None else None
    numerator = float(numerator)
    denominator = float(denominator)
    
    metric = numerator / denominator * 100 if denominator > 0 else 100.0
    metric = round(metric, 2)
    
    status = "GREEN"
    if alert is not None and warning is not None:
        if metric < alert:
            status = "RED"
        elif metric < warning and metric >= alert:
            status = "YELLOW"
    logger.debug(f"Metric calculated: {metric}%, status: {status}, alert={alert}, warning={warning}, num={numerator}, denom={denominator}")
    return {
        "metric": metric,
        "status": status,
        "numerator": numerator,
        "denominator": denominator
    }

-------
def process_tier1(spark, control_config):
    """Calculate Tier 1 metrics (coverage: % of roles evaluated) and evidence."""
    metric_id = control_config["metric_ids"]["tier1"]
    logger.info(f"Processing Tier 1 for control {control_config['cloud_control_id']}, metric {metric_id}")

    tier1_threshold_query = THRESHOLD_QUERY_TEMPLATE.format(metric_id=metric_id)
    threshold_df = spark.read.format(SNOWFLAKE_SOURCE_NAME).options(**sfOptions).option("query", tier1_threshold_query).load()
    threshold_data = threshold_df.first()
    alert = threshold_data["ALERT_THRESHOLD"] if threshold_data else None
    warning = threshold_data["WARNING_THRESHOLD"] if threshold_data else None
    logger.debug(f"Tier 1 thresholds: alert={alert}, warning={warning}")

    df_filtered = spark.sql("""
        SELECT RESOURCE_ID, AMAZON_RESOURCE_NAME, ACCOUNT, BA, ROLE_TYPE
        FROM all_iam_roles
        WHERE ACCOUNT IN (SELECT ACCOUNT FROM approved_accounts)
          AND ROLE_TYPE = 'MACHINE'
    """)
    total_roles = df_filtered.count()
    logger.debug(f"Total machine roles: {total_roles}")

    evaluated_roles = spark.table("evaluated_roles")
    evaluated_count = df_filtered.join(
        evaluated_roles, 
        df_filtered.AMAZON_RESOURCE_NAME == evaluated_roles.RESOURCE_NAME, 
        "inner"
    ).count()
    logger.debug(f"Evaluated roles: {evaluated_count}")

    metrics = calculate_metrics(alert, warning, evaluated_count, total_roles)
    metrics_df = spark.createDataFrame([(
        date.today(),
        control_config["ctrl_id"],  # Use Control ID (e.g., CTRL-1234567) instead of cloud_control_id
        metric_id,
        metrics["metric"],
        metrics["status"],
        metrics["numerator"],
        metrics["denominator"]
    )], ["DATE", "CTRL_ID", "MONITORING_METRIC_NUMBER", "MONITORING_METRIC", "COMPLIANCE_STATUS", "NUMERATOR", "DENOMINATOR"])
    logger.info(f"Tier 1 metrics: {metrics}")

    evidence_df = df_filtered.join(
        evaluated_roles, 
        df_filtered.AMAZON_RESOURCE_NAME == evaluated_roles.RESOURCE_NAME, 
        "left_outer"
    ).select("RESOURCE_ID", "AMAZON_RESOURCE_NAME", col("COMPLIANCE_STATUS").alias("EVALUATION_STATUS"))
    logger.debug(f"Tier 1 evidence rows: {evidence_df.count()}")

    return metrics_df, evidence_df

-------
def process_tier2(spark, control_config):
    """Calculate Tier 2 metrics (compliance: % of roles compliant) and evidence."""
    metric_id = control_config["metric_ids"]["tier2"]
    logger.info(f"Processing Tier 2 for control {control_config['cloud_control_id']}, metric {metric_id}")

    tier2_threshold_query = THRESHOLD_QUERY_TEMPLATE.format(metric_id=metric_id)
    threshold_df = spark.read.format(SNOWFLAKE_SOURCE_NAME).options(**sfOptions).option("query", tier2_threshold_query).load()
    threshold_data = threshold_df.first()
    alert = threshold_data["ALERT_THRESHOLD"] if threshold_data else None
    warning = threshold_data["WARNING_THRESHOLD"] if threshold_data else None
    logger.debug(f"Tier 2 thresholds: alert={alert}, warning={warning}")

    df_combined = spark.sql("""
        SELECT a.RESOURCE_ID, a.AMAZON_RESOURCE_NAME, a.ACCOUNT, a.BA, a.ROLE_TYPE, e.COMPLIANCE_STATUS
        FROM all_iam_roles a
        LEFT JOIN evaluated_roles e
        ON a.AMAZON_RESOURCE_NAME = e.RESOURCE_NAME
        WHERE a.ACCOUNT IN (SELECT ACCOUNT FROM approved_accounts)
          AND a.ROLE_TYPE = 'MACHINE'
    """)
    total_roles = df_combined.count()
    logger.debug(f"Total roles in Tier 2: {total_roles}")

    compliant_roles = df_combined.filter(col("COMPLIANCE_STATUS").isin(["Compliant", "CompliantControlAllowance"])).count()
    logger.debug(f"Compliant roles: {compliant_roles}")

    metrics = calculate_metrics(alert, warning, compliant_roles, total_roles)
    metrics_df = spark.createDataFrame([(
        date.today(),
        control_config["ctrl_id"],  # Use Control ID
        metric_id,
        metrics["metric"],
        metrics["status"],
        metrics["numerator"],
        metrics["denominator"]
    )], ["DATE", "CTRL_ID", "MONITORING_METRIC_NUMBER", "MONITORING_METRIC", "COMPLIANCE_STATUS", "NUMERATOR", "DENOMINATOR"])
    logger.info(f"Tier 2 metrics: {metrics}")

    df_combined.createOrReplaceTempView("evaluated_roles_with_compliance")
    evidence_df = df_combined.select("RESOURCE_ID", "AMAZON_RESOURCE_NAME", "COMPLIANCE_STATUS")
    logger.debug(f"Tier 2 evidence rows: {evidence_df.count()}")

    return metrics_df, evidence_df
--------
def process_tier3(spark, control_config):
    """Calculate Tier 3 metrics (SLA compliance: % of non-compliant roles within SLA) and evidence."""
    metric_id = control_config["metric_ids"]["tier3"]
    logger.info(f"Processing Tier 3 for control {control_config['cloud_control_id']}, metric {metric_id}")

    tier3_threshold_query = THRESHOLD_QUERY_TEMPLATE.format(metric_id=metric_id)
    threshold_df = spark.read.format(SNOWFLAKE_SOURCE_NAME).options(**sfOptions).option("query", tier3_threshold_query).load()
    threshold_data = threshold_df.first()
    alert = threshold_data["ALERT_THRESHOLD"] if threshold_data else None
    warning = threshold_data["WARNING_THRESHOLD"] if threshold_data else None
    logger.debug(f"Tier 3 thresholds: alert={alert}, warning={warning}")

    if not 'evaluated_roles_with_compliance' in spark.catalog.listTables():
        logger.error("Missing evaluated_roles_with_compliance from Tier 2")
        raise ValueError("evaluated_roles_with_compliance temp view not found. Ensure Tier 2 ran successfully.")
    df_evaluated_roles = spark.table("evaluated_roles_with_compliance")

    total_non_compliant = df_evaluated_roles.filter(col("COMPLIANCE_STATUS") == "NonCompliant").count()
    logger.debug(f"Total non-compliant roles: {total_non_compliant}")

    if total_non_compliant <= 0:
        logger.warning("No non-compliant roles found. Setting metric to 100% GREEN.")
        metrics = {"metric": 100.0, "status": "GREEN", "numerator": 0.0, "denominator": 0.0}
        evidence_data = {
            "RESOURCE_ID": [None], "AMAZON_RESOURCE_NAME": [None], "ACCOUNT": [None], "BA": [None], "ROLE_TYPE": [None],
            "CONTROL_RISK": [None], "OPEN_DATE": [None], "DAYS_OPEN": [None], "SLA_LIMIT": [None], "SLA_STATUS": [None],
            "NOTES": ["All Evaluated Roles are Compliant"]
        }
    else:
        non_compliant_resources = [(row["RESOURCE_ID"], row["AMAZON_RESOURCE_NAME"], row["ACCOUNT"], row["BA"], row["ROLE_TYPE"])
                                   for row in df_evaluated_roles.filter(col("COMPLIANCE_STATUS") == "NonCompliant").collect()]
        resource_ids = [resource_id for resource_id, _, _, _, _ in non_compliant_resources]
        resource_id_list = ",".join([f"'{rid}'" for rid in resource_ids])
        logger.debug(f"Non-compliant resource IDs: {len(resource_ids)}")

        # Use cloud_control_id for the query, as it’s the identifier in the source data
        sla_query = f"""
        SELECT 
            RESOURCE_ID,
            CONTROL_RISK,
            OPEN_DATE_UTC_TIMESTAMP
        FROM CLCN_DB.PHDP_CLOUD.OZONE_NON_COMPLIANT_RESOURCES_TCRD_VIEW_V01
        WHERE CONTROL_ID = '{control_config["cloud_control_id"]}'
          AND RESOURCE_ID IN ({resource_id_list})
          AND ID NOT IN (SELECT ID FROM CLCN_DB.PHDP_CLOUD.OZONE_CLOSED_NON_COMPLIANT_RESOURCES_V04)
        """
        df_sla_data = spark.read.format(SNOWFLAKE_SOURCE_NAME).options(**sfOptions).option("query", sla_query).load()
        logger.debug(f"SLA data rows: {df_sla_data.count()}")

        sla_thresholds = {"Critical": 0, "High": 30, "Medium": 60, "Low": 90}
        current_date = datetime.now()
        sla_data = [(row["RESOURCE_ID"], row["CONTROL_RISK"], row["OPEN_DATE_UTC_TIMESTAMP"]) for row in df_sla_data.collect()]
        
        past_sla_count = 0
        sla_data_map = {}
        for resource_id, control_risk, open_date in sla_data:
            if open_date is not None and control_risk in sla_thresholds:
                days_open = (current_date - open_date).days
                sla_limit = sla_thresholds.get(control_risk, 90)
                sla_status = "Past SLA" if days_open > sla_limit else "Within SLA"
                if days_open > sla_limit:
                    past_sla_count += 1
            else:
                days_open = None
                sla_limit = None
                sla_status = "Unknown"
                logger.warning(f"Invalid SLA data for resource_id={resource_id}")
            sla_data_map[resource_id] = {
                "CONTROL_RISK": control_risk, "OPEN_DATE": open_date, "DAYS_OPEN": days_open,
                "SLA_LIMIT": sla_limit, "SLA_STATUS": sla_status
            }

        within_sla_count = total_non_compliant - past_sla_count if total_non_compliant >= past_sla_count else 0
        metrics = calculate_metrics(alert, warning, within_sla_count, total_non_compliant)
        logger.info(f"Tier 3 metrics: {metrics}")

        evidence_rows = []
        for resource_id, arn, account, ba, role_type in non_compliant_resources:
            sla_data = sla_data_map.get(resource_id, {
                "CONTROL_RISK": "Unknown", "OPEN_DATE": None, "DAYS_OPEN": None, "SLA_LIMIT": None, "SLA_STATUS": "Unknown"
            })
            evidence_rows.append((
                resource_id, arn, account, ba, role_type,
                sla_data["CONTROL_RISK"], sla_data["OPEN_DATE"], str(sla_data["DAYS_OPEN"]), str(sla_data["SLA_LIMIT"]),
                sla_data["SLA_STATUS"], f"NonCompliant - {sla_data['SLA_STATUS']}"
            ))

    evidence_schema = StructType([
        StructField("RESOURCE_ID", StringType(), True),
        StructField("AMAZON_RESOURCE_NAME", StringType(), True),
        StructField("ACCOUNT", StringType(), True),
        StructField("BA", StringType(), True),
        StructField("ROLE_TYPE", StringType(), True),
        StructField("CONTROL_RISK", StringType(), True),
        StructField("OPEN_DATE", TimestampType(), True),
        StructField("DAYS_OPEN", StringType(), True),
        StructField("SLA_LIMIT", StringType(), True),
        StructField("SLA_STATUS", StringType(), True),
        StructField("NOTES", StringType(), True)
    ])

    metrics_df = spark.createDataFrame([(
        date.today(),
        control_config["ctrl_id"],  # Use Control ID
        metric_id,
        metrics["metric"],
        metrics["status"],
        metrics["numerator"],
        metrics["denominator"]
    )], ["DATE", "CTRL_ID", "MONITORING_METRIC_NUMBER", "MONITORING_METRIC", "COMPLIANCE_STATUS", "NUMERATOR", "DENOMINATOR"])

    evidence_df = spark.createDataFrame(
        evidence_rows if total_non_compliant > 0 else [(
            None, None, None, None, None, None, None, None, None, None, "All Evaluated Roles are Compliant"
        )],
        schema=evidence_schema
    )
    logger.debug(f"Tier 3 evidence rows: {evidence_df.count()}")

    return metrics_df, evidence_df

-----
def write_to_snowflake(spark, df, metric_id, evidence_df):
    """Save metrics to Snowflake (evidence stored separately, not written)."""
    logger.info(f"Writing metrics for {metric_id} to Snowflake")
    
    # Write only the metrics DataFrame (no evidence column yet in Snowflake)
    df.write.format(SNOWFLAKE_SOURCE_NAME)\
        .options(**sfOptions)\
        .option("dbtable", SNOWFLAKE_TABLE)\
        .mode("append")\
        .save()
    logger.info(f"Wrote metrics for {metric_id} to Snowflake (evidence stored separately with {evidence_df.count()} rows)")
    # Evidence is passed back but not written; we’ll display it later

-------
def main(spark):
    """Run the entire pipeline and display results."""
    logger.info("Starting consolidated control pipeline")

    try:
        df_approved_accounts = get_approved_accounts(spark)
    except Exception as e:
        logger.error(f"Failed to fetch approved accounts: {str(e)}", exc_info=True)
        raise

    try:
        df_all_iam_roles = spark.read.format(SNOWFLAKE_SOURCE_NAME).options(**sfOptions).option("query", ALL_IAM_ROLES_QUERY).load()
        df_all_iam_roles.cache().createOrReplaceTempView("all_iam_roles")
        if df_all_iam_roles.isEmpty():
            raise ValueError("Empty IAM roles dataset")
        logger.info(f"Loaded {df_all_iam_roles.count()} IAM roles")
        logger.debug(f"IAM roles schema: {df_all_iam_roles.schema}")
    except Exception as e:
        logger.error(f"Failed to load IAM roles: {str(e)}", exc_info=True)
        raise

    # Store results for all controls
    results = {}

    for control in CONTROL_CONFIGS:
        logger.info(f"Processing control: {control['cloud_control_id']}")
        try:
            evaluated_query = EVALUATED_ROLES_QUERY.format(control_id=control["cloud_control_id"])
            df_evaluated_roles = spark.read.format(SNOWFLAKE_SOURCE_NAME).options(**sfOptions).option("query", evaluated_query).load()
            df_evaluated_roles.cache().createOrReplaceTempView("evaluated_roles")
            logger.debug(f"Evaluated roles count: {df_evaluated_roles.count()}")

            # Process tiers and store results
            results[control["cloud_control_id"]] = {"metrics": {}, "evidence": {}}

            tier1_df, tier1_evidence = process_tier1(spark, control)
            write_to_snowflake(spark, tier1_df, control["metric_ids"]["tier1"], tier1_evidence)
            results[control["cloud_control_id"]]["metrics"]["tier1"] = tier1_df
            results[control["cloud_control_id"]]["evidence"]["tier1"] = tier1_evidence

            tier2_df, tier2_evidence = process_tier2(spark, control)
            write_to_snowflake(spark, tier2_df, control["metric_ids"]["tier2"], tier2_evidence)
            results[control["cloud_control_id"]]["metrics"]["tier2"] = tier2_df
            results[control["cloud_control_id"]]["evidence"]["tier2"] = tier2_evidence

            if control.get("requires_tier3", False):
                tier3_df, tier3_evidence = process_tier3(spark, control)
                write_to_snowflake(spark, tier3_df, control["metric_ids"]["tier3"], tier3_evidence)
                results[control["cloud_control_id"]]["metrics"]["tier3"] = tier3_df
                results[control["cloud_control_id"]]["evidence"]["tier3"] = tier3_evidence

        except Exception as e:
            logger.error(f"Error processing control {control['cloud_control_id']}: {str(e)}", exc_info=True)
            raise

    # Display all results
    logger.info("Displaying final results for all controls")
    for control_id, data in results.items():
        print(f"\n=== Control: {control_id} ===")
        for tier, metrics_df in data["metrics"].items():
            print(f"\nTier {tier} Metrics:")
            metrics_df.show(truncate=False)  # Display metrics DataFrame
        for tier, evidence_df in data["evidence"].items():
            print(f"\nTier {tier} Evidence:")
            evidence_df.show(truncate=False)  # Display evidence DataFrame

    logger.info("All controls processed and results displayed successfully")
    return results  # Return results for potential further use

if __name__ == "__main__":
    if 'spark' not in globals():
        logger.error("No Spark session found. Must run in Databricks or similar.")
        raise Exception("No Spark session found. This script must be run in a Spark environment.")
    
    spark = SparkSession.builder\
        .appName("ConsolidatedControlMetrics")\
        .config("spark.sql.shuffle.partitions", "8")\
        .config("spark.executor.memory", "4g")\
        .getOrCreate()
    
    try:
        validate_configs()
        main(spark)
    except Exception as e:
        logger.error(f"Pipeline failed: {str(e)}", exc_info=True)
        sys.exit(1)
    finally:
        spark.stop()
        logger.info("Spark session stopped")
