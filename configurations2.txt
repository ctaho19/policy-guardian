import os
import time
import json
import requests
from datetime import datetime
from typing import Dict, List, Set, Tuple, Optional, Any
import pyspark.sql.functions as F
from pyspark.sql.types import StructType, StructField, StringType, ArrayType

# API details
API_URL = "https://api.cloud.capitalone.com/internal-operations/cloud-service/aws-tooling/search-resource-configurations"
API_HEADERS = {
    'Accept': 'application/json;v=1.0',
    'Authorization': 'Bearer YOUR_API_TOKEN',  # Replace with actual token
    'Content-Type': 'application/json'
}
API_CALL_DELAY = 0.15  # Delay between API calls to avoid rate limits
API_TIMEOUT = 60  # Timeout for API calls

# Snowflake connection parameters using dbutils.secrets
username = dbutils.secrets.get(scope="uno201_scope", key="eid")
password = dbutils.secrets.get(scope="uno201_scope", key="password")

# Define Snowflake connection options
SNOWFLAKE_SOURCE_NAME = "net.snowflake.spark.snowflake"
sfOptions = dict(
    sfUrl="prod.us-east-1.capitalone.snowflakecomputing.com",
    sfUser=username,
    sfPassword=password,
    sfDatabase="SB",
    sfSchema="USER_UNO201",
    sfWarehouse="CYBR_Q_DI_WH",
)

# Configure logging
import logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def fetch_certificates_from_snowflake():
    """Fetch certificates from Snowflake using Spark."""
    try:
        query = """
        SELECT DISTINCT 
            CERTIFICATE_ARN, 
            CERTIFICATE_ID, 
            NOMINAL_ISSUER, 
            NOT_VALID_BEFORE_UTC_TIMESTAMP, 
            NOT_VALID_AFTER_UTC_TIMESTAMP, 
            LAST_USAGE_OBSERVATION_UTC_TIMESTAMP
        FROM CYBR_DB.PHDP_CYBR.CERTIFICATE_CATALOG_CERTIFICATE_USAGE 
        WHERE CERTIFICATE_ARN LIKE '%arn:aws:acm%' 
          AND NOMINAL_ISSUER = 'Amazon' 
          AND NOT_VALID_AFTER_UTC_TIMESTAMP >= DATEADD('DAY', -365, CURRENT_TIMESTAMP)
        """
        logger.info("Fetching certificates from Snowflake using Spark...")
        
        # Use Spark to query Snowflake
        snowflake_df = (
            spark.read.format(SNOWFLAKE_SOURCE_NAME)
            .options(**sfOptions)
            .option("query", query)
            .load()
        )
        
        # Convert all column names to lowercase for consistency
        snowflake_df = snowflake_df.toDF(*[c.lower() for c in snowflake_df.columns])
        
        logger.info(f"Retrieved {snowflake_df.count()} certificates from Snowflake")
        return snowflake_df
    except Exception as e:
        logger.error(f"Error fetching certificates from Snowflake: {e}")
        raise

def fetch_certificates_from_api(payload: Dict) -> List[Dict]:
    """Fetch certificates from the AWS Tooling API with pagination."""
    all_certificates = []
    next_record_key = None
    page_count = 0
    
    logger.info(f"Fetching certificates from API with payload: {json.dumps(payload, indent=2)}")
    
    while True:
        page_count += 1
        current_url = API_URL
        params = {'limit': 10000}
        
        if next_record_key:
            params['nextRecordKey'] = next_record_key
        
        try:
            logger.info(f"Fetching page {page_count} from API...")
            response = requests.post(
                current_url, 
                headers=API_HEADERS, 
                json=payload,
                params=params,
                timeout=API_TIMEOUT, 
                verify=False
            )
            response.raise_for_status()
            data = response.json()
            
            certificates = data.get('resourceConfigurations', [])
            logger.info(f"Retrieved {len(certificates)} certificates from page {page_count}")
            all_certificates.extend(certificates)
            
            next_record_key = data.get('nextRecordKey')
            if not next_record_key:
                logger.info("No more pages to fetch")
                break
                
            # Sleep to avoid rate limits
            time.sleep(API_CALL_DELAY)
            
        except requests.exceptions.RequestException as e:
            logger.error(f"API request failed on page {page_count}: {e}")
            # Continue with certificates fetched so far
            break
    
    logger.info(f"Total certificates fetched from API: {len(all_certificates)}")
    return all_certificates

def flatten_api_certificates(api_certificates: List[Dict]):
    """
    Convert API certificates list to a flattened Spark DataFrame
    with a separate column for each configuration item.
    """
    if not api_certificates:
        return spark.createDataFrame([], StructType([]))
    
    # First, create a DataFrame from the top-level attributes
    top_level_schema = StructType([
        StructField("amazonResourceName", StringType(), True),
        StructField("resourceId", StringType(), True),
        StructField("awsAccountId", StringType(), True),
        StructField("awsRegion", StringType(), True),
        StructField("businessApplicationName", StringType(), True),
        StructField("environment", StringType(), True),
        StructField("source", StringType(), True),
        StructField("configurationList", ArrayType(
            StructType([
                StructField("configurationName", StringType(), True),
                StructField("configurationValue", StringType(), True)
            ])
        ), True),
        StructField("supplementaryConfiguration", ArrayType(
            StructType([
                StructField("supplementaryConfigurationName", StringType(), True),
                StructField("supplementaryConfigurationValue", StringType(), True)
            ])
        ), True)
    ])
    
    # Create a DataFrame from the API certificates
    api_df = spark.createDataFrame(api_certificates, schema=top_level_schema)
    
    # Create a mapping of configurationName to column names, collecting all unique configuration names
    config_names = set()
    supp_config_names = set()
    
    for cert in api_certificates:
        for config in cert.get("configurationList", []):
            if "configurationName" in config:
                config_names.add(config["configurationName"])
        for config in cert.get("supplementaryConfiguration", []):
            if "supplementaryConfigurationName" in config:
                supp_config_names.add(config["supplementaryConfigurationName"])
    
    # Now, for each unique configuration name, create a new column in the DataFrame
    flattened_df = api_df
    
    for config_name in config_names:
        # Create a UDF to extract the configuration value for this configuration name
        flattened_df = flattened_df.withColumn(
            config_name,
            F.expr(f"""
                CASE
                    WHEN configurationList IS NOT NULL THEN
                        (SELECT configurationValue FROM (SELECT explode(configurationList) as config) 
                         WHERE config.configurationName = '{config_name}' LIMIT 1)
                    ELSE NULL
                END
            """)
        )
    
    for config_name in supp_config_names:
        # Create a UDF to extract the supplementary configuration value for this configuration name
        flattened_df = flattened_df.withColumn(
            config_name,
            F.expr(f"""
                CASE
                    WHEN supplementaryConfiguration IS NOT NULL THEN
                        (SELECT supplementaryConfigurationValue FROM (SELECT explode(supplementaryConfiguration) as config) 
                         WHERE config.supplementaryConfigurationName = '{config_name}' LIMIT 1)
                    ELSE NULL
                END
            """)
        )
    
    # Drop the arrays since we've extracted the values
    flattened_df = flattened_df.drop("configurationList", "supplementaryConfiguration")
    
    # Convert all column names to lowercase for consistency
    flattened_df = flattened_df.toDF(*[c.lower() for c in flattened_df.columns])
    
    return flattened_df

def identify_common_attributes_spark(df_snowflake, df_api):
    """Identify common attributes between dataset and API certificates using Spark."""
    # Convert ARN columns to lowercase for consistency
    df_snowflake = df_snowflake.withColumn("certificate_arn", F.lower(F.col("certificate_arn")))
    df_api = df_api.withColumn("amazonresourcename", F.lower(F.col("amazonresourcename")))
    
    # Get counts for analysis
    snowflake_count = df_snowflake.count()
    api_count = df_api.count()
    
    # Find matching certificates using a join
    df_matching = df_api.join(
        df_snowflake,
        df_api.amazonresourcename == df_snowflake.certificate_arn,
        "inner"
    )
    matching_count = df_matching.count()
    
    # Find API-only and Snowflake-only certificates
    df_api_only = df_api.join(
        df_snowflake,
        df_api.amazonresourcename == df_snowflake.certificate_arn,
        "left_anti"  # Equivalent to NOT IN subquery
    )
    api_only_count = df_api_only.count()
    
    df_snowflake_only = df_snowflake.join(
        df_api,
        df_snowflake.certificate_arn == df_api.amazonresourcename,
        "left_anti"  # Equivalent to NOT IN subquery
    )
    snowflake_only_count = df_snowflake_only.count()
    
    logger.info(f"Matching certificates: {matching_count}")
    logger.info(f"API-only certificates: {api_only_count}")
    logger.info(f"Snowflake-only certificates: {snowflake_only_count}")
    
    # Analyze configurations that might differentiate between matching and non-matching
    differences = {}
    common_configurations = {}
    
    # Only analyze columns that have reasonable, non-null values
    # Skip identifier columns
    skip_columns = ["amazonresourcename", "resourceid", "certificate_arn", "certificate_id"]
    
    # Get a list of all columns in the API DataFrame, excluding identifiers
    api_columns = [col for col in df_api.columns if col not in skip_columns]
    
    for col in api_columns:
        # Check if the column has a significant number of non-null values
        non_null_count = df_api.filter(F.col(col).isNotNull()).count()
        if non_null_count / api_count < 0.05:  # Skip columns with less than 5% non-null values
            continue
        
        # Get unique values for matching certificates
        if matching_count > 0:
            matching_values = [row[col] for row in df_matching.select(col).distinct().filter(F.col(col).isNotNull()).collect()]
        else:
            matching_values = []
            
        # Get unique values for non-matching certificates
        if api_only_count > 0:
            non_matching_values = [row[col] for row in df_api_only.select(col).distinct().filter(F.col(col).isNotNull()).collect()]
        else:
            non_matching_values = []
        
        # Check if there are differences in values
        if matching_values and set(matching_values) != set(non_matching_values):
            # Find values that are common in the matching set but rare in the non-matching set
            distinctive_values = []
            
            for val in matching_values:
                if val and (val not in non_matching_values or 
                           (df_matching.filter(F.col(col) == val).count() / matching_count) > 
                           (df_api_only.filter(F.col(col) == val).count() / api_only_count) * 3):
                    distinctive_values.append(val)
            
            if distinctive_values:
                differences[col] = {
                    "matching_values": matching_values,
                    "non_matching_values": non_matching_values,
                    "distinctive_values": distinctive_values
                }
                
                # Look for common values in the matching set
                common_value = None
                
                # If there's only one unique value in matching certificates
                if len(matching_values) == 1:
                    common_value = matching_values[0]
                else:
                    # Get value counts to find the most common value
                    value_counts = df_matching.groupBy(col).count().orderBy(F.desc("count"))
                    
                    if value_counts.count() > 0:
                        top_row = value_counts.first()
                        top_value = top_row[col]
                        top_count = top_row["count"]
                        
                        # If the most common value appears in >80% of matching records
                        if top_count / matching_count > 0.8:
                            common_value = top_value
                
                if common_value:
                    common_configurations[col] = common_value
    
    return {
        "matching_count": matching_count,
        "api_only_count": api_only_count,
        "snowflake_only_count": snowflake_only_count,
        "differences": differences,
        "common_configurations": common_configurations
    }

def suggest_api_payload(analysis_results):
    """Suggest an improved API payload based on analysis results."""
    # Start with the basic payload
    suggested_payload = {
        "searchParameters": [
            {
                "resourceType": "AWS::ACM::Certificate",
                "configurationItems": [
                    {
                        "configurationName": "issuer",
                        "configurationValue": "Amazon"
                    }
                ]
            }
        ]
    }
    
    # Add additional configurations that seem to be common across dataset certificates
    common_configs = analysis_results.get("common_configurations", {})
    for config_name, config_value in common_configs.items():
        # Skip if already present
        if config_name in ['issuer']:
            continue
            
        # Check if it's a configuration item or a top-level attribute
        if config_name.startswith("configuration."):
            # Add as a configuration item
            suggested_payload["searchParameters"][0]["configurationItems"].append({
                "configurationName": config_name,
                "configurationValue": config_value
            })
        elif config_name.startswith("supplementaryconfiguration."):
            # Add as a supplementary configuration item
            if "supplementaryConfigurationItems" not in suggested_payload["searchParameters"][0]:
                suggested_payload["searchParameters"][0]["supplementaryConfigurationItems"] = []
            
            suggested_payload["searchParameters"][0]["supplementaryConfigurationItems"].append({
                "supplementaryConfigurationName": config_name,
                "supplementaryConfigurationValue": config_value
            })
        else:
            # Add as a top-level attribute (like businessApplicationName)
            suggested_payload["searchParameters"][0][config_name] = config_value
    
    return suggested_payload

def main():
    # 1. Fetch certificates from Snowflake
    try:
        df_snowflake = fetch_certificates_from_snowflake()
        # Cache the DataFrame for faster performance
        df_snowflake.cache()
        print("Snowflake certificates:")
        display(df_snowflake.limit(10))
    except Exception as e:
        logger.error(f"Failed to fetch Snowflake data: {e}")
        # Create an empty DataFrame with the expected schema
        df_snowflake = spark.createDataFrame([], StructType([
            StructField("certificate_arn", StringType(), True),
            StructField("certificate_id", StringType(), True),
            StructField("nominal_issuer", StringType(), True),
            StructField("not_valid_before_utc_timestamp", StringType(), True),
            StructField("not_valid_after_utc_timestamp", StringType(), True),
            StructField("last_usage_observation_utc_timestamp", StringType(), True)
        ]))
    
    # 2. Fetch certificates from API with initial payload
    initial_payload = {
        "searchParameters": [
            {
                "resourceType": "AWS::ACM::Certificate",
                "configurationItems": [
                    {
                        "configurationName": "issuer",
                        "configurationValue": "Amazon"
                    }
                ]
            }
        ]
    }
    
    try:
        api_certificates = fetch_certificates_from_api(initial_payload)
        df_api = flatten_api_certificates(api_certificates)
        # Cache the DataFrame for faster performance
        df_api.cache()
        print("API certificates:")
        display(df_api.limit(10))
    except Exception as e:
        logger.error(f"Failed to fetch API data: {e}")
        df_api = spark.createDataFrame([], StructType([]))
    
    # 3. Compare and analyze certificates
    if not df_snowflake.isEmpty() and not df_api.isEmpty():
        analysis_results = identify_common_attributes_spark(df_snowflake, df_api)
        
        # 4. Suggest improved API payload
        suggested_payload = suggest_api_payload(analysis_results)
        
        # 5. Print results
        print("\n=== Analysis Results ===")
        print(f"Matching certificates: {analysis_results['matching_count']}")
        print(f"API-only certificates: {analysis_results['api_only_count']}")
        print(f"Snowflake-only certificates: {analysis_results['snowflake_only_count']}")
        
        print("\n=== Common Configurations ===")
        for config, value in analysis_results['common_configurations'].items():
            print(f"{config}: {value}")
        
        print("\n=== Suggested API Payload ===")
        print(json.dumps(suggested_payload, indent=2))
        
        # 6. Save results to tables for better viewing in Databricks
        # Convert the analysis results to a Spark DataFrame
        diff_rows = []
        for col_name, diff in analysis_results['differences'].items():
            diff_rows.append({
                "column_name": col_name,
                "matching_values": str(diff["matching_values"]),
                "non_matching_values": str(diff["non_matching_values"]),
                "distinctive_values": str(diff["distinctive_values"])
            })
        
        if diff_rows:
            diff_df = spark.createDataFrame(diff_rows)
            print("\n=== Differences Analysis ===")
            display(diff_df)
        
        # Save the suggested payload for reference
        with open("/tmp/suggested_payload.json", "w") as f:
            json.dump(suggested_payload, indent=2, fp=f)
        
        print(f"\nResults saved to /tmp/suggested_payload.json")
        
        # Cache dataframes for faster access without creating temporary views
        df_snowflake.cache()
        df_api.cache()
        
        return {
            "snowflake_df": df_snowflake,
            "api_df": df_api,
            "analysis_results": analysis_results,
            "suggested_payload": suggested_payload
        }
    else:
        logger.error("Cannot perform analysis because one or both datasets are empty")
        return None

# Run the main function if this is executed directly
if __name__ == "__main__":
    results = main()
