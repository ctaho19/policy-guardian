import logging
from pyspark.sql.functions import col, count

# Configure logging
logging.basicConfig(level=logging.INFO, 
                    format='%(asctime)s - %(levelname)s - %(message)s',
                    filename='databricks_debug.log')  # Logs to a file; use console for immediate output by removing filename

logger = logging.getLogger(__name__)

# Function to log DataFrame details
def log_dataframe_details(df, name):
    logger.info(f"Inspecting DataFrame: {name}")
    logger.info(f"Schema: {df.schema}")
    logger.info(f"Row count: {df.count()}")
    logger.info(f"Show first 5 rows: {df.show(5, truncate=False)}")
    logger.info(f"Is cached: {df.is_cached}")
    logger.info(f"Storage level: {df.storageLevel}")

try:
    # Assuming mapped_roles_simplified and additional_mapped_roles are defined
    logger.info("Starting union of mapped roles DataFrames")
    
    all_mapped_roles = mapped_roles_simplified.union(additional_mapped_roles)
    log_dataframe_details(all_mapped_roles, "all_mapped_roles")

    logger.info("Filtering and aggregating Machine roles")
    machine_roles_count = all_mapped_roles.filter(
        col("Role_Type") == "MACHINE"
    ).agg(
        count("*").alias("Machine_Roles_Count")
    )
    log_dataframe_details(machine_roles_count, "machine_roles_count")

    # Test output with show (safer than collect)
    logger.info("Attempting to display machine_roles_count with show")
    machine_roles_count.show()

    # Optionally test collect with logging for debugging
    logger.info("Attempting to collect machine_roles_count (if needed)")
    try:
        machine_count_value = machine_roles_count.collect()[0]["Machine_Roles_Count"]
        logger.info(f"Successfully collected count: {machine_count_value}")
        print(machine_count_value)
    except Exception as e:
        logger.error(f"Error during collect: {str(e)}")

    # Optionally test display if show works
    logger.info("Attempting to display machine_roles_count with Databricks display")
    display(machine_roles_count)

except Exception as e:
    logger.error(f"Unexpected error in pipeline: {str(e)}")
    raise

# Log Databricks runtime version for reference
from pyspark.sql import SparkSession
spark = SparkSession.builder.getOrCreate()
logger.info(f"Databricks Runtime Version: {spark.version}")
