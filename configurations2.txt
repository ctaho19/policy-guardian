# Get current date as a Python object
current_date_value = spark.range(1).select(current_date()).first()[0]
log_step("Current date", f"Value: {current_date_value}, Type: {type(current_date_value)}")

# Create metrics_data with Python objects
metrics_data = [(
    current_date_value,  # Now a datetime.date object
    'MNTR-XXXXX',
    results["metric"],
    results["status"],
    results["numerator"],
    results["denominator"]
)]

# Define schema (adjust if yours differs)
metrics_schema = StructType([
    StructField("DATE", DateType(), False),
    StructField("MONITORING_METRIC_NUMBER", StringType(), False),
    StructField("MONITORING_METRIC", DoubleType(), False),
    StructField("COMPLIANCE_STATUS", StringType(), False),
    StructField("NUMERATOR", LongType(), False),
    StructField("DENOMINATOR", LongType(), False)
])

# Create DataFrame
df_result = spark.createDataFrame(metrics_data, metrics_schema)
log_step("DataFrame created", "Tier 1 metrics DataFrame successfully created")

-----------------------------------------------------------------------------------
# Get current date as a Python object
current_date_value = spark.range(1).select(current_date()).first()[0]
log_step("Current date", f"Value: {current_date_value}, Type: {type(current_date_value)}")

# Create Tier 2 metrics_data with Python objects
tier2_metrics_data = [(
    current_date_value,  # Now a datetime.date object
    'MNTR-XXXXX-T2',
    results["metric"],
    results["status"],
    results["numerator"],
    results["denominator"]
)]

# Create Tier 2 DataFrame (reuse schema from Tier 1 or redefine if different)
df_tier2_result = spark.createDataFrame(tier2_metrics_data, metrics_schema)
log_step("DataFrame created", "Tier 2 metrics DataFrame successfully created")
