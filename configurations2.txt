# COMMAND ----------

# MAGIC %md
# MAGIC ## Tier 3 Supporting Evidence
# MAGIC Create supporting evidence for Tier 3 metrics based on non-compliant resources

# COMMAND ----------
from pyspark.sql.functions import col
from pyspark.sql.utils import AnalysisException
from pyspark.sql.types import StructType, StructField, StringType, TimestampType
import logging
import pandas as pd
from datetime import datetime

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

try:
    logger.info("Creating Tier 3 supporting evidence")
    
    # Check for evaluated_roles_with_compliance temp view using a query-based approach
    logger.info("Verifying existence of evaluated_roles_with_compliance temp view")
    try:
        spark.sql("SELECT 1 FROM evaluated_roles_with_compliance LIMIT 1")
        df_evaluated_roles = spark.table("evaluated_roles_with_compliance")
        logger.info("evaluated_roles_with_compliance temp view exists and is accessible.")
    except AnalysisException:
        logger.error("evaluated_roles_with_compliance temp view not found. Ensure Tier 2 ran successfully.")
        raise ValueError("evaluated_roles_with_compliance temp view not found. Ensure Tier 2 ran successfully.")
    
    # Get non-compliant resources from Tier 2
    non_compliant_df = df_evaluated_roles.filter(col("COMPLIANCE_STATUS") == "NonCompliant")
    
    # If no non-compliant resources, create a simple evidence DataFrame
    if non_compliant_df.count() == 0:
        logger.info("No non-compliant resources found for Tier 3 supporting evidence")
        evidence_data = {
            "RESOURCE_ID": [None],
            "ARN": [None],
            "ACCOUNT": [None],
            "BA": [None],
            "ROLE_TYPE": [None],
            "CONTROL_RISK": [None],
            "OPEN_DATE": [None],
            "DAYS_OPEN": [None],
            "SLA_LIMIT": [None],
            "SLA_STATUS": [None],
            "NOTES": ["All Evaluated Roles are Compliant"]
        }
    else:
        # Get SLA data for non-compliant resources
        non_compliant_resources = [(row["RESOURCE_ID"], row["ARN"], row["ACCOUNT"], row["BA"], row["ROLE_TYPE"]) 
                                  for row in non_compliant_df.collect()]
        
        resource_ids = [res[0] for res in non_compliant_resources]
        resource_ids_str = ", ".join([f"'{id}'" for id in resource_ids])
        
        # Query to get SLA data for non-compliant resources
        sla_query = f"""
        SELECT 
            RESOURCE_ID,
            CONTROL_RISK,
            OPEN_DATE_UTC_TIMESTAMP
        FROM EIAM_DB.PHDP_CYBR_IAM.IDENTITY_REPORTS_CONTROLS_VIOLATIONS_SLA
        WHERE RESOURCE_ID IN ({resource_ids_str})
        AND CONTROL_ID = 'CM-2.AWS.12.v02'
        """
        
        logger.info(f"Executing SLA query for non-compliant resources")
        df_sla_data = spark.read.format(SNOWFLAKE_SOURCE_NAME) \
            .options(**sfOptions) \
            .option("query", sla_query) \
            .load()
        
        # Define SLA thresholds
        sla_thresholds = {
            "Critical": 0,
            "High": 30,
            "Medium": 60,
            "Low": 90
        }
        
        # Create a map of resource_id to SLA data
        sla_data_map = {}
        current_date = datetime.now()
        
        for row in df_sla_data.collect():
            resource_id = row["RESOURCE_ID"]
            control_risk = row["CONTROL_RISK"]
            open_date = row["OPEN_DATE_UTC_TIMESTAMP"]
            
            if open_date is not None and control_risk in sla_thresholds:
                days_open = (current_date - open_date).days
                sla_limit = sla_thresholds.get(control_risk, 90)
                sla_status = "Past SLA" if days_open > sla_limit else "Within SLA"
                
                sla_data_map[resource_id] = {
                    "CONTROL_RISK": control_risk,
                    "OPEN_DATE": open_date,
                    "DAYS_OPEN": days_open,
                    "SLA_LIMIT": sla_limit,
                    "SLA_STATUS": sla_status
                }
        
        # Combine non-compliant resources with SLA data
        evidence_rows = []
        for resource_id, arn, account, ba, role_type in non_compliant_resources:
            sla_data = sla_data_map.get(resource_id, {
                "CONTROL_RISK": "Unknown",
                "OPEN_DATE": None,
                "DAYS_OPEN": None,
                "SLA_LIMIT": None,
                "SLA_STATUS": "Unknown"
            })
            
            evidence_rows.append({
                "RESOURCE_ID": resource_id,
                "ARN": arn,
                "ACCOUNT": account,
                "BA": ba,
                "ROLE_TYPE": role_type,
                "CONTROL_RISK": sla_data["CONTROL_RISK"],
                "OPEN_DATE": sla_data["OPEN_DATE"],
                "DAYS_OPEN": sla_data["DAYS_OPEN"],
                "SLA_LIMIT": sla_data["SLA_LIMIT"],
                "SLA_STATUS": sla_data["SLA_STATUS"],
                "NOTES": f"NonCompliant - {sla_data['SLA_STATUS']}"
            })
        
        # Convert to dictionary format for DataFrame creation
        evidence_data = {
            "RESOURCE_ID": [row["RESOURCE_ID"] for row in evidence_rows],
            "ARN": [row["ARN"] for row in evidence_rows],
            "ACCOUNT": [row["ACCOUNT"] for row in evidence_rows],
            "BA": [row["BA"] for row in evidence_rows],
            "ROLE_TYPE": [row["ROLE_TYPE"] for row in evidence_rows],
            "CONTROL_RISK": [row["CONTROL_RISK"] for row in evidence_rows],
            "OPEN_DATE": [row["OPEN_DATE"] for row in evidence_rows],
            "DAYS_OPEN": [row["DAYS_OPEN"] for row in evidence_rows],
            "SLA_LIMIT": [row["SLA_LIMIT"] for row in evidence_rows],
            "SLA_STATUS": [row["SLA_STATUS"] for row in evidence_rows],
            "NOTES": [row["NOTES"] for row in evidence_rows]
        }
    
    logger.info(f"Supporting evidence data: {evidence_data}")
    
    # Define schema for the supporting evidence DataFrame
    evidence_schema = StructType([
        StructField("RESOURCE_ID", StringType(), True),
        StructField("ARN", StringType(), True),
        StructField("ACCOUNT", StringType(), True),
        StructField("BA", StringType(), True),
        StructField("ROLE_TYPE", StringType(), True),
        StructField("CONTROL_RISK", StringType(), True),
        StructField("OPEN_DATE", TimestampType(), True),
        StructField("DAYS_OPEN", StringType(), True),
        StructField("SLA_LIMIT", StringType(), True),
        StructField("SLA_STATUS", StringType(), True),
        StructField("NOTES", StringType(), True)
    ])
    
    # Create DataFrame with explicit schema
    pd_evidence = pd.DataFrame(evidence_data)
    df_supporting_evidence = spark.createDataFrame(pd_evidence, schema=evidence_schema)
    logger.info("Supporting evidence DataFrame created with explicit schema")
    df_supporting_evidence.printSchema()
    logger.info("Supporting evidence DataFrame content:")
    df_supporting_evidence.show(20, truncate=False)
    
    # Register as temp view
    df_supporting_evidence.createOrReplaceTempView("tier3_evidence")
    
except Exception as e:
    logger.error(f"ERROR in Tier 3 supporting evidence: {str(e)}")
    raise
