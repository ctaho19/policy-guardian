def fetch_resources(timeout: int = 60, max_retries: int = 3, page_size: int = 1000) -> Tuple[List[Dict], Dict[str, int]]:
    """
    Fetch all KMS resources and analyze their origin values.
    
    Args:
        timeout: Request timeout in seconds
        max_retries: Maximum number of retry attempts
        page_size: Number of resources to fetch per page
        
    Returns:
        Tuple containing list of resources and dictionary of origin value counts
    """
    all_resources = []
    origin_counts = {}
    next_record_key = ""
    page_count = 0
    unique_resource_ids = set()
    
    # Get expected total count
    payload = {
        "searchParameters": [{
            "resourceType": "AWS::KMS::Key"
        }]
    }
    expected_count = get_summary_count(payload)
    if expected_count is not None:
        logger.info(f"Expected total resources from summary: {expected_count}")
    else:
        logger.warning("Could not get expected count from summary API")
    
    # Base payload with essential fields
    payload = {
        "searchParameters": [{
            "resourceType": "AWS::KMS::Key"
        }],
        "responseFields": [
            "accountName",
            "accountResourceId", 
            "awsAccountId",
            "awsRegion",
            "environment",
            "resourceCreationTimestamp",
            "resourceId",
            "resourceType",
            "configuration.origin"
        ],
        "limit": page_size
    }
    
    start_time = datetime.now()
    
    while True:  # Continue until no more pages
        for retry in range(max_retries + 1):
            try:
                logger.info(f"Requesting page {page_count + 1}" + (f" (retry {retry})" if retry > 0 else ""))
                
                # Add nextRecordKey if we have one
                if next_record_key:
                    payload["nextRecordKey"] = next_record_key
                
                response = requests.post(
                    CONFIG_URL,
                    headers=HEADERS,
                    json=payload,
                    verify=False,
                    timeout=timeout
                )
                
                if response.status_code == 200:
                    data = response.json()
                    resources = data.get("resourceConfigurations", [])
                    
                    # Debug first page response
                    if page_count == 0:
                        logger.debug(f"First page response structure:\n{json.dumps(data, indent=2)}")
                        if resources:
                            logger.debug(f"Sample resource structure:\n{json.dumps(resources[0], indent=2)}")
                    
                    # Process resources
                    for resource in resources:
                        resource_id = resource.get('resourceId')
                        if resource_id in unique_resource_ids:
                            logger.warning(f"Duplicate resource ID found: {resource_id} on page {page_count + 1}")
                            continue
                            
                        unique_resource_ids.add(resource_id)
                        
                        # Extract origin value
                        origin = "Unknown"
                        config_list = resource.get("configurationList", [])
                        for config in config_list:
                            if config["configurationName"] == "configuration.origin":
                                origin = config["configurationValue"]
                                break
                        
                        # Update counts
                        origin_counts[origin] = origin_counts.get(origin, 0) + 1
                        
                        # Store resource with origin
                        resource["extracted_origin"] = origin
                        all_resources.append(resource)
                        
                    # Update pagination info and validate
                    next_record_key = data.get("nextRecordKey", "")
                    logger.info(f"Page {page_count + 1}: Fetched {len(resources)} resources (total: {len(all_resources)})")
                    logger.info(f"Next record key: {next_record_key}")
                    
                    # Validate against expected count
                    if expected_count and len(all_resources) >= expected_count:
                        logger.info(f"Reached expected count of {expected_count} resources")
                        break
                    
                    # Update payload with next record key
                    if next_record_key:
                        payload["nextRecordKey"] = next_record_key
                    else:
                        logger.info("No more pages to fetch")
                    
                    page_count += 1
                    break
                    
                elif response.status_code == 429:
                    wait_time = min(2 ** retry, 30)
                    logger.warning(f"Rate limited (429). Waiting {wait_time}s before retry {retry+1}/{max_retries}.")
                    time.sleep(wait_time)
                    if retry == max_retries:
                        logger.error("Max retries reached for rate limiting.")
                        break
                        
                else:
                    logger.error(f"API error: {response.status_code} - {response.text}")
                    if retry < max_retries:
                        wait_time = min(2 ** retry, 30)
                        logger.info(f"Retrying in {wait_time}s... (Attempt {retry+1}/{max_retries})")
                        time.sleep(wait_time)
                    else:
                        logger.error(f"Failed after {max_retries} retries")
                        break
                        
            except Exception as e:
                logger.error(f"Request failed: {str(e)}")
                if retry < max_retries:
                    wait_time = min(2 ** retry, 30)
                    logger.info(f"Retrying in {wait_time}s... (Attempt {retry+1}/{max_retries})")
                    time.sleep(wait_time)
                else:
                    logger.error(f"Failed after {max_retries} retries")
                    break
        
        # Exit if no more pages or we've reached the limit
        if not next_record_key:
            break
    
    total_time = (datetime.now() - start_time).total_seconds()
    logger.info(f"Fetch complete: {len(all_resources)} resources in {page_count} pages, {total_time:.1f} seconds")
    
    return all_resources, origin_counts

def main():
    # Fetch all resources and analyze origins
    resources, origin_counts = fetch_resources()
    
    # Print summary with total count
    print(f"\nTotal Resources Found: {len(resources)}")
    print("\nOrigin Value Distribution:")
    print("-" * 30)
    for origin, count in origin_counts.items():
        percentage = (count / len(resources)) * 100
        print(f"{origin}: {count} ({percentage:.1f}%)")
    
    # Print sample resources
    print("\nSample Resources (first 5):")
    print("-" * 30)
    for resource in resources[:5]:
        print(f"Resource ID: {resource['resourceId']}")
        print(f"Account: {resource['awsAccountId']}")
        print(f"Region: {resource['awsRegion']}")
        print(f"Origin: {resource['extracted_origin']}")
        print("-" * 30)

if __name__ == "__main__":
    main()
