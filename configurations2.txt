import requests
import json
import logging
import time
from typing import Dict, List, Optional, Tuple
from datetime import datetime

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# API Configuration
CONFIG_URL = "https://api.cloud.[company].com/internal-operations/cloud-service/aws-tooling/search-resource-configurations"
HEADERS = {
    "Accept": "application/json;v=1.0",
    "Authorization": "Bearer your_actual_token_here",  # Replace with your token
    "Content-Type": "application/json"
}

def get_summary_count(payload: Dict, timeout: int = 30, max_retries: int = 2) -> Optional[int]:
    """Fetch the total count of resources from the summary-view API."""
    logger.info("Fetching summary count...")
    
    for retry in range(max_retries + 1):
        try:
            response = requests.post(
                CONFIG_URL.replace('search-resource-configurations', 'summary-view'),
                headers=HEADERS,
                json=payload,
                verify=False,
                timeout=timeout
            )
            
            if response.status_code == 200:
                data = response.json()
                level1_list = data.get("level1List", [])
                if not level1_list:
                    logger.warning("Empty level1List in response")
                    return 0
                    
                count = level1_list[0].get("level1ResourceCount", 0)
                logger.info(f"Summary count: {count}")
                return count
            
            elif response.status_code == 429:
                wait_time = min(2 ** retry, 30)
                logger.warning(f"Rate limited (429). Waiting {wait_time}s before retry {retry+1}/{max_retries}.")
                time.sleep(wait_time)
            else:
                logger.error(f"Summary API failed: {response.status_code} - {response.text}")
                if retry < max_retries:
                    time.sleep(min(2 ** retry, 15))
                
        except Exception as e:
            logger.error(f"Summary API request failed: {str(e)}")
            if retry < max_retries:
                time.sleep(min(2 ** retry, 15))
    
    return None

def fetch_resources(timeout: int = 60, max_retries: int = 3, page_size: int = 10000) -> Tuple[List[Dict], Dict[str, int]]:
    """
    Fetch all KMS resources using pagination API with nextRecordKey as query param.

    Args:
        timeout: Request timeout in seconds
        max_retries: Maximum number of retry attempts
        page_size: Number of resources to fetch per page (max 10000)

    Returns:
        Tuple containing list of resources and dictionary of origin value counts
    """
    all_resources = []
    origin_counts = {}
    unique_resource_ids = set()
    next_record_key = ""
    page_count = 0

    # Configure debug logging
    debug_logger = logging.getLogger('pagination_debug')
    debug_logger.setLevel(logging.DEBUG)
    fh = logging.FileHandler('pagination_debug.log')
    fh.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
    debug_logger.addHandler(fh)

    debug_logger.info("=== Starting new fetch_resources call ===")
    debug_logger.info(f"Parameters: timeout={timeout}, max_retries={max_retries}, page_size={page_size}")

    # Get expected total count
    payload = {"searchParameters": [{"resourceType": "AWS::KMS::Key"}]}
    expected_count = get_summary_count(payload)
    debug_logger.info(f"Expected total resources: {expected_count if expected_count is not None else 'Unknown'}")

    # Base payload (no limit or nextRecordKey here)
    payload = {
        "searchParameters": [{"resourceType": "AWS::KMS::Key"}],
        "responseFields": [
            "accountName", "accountResourceId", "amazonResourceName", "asvName",
            "awsAccountId", "awsRegion", "businessApplicationName",
            "environment", "resourceCreationTimestamp", "resourceId",
            "resourceType", "configurationList", "configuration.origin"
        ]
    }

    start_time = datetime.now()

    while True:
        logger.info(f"Processing page {page_count + 1} with nextRecordKey: '{next_record_key or 'None'}'")

        for retry in range(max_retries + 1):
            # Fresh params for each retry
            params = {"limit": min(page_size, 10000)}  # Cap at API max
            if next_record_key:
                params["nextRecordKey"] = next_record_key

            try:
                logger.info(f"Requesting page {page_count + 1}" + (f" (retry {retry})" if retry > 0 else ""))
                # Log the full URL for debugging
                full_url = requests.Request('POST', CONFIG_URL, headers=HEADERS, json=payload, params=params).prepare().url
                debug_logger.debug(f"Full URL: {full_url}")
                debug_logger.debug(f"Payload: {json.dumps(payload, indent=2)}")

                response = requests.post(
                    CONFIG_URL,
                    headers=HEADERS,
                    json=payload,
                    params=params,
                    verify=False,
                    timeout=timeout
                )
                debug_logger.debug(f"Response status code: {response.status_code}")
                debug_logger.debug(f"Raw response content: {response.text[:1000]}...")

                if response.status_code == 200:
                    data = response.json()
                    resources = data.get("resourceConfigurations", [])
                    new_next_record_key = data.get("nextRecordKey", "")

                    debug_logger.info(f"Page {page_count + 1}: {len(resources)} resources, nextRecordKey: '{new_next_record_key or 'None'}'")

                    # Process resources
                    new_resources_count = 0
                    for resource in resources:
                        resource_id = resource.get('resourceId')
                        if resource_id in unique_resource_ids:
                            debug_logger.warning(f"Duplicate resource detected - ID: {resource_id}")
                            continue
                        unique_resource_ids.add(resource_id)
                        new_resources_count += 1

                        # Extract origin value
                        origin = "Unknown"
                        config_list = resource.get("configurationList", [])
                        for config in config_list:
                            if config["configurationName"] == "configuration.origin":
                                origin = config["configurationValue"]
                                break
                        origin_counts[origin] = origin_counts.get(origin, 0) + 1
                        resource["extracted_origin"] = origin
                        all_resources.append(resource)

                    # Update nextRecordKey
                    next_record_key = new_next_record_key

                    debug_logger.info(f"Page {page_count + 1} complete: {new_resources_count} new resources, total: {len(all_resources)}")
                    if expected_count:
                        debug_logger.info(f"Progress: {len(unique_resource_ids)}/{expected_count} resources")

                    # Stop if no more pages
                    if not next_record_key:
                        debug_logger.info("No nextRecordKey in response - pagination complete")
                        break

                    page_count += 1
                    if page_count > 100:
                        debug_logger.error("Exceeded maximum expected pages - stopping")
                        break

                    break  # Exit retry loop on success

                elif response.status_code == 429:
                    wait_time = min(2 ** retry, 30)
                    logger.warning(f"Rate limited (429). Waiting {wait_time}s before retry {retry+1}/{max_retries}.")
                    time.sleep(wait_time)
                else:
                    logger.error(f"API error: {response.status_code} - {response.text}")
                    if retry < max_retries:
                        time.sleep(min(2 ** retry, 30))
                    else:
                        break

            except Exception as e:
                logger.error(f"Request failed: {str(e)}")
                if retry < max_retries:
                    time.sleep(min(2 ** retry, 30))
                else:
                    break

        if not next_record_key or page_count > 100:
            break

    total_time = (datetime.now() - start_time).total_seconds()
    debug_logger.info(f"Completed: {len(all_resources)} resources in {page_count + 1} pages, {total_time:.1f}s")
    debug_logger.info(f"Origin distribution: {json.dumps(origin_counts, indent=2)}")

    logger.info(f"Fetch complete: {len(all_resources)} resources in {page_count + 1} pages, {total_time:.1f} seconds")
    return all_resources, origin_counts

def main():
    resources, origin_counts = fetch_resources()
    
    print(f"\nTotal Resources Found: {len(resources)}")
    print("\nOrigin Value Distribution:")
    print("-" * 30)
    for origin, count in origin_counts.items():
        percentage = (count / len(resources)) * 100 if resources else 0
        print(f"{origin}: {count} ({percentage:.1f}%)")
    
    print("\nSample Resources (first 5):")
    print("-" * 30)
    for resource in resources[:5]:
        print(f"Resource ID: {resource['resourceId']}")
        print(f"Account: {resource['awsAccountId']}")
        print(f"Region: {resource['awsRegion']}")
        print(f"Origin: {resource['extracted_origin']}")
        print("-" * 30)

if __name__ == "__main__":
    main()
