def get_summary_count(payload: Dict, timeout: int = 30, max_retries: int = 2) -> Optional[int]:
    """Fetch the total count of resources from the summary-view API."""
    logger.info("Fetching summary count...")
    
    for retry in range(max_retries + 1):
        try:
            response = requests.post(
                CONFIG_URL.replace('search_resource_configurations', 'summary_view'),
                headers=HEADERS,
                json=payload,
                verify=False,
                timeout=timeout
            )
            
            if response.status_code == 200:
                data = response.json()
                level1_list = data.get("level1List", [])
                if not level1_list:
                    logger.warning("Empty level1List in response")
                    return 0
                    
                count = level1_list[0].get("level1ResourceCount", 0)
                logger.info(f"Summary count: {count}")
                return count
            
            elif response.status_code == 429:
                wait_time = min(2 ** retry, 30)
                logger.warning(f"Rate limited (429). Waiting {wait_time}s before retry {retry+1}/{max_retries}.")
                time.sleep(wait_time)
            else:
                logger.error(f"Summary API failed: {response.status_code} - {response.text}")
                if retry < max_retries:
                    time.sleep(min(2 ** retry, 15))
                
        except Exception as e:
            logger.error(f"Summary API request failed: {str(e)}")
            if retry < max_retries:
                time.sleep(min(2 ** retry, 15))
    
    return None

def fetch_resources(timeout: int = 60, max_retries: int = 3, page_size: int = 1000) -> Tuple[List[Dict], Dict[str, int]]:
    """
    Fetch all KMS resources using the pagination API with enhanced handling.

    Args:
        timeout: Request timeout in seconds
        max_retries: Maximum number of retry attempts
        page_size: Number of resources to fetch per page

    Returns:
        Tuple containing list of resources and dictionary of origin value counts
    """
    all_resources = []
    origin_counts = {}
    unique_resource_ids = set()
    next_record_key = ""
    page_count = 0
    consecutive_empty_pages = 0
    max_consecutive_empty = 3

    # Configure debug logging
    debug_logger = logging.getLogger('pagination_debug')
    debug_logger.setLevel(logging.DEBUG)
    fh = logging.FileHandler('pagination_debug.log')
    fh.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
    debug_logger.addHandler(fh)

    debug_logger.info("=== Starting new fetch_resources call ===")
    debug_logger.info(f"Parameters: timeout={timeout}, max_retries={max_retries}, page_size={page_size}")

    # Get expected total count
    summary_payload = {"searchParameters": [{"resourceType": "AWS::KMS::Key"}]}
    expected_count = get_summary_count(summary_payload)
    debug_logger.info(f"Expected total resources: {expected_count if expected_count is not None else 'Unknown'}")

    start_time = datetime.now()

    while True:
        current_page_key = next_record_key  # Key used for this request

        # Prepare fresh payload for each request
        payload = {
            "searchParameters": [{"resourceType": "AWS::KMS::Key"}],
            "responseFields": [
                "accountName", "accountResourceId", "awsAccountId", "awsRegion",
                "environment", "resourceCreationTimestamp", "resourceId", "resourceType",
                "configuration.origin"
            ],
            "limit": page_size
        }
        if next_record_key:
            payload["nextRecordKey"] = next_record_key

        logger.info(f"Fetching page {page_count + 1} with nextRecordKey: '{next_record_key}'")
        debug_logger.debug(f"Request payload: {json.dumps(payload, indent=2)}")

        for retry in range(max_retries + 1):
            try:
                response = requests.post(
                    CONFIG_URL, headers=HEADERS, json=payload, verify=False, timeout=timeout
                )
                debug_logger.debug(f"Response status: {response.status_code}, headers: {dict(response.headers)}")

                if response.status_code == 200:
                    data = response.json()
                    resources = data.get("resourceConfigurations", [])
                    new_next_record_key = data.get("nextRecordKey", "")

                    # Log page details
                    debug_logger.info(f"Page {page_count + 1}: {len(resources)} resources, nextRecordKey: '{new_next_record_key}'")

                    # Check for empty pages
                    if not resources:
                        consecutive_empty_pages += 1
                        debug_logger.warning(f"Empty page {consecutive_empty_pages}/{max_consecutive_empty}")
                        if consecutive_empty_pages >= max_consecutive_empty:
                            debug_logger.error("Too many consecutive empty pages, stopping")
                            next_record_key = ""  # Force exit
                            break
                    else:
                        consecutive_empty_pages = 0

                    # Process resources
                    for resource in resources:
                        resource_id = resource.get("resourceId")
                        if resource_id in unique_resource_ids:
                            debug_logger.warning(f"Duplicate resource ID: {resource_id} on page {page_count + 1}")
                            continue
                        unique_resource_ids.add(resource_id)

                        # Extract origin
                        origin = "Unknown"
                        for config in resource.get("configurationList", []):
                            if config["configurationName"] == "configuration.origin":
                                origin = config["configurationValue"]
                                break
                        origin_counts[origin] = origin_counts.get(origin, 0) + 1
                        resource["extracted_origin"] = origin
                        all_resources.append(resource)

                    # Check for duplicate nextRecordKey
                    if new_next_record_key == current_page_key and current_page_key:
                        debug_logger.warning(f"Same nextRecordKey '{new_next_record_key}' returned, stopping")
                        next_record_key = ""  # Force exit
                        break
                    next_record_key = new_next_record_key

                    # Exit if no more pages
                    if not next_record_key:
                        debug_logger.info("No nextRecordKey, pagination complete")
                        break

                    page_count += 1
                    if page_count > 100:
                        debug_logger.error("Exceeded 100 pages, possible pagination issue")
                        break

                    # Log progress
                    debug_logger.info(f"Progress: {len(unique_resource_ids)}/{expected_count or 'Unknown'} resources")
                    break  # Successful request, exit retry loop

                elif response.status_code == 429:
                    wait_time = min(2 ** retry, 30)
                    logger.warning(f"Rate limited, retry {retry+1}/{max_retries} after {wait_time}s")
                    time.sleep(wait_time)
                else:
                    logger.error(f"API error: {response.status_code} - {response.text}")
                    if retry < max_retries:
                        time.sleep(min(2 ** retry, 30))
                    else:
                        break

            except Exception as e:
                logger.error(f"Request failed: {e}")
                if retry < max_retries:
                    time.sleep(min(2 ** retry, 30))
                else:
                    break

        if not next_record_key or page_count > 100:
            break

    total_time = (datetime.now() - start_time).total_seconds()
    debug_logger.info(f"Completed: {len(all_resources)} resources, {page_count} pages, {total_time:.1f}s")
    debug_logger.info(f"Origin counts: {json.dumps(origin_counts, indent=2)}")

    return all_resources, origin_counts
def main():
    # Fetch all resources and analyze origins
    resources, origin_counts = fetch_resources()
    
    # Print summary with total count
    print(f"\nTotal Resources Found: {len(resources)}")
    print("\nOrigin Value Distribution:")
    print("-" * 30)
    for origin, count in origin_counts.items():
        percentage = (count / len(resources)) * 100
        print(f"{origin}: {count} ({percentage:.1f}%)")
    
    # Print sample resources
    print("\nSample Resources (first 5):")
    print("-" * 30)
    for resource in resources[:5]:
        print(f"Resource ID: {resource['resourceId']}")
        print(f"Account: {resource['awsAccountId']}")
        print(f"Region: {resource['awsRegion']}")
        print(f"Origin: {resource['extracted_origin']}")
        print("-" * 30)

if __name__ == "__main__":
    main()
