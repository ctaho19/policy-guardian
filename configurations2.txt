# COMMAND ----------
# MAGIC %md
# MAGIC ## 8. Tier 2 Compliance Analysis
# MAGIC Analyze compliance status of evaluated roles

try:
    # Get thresholds for Tier 2
    tier2_threshold_query = """
    SELECT 
        MONITORING_METRIC_ID,
        ALERT_THRESHOLD,
        WARNING_THRESHOLD
    FROM EIAM_DB.PHDP_CYBR_IAM.CYBER_CONTROLS_MONITORING_THRESHOLD
    WHERE MONITORING_METRIC_ID = 'MNTR-XXXXX-T2'
    """
    
    tier2_thresholds_df = spark.read.format(SNOWFLAKE_SOURCE_NAME) \
        .options(**sfOptions) \
        .option("query", tier2_threshold_query) \
        .load()
    
    # Extract threshold values
    tier2_alert_threshold = tier2_thresholds_df.collect()[0]['ALERT_THRESHOLD']
    tier2_warning_threshold = tier2_thresholds_df.collect()[0]['WARNING_THRESHOLD']
    
    # Filter for evaluated roles and calculate compliance
    evaluated_roles = df_filtered.filter(col("IS_EVALUATED") == 1)
    compliant_roles = evaluated_roles.filter(
        col("COMPLIANCE_STATUS").rlike("^Compliant.*")  # Matches any status starting with "Compliant"
    )
    
    # Calculate Tier 2 metrics
    tier2_numerator = compliant_roles.count()
    tier2_denominator = evaluated_roles.count()  # Should match Tier 1 numerator
    tier2_metric = round(100.0 * tier2_numerator / tier2_denominator, 2) if tier2_denominator > 0 else 0.0
    
    # Determine compliance status based on thresholds
    tier2_compliance_status = "RED" if tier2_metric <= tier2_warning_threshold else \
                            "YELLOW" if tier2_metric <= tier2_alert_threshold else \
                            "GREEN"
    
    # Create Tier 2 metrics DataFrame
    tier2_metrics_data = [(
        current_date(), 
        'MNTR-XXXXX-T2',
        tier2_metric,
        tier2_compliance_status,
        tier2_numerator,
        tier2_denominator
    )]
    
    tier2_metrics_schema = StructType([
        StructField("DATE", DateType(), False),
        StructField("MONITORING_METRIC_NUMBER", StringType(), False),
        StructField("MONITORING_METRIC", DoubleType(), False),
        StructField("COMPLIANCE_STATUS", StringType(), False),
        StructField("NUMERATOR", LongType(), False),
        StructField("DENOMINATOR", LongType(), False)
    ])
    
    df_tier2_result = spark.createDataFrame(tier2_metrics_data, tier2_metrics_schema)
    
    # Create supporting evidence DataFrame
    evidence_schema = StructType([
        StructField("RESOURCE_ID", StringType(), True),
        StructField("ARN", StringType(), True),
        StructField("ACCOUNT", StringType(), True),
        StructField("BA", StringType(), True),
        StructField("CREATE_DATE", TimestampType(), True),
        StructField("ROLE_TYPE", StringType(), True),
        StructField("NOTES", StringType(), True)
    ])
    
    if tier2_numerator < tier2_denominator:
        # Get non-compliant roles
        df_tier2_evidence = evaluated_roles.filter(~col("COMPLIANCE_STATUS").rlike("^Compliant.*")) \
            .select(
                col("RESOURCE_ID"),
                col("AMAZON_RESOURCE_NAME").alias("ARN"),
                col("ACCOUNT"),
                col("BA"),
                col("CREATE_DATE"),
                col("ROLE_TYPE"),
                col("COMPLIANCE_STATUS").alias("NOTES")
            ) \
            .orderBy(["ACCOUNT", "AMAZON_RESOURCE_NAME"])
    else:
        # All evaluated roles are compliant
        df_tier2_evidence = spark.createDataFrame([
            (None, None, None, None, None, None, "All Evaluated Roles are Compliant")
        ], evidence_schema)
    
    # Display results
    log_step("Tier 2 Metrics")
    df_tier2_result.show()
    
    log_step("Tier 2 Supporting Evidence")
    df_tier2_evidence.show(20, truncate=False)
    
except Exception as e:
    log_step("ERROR in Tier 2 analysis", 
             f"Failed to generate Tier 2 metrics: {str(e)}")
    raise
