import requests
import json
import os
import datetime
import csv # Import the csv module
from dotenv import load_dotenv

# --- Configuration & Setup ---
load_dotenv() # Load variables from .env file

# CloudRadar API Details from Environment Variables
API_TOKEN = os.getenv("CLOUDRADAR_API_TOKEN")
API_URL = "https://api.cloud.capitalone.com/internal-operations/cloud-service/aws-tooling/search-resource-configurations"
API_HEADERS = {
    'Accept': 'application/json;v=1.0',
    'Authorization': f'Bearer {API_TOKEN}',
    'Content-Type': 'application/json'
}
API_LIMIT = 10000
DATASET_CSV_FILE = "dataset_arns.csv" # <--- Name of your CSV file
CSV_ARN_COLUMN_NAME = "CERTIFICATE_ARN" # <--- Column header in your CSV
OUTPUT_FILE = "tier1_test_results.txt" # Optional: File to write results

# Check if essential variables are loaded
if not API_TOKEN:
    print("ERROR: Missing CLOUDRADAR_API_TOKEN in .env file or environment variables.")
    exit(1)

# --- Helper Function to Fetch CloudRadar ARNs with Pagination (Same as before) ---
def fetch_cloudradar_arns(url, headers, limit):
    """Fetches all AWS::ACM::Certificate ARNs issued by Amazon from CloudRadar, handling pagination."""
    all_arns = set() # Using a set for automatic deduplication
    next_key = None
    page_count = 0

    while True:
        page_count += 1
        print(f"Fetching page {page_count} from CloudRadar API...")
        payload = {
            "responseFields": ["amazonResourceName"],
            "searchParameters": [{
                "resourceType": "AWS::ACM::Certificate",
                "configurationList": [{
                    "configurationName": "ConfigurationObject.issuer",
                    "configurationValue": "Amazon"
                }]
            }],
            "limit": limit
        }
        if next_key:
            payload["nextRecordKey"] = next_key

        try:
            response = requests.post(url, headers=headers, data=json.dumps(payload), timeout=180)
            response.raise_for_status()
            data = response.json()

            resources = data.get("resourceConfigurations", [])
            fetched_count = 0
            if resources:
                for config in resources:
                    if config and "amazonResourceName" in config and config["amazonResourceName"]:
                        all_arns.add(config["amazonResourceName"])
                        fetched_count += 1

            next_key = data.get("nextRecordKey")
            print(f"Page {page_count} fetched. Found {fetched_count} valid items. Next key exists: {bool(next_key)}")

            if not next_key:
                break

        except requests.exceptions.Timeout:
            print(f"ERROR: CloudRadar API request timed out on page {page_count}.")
            print("Returning ARNs fetched so far (might be incomplete).")
            return all_arns, False # Indicate failure

        except requests.exceptions.RequestException as e:
            print(f"ERROR: CloudRadar API request failed: {e}")
            if hasattr(e, 'response') and e.response is not None:
                print(f"Response Status Code: {e.response.status_code}")
                print(f"Response Text: {e.response.text[:500]}...")
            print("Returning ARNs fetched so far (might be incomplete).")
            return all_arns, False # Indicate failure

        except json.JSONDecodeError as e:
            print(f"ERROR: Failed to decode JSON response from CloudRadar API: {e}")
            if 'response' in locals():
                 print(f"Response Text: {response.text[:500]}...")
            return all_arns, False # Indicate failure

    print(f"Finished fetching from CloudRadar. Total distinct ARNs found: {len(all_arns)}")
    return all_arns, True # Return True for success

# --- Helper Function to Load Dataset ARNs from CSV ---
def load_dataset_arns_from_csv(filepath, arn_column_name):
    """Loads ARNs from a CSV file, expecting a specific column header."""
    dataset_arns = set()
    print(f"\nLoading dataset ARNs from CSV file: {filepath}")
    try:
        if not os.path.exists(filepath):
            print(f"ERROR: Dataset CSV file not found at '{filepath}'")
            return dataset_arns, False

        with open(filepath, mode='r', newline='', encoding='utf-8') as csvfile:
            reader = csv.reader(csvfile)
            try:
                header = next(reader) # Read the header row
            except StopIteration:
                print(f"ERROR: CSV file '{filepath}' appears to be empty.")
                return dataset_arns, False # Empty file

            # Find the index of the ARN column
            try:
                arn_column_index = header.index(arn_column_name)
                print(f"Found ARN column '{arn_column_name}' at index {arn_column_index}.")
            except ValueError:
                print(f"ERROR: Column '{arn_column_name}' not found in CSV header: {header}")
                print(f"Please ensure the CSV file '{filepath}' has the correct header.")
                return dataset_arns, False

            # Read the rest of the rows
            row_count = 0
            valid_arn_count = 0
            for row in reader:
                row_count += 1
                if len(row) > arn_column_index:
                    arn = row[arn_column_index].strip()
                    if arn and arn.startswith("arn:aws:acm"): # Basic validation
                        dataset_arns.add(arn)
                        valid_arn_count += 1
                # else: # Optional: warn about short rows
                #     print(f"Warning: Row {row_count + 1} has fewer columns than expected: {row}")


        print(f"Processed {row_count} data rows from CSV.")
        print(f"Loaded {len(dataset_arns)} distinct and valid ACM ARNs from CSV file.")
        return dataset_arns, True
    except Exception as e:
        print(f"ERROR reading dataset CSV file '{filepath}': {e}")
        return dataset_arns, False

# --- Main Execution ---

# 1. Fetch from CloudRadar
print("--- Step 1: Fetching Data from CloudRadar ---")
cloudradar_arns_set, radar_success = fetch_cloudradar_arns(API_URL, API_HEADERS, API_LIMIT)
if not radar_success:
    print("\nAborting due to failure fetching CloudRadar data.")
    exit(1)
cloudradar_count = len(cloudradar_arns_set)
print(f"CloudRadar distinct ARN count (Denominator): {cloudradar_count}")

# 2. Load Dataset ARNs from CSV
print("\n--- Step 2: Loading Dataset Data from CSV ---")
dataset_arns_set, load_success = load_dataset_arns_from_csv(DATASET_CSV_FILE, CSV_ARN_COLUMN_NAME)
if not load_success:
    print("\nAborting due to failure loading dataset CSV data.")
    exit(1)
dataset_count = len(dataset_arns_set)
# Note: dataset_count here is just the count loaded from the file, not directly used in T1 metric calculation

# 3. Calculate Tier 1 Metric
print("\n--- Step 3: Calculating Tier 1 Metric ---")
if cloudradar_count > 0:
    # Find intersection (ARNs present in both CloudRadar and the dataset CSV)
    matched_arns_set = cloudradar_arns_set.intersection(dataset_arns_set)
    numerator_t1 = len(matched_arns_set)
    denominator_t1 = float(cloudradar_count)

    # Handle potential division by zero, although checked cloudradar_count > 0
    metric_t1 = round((numerator_t1 * 100.0 / denominator_t1), 2) if denominator_t1 > 0 else 0.0
    # Status check: strict equality needed for 100%
    status_t1 = "GREEN" if numerator_t1 == int(denominator_t1) else "RED"
else:
    print("CloudRadar reported 0 in-scope certificates. Setting Tier 1 to 100% GREEN.")
    numerator_t1 = 0
    denominator_t1 = 0.0
    metric_t1 = 100.0
    status_t1 = "GREEN"

# Format Tier 1 Output
current_date_str = datetime.date.today().isoformat()
tier1_result = {
    "DATE": current_date_str,
    "CTRL_ID": "CTRL-1077188",
    "MONITORING_METRIC_NUMBER": "MNTR-1077188-T1",
    "MONITORING_METRIC": float(metric_t1),
    "COMPLIANCE_STATUS": status_t1,
    "NUMERATOR": float(numerator_t1),
    "DENOMINATOR": float(denominator_t1)
}

# --- Output Results to Terminal ---
print("\n--- Tier 1 Metric Result (Compared to Dataset CSV) ---")
terminal_output = []
terminal_output.append("--- Tier 1 Metric Result ---")
for key, value in tier1_result.items():
    line = f"  {key}: {value}"
    print(line)
    terminal_output.append(line)

terminal_output.append("\nCalculation Summary:")
summary_lines = [
    f"  CloudRadar ARNs (Denominator): {int(denominator_t1)}",
    f"  Matched ARNs in Dataset CSV (Numerator): {int(numerator_t1)}",
    f"  Metric Value: {metric_t1}%",
    f"  Compliance Status: {status_t1}"
]
for line in summary_lines:
    print(line)
    terminal_output.append(line)

# 4. Generate Supporting Evidence (Missing ARNs)
print("\n--- Step 4: Generating Supporting Evidence (ARNs Missing from Dataset CSV) ---")
terminal_output.append("\n--- Supporting Evidence (Missing ARNs) ---")

# Find difference (ARNs in CloudRadar but NOT in Dataset CSV)
missing_arns_set = cloudradar_arns_set.difference(dataset_arns_set)
missing_count = len(missing_arns_set)

if missing_count > 0:
    missing_header = f"\nFound {missing_count} ARN(s) in CloudRadar that are MISSING from the dataset CSV ('{DATASET_CSV_FILE}'):"
    print(missing_header)
    terminal_output.append(missing_header)
    count = 0
    for arn in sorted(list(missing_arns_set)): # Sort for consistent output
        line = f"  - {arn}"
        print(line)
        terminal_output.append(line)
        count += 1
        if count >= 200: # Limit printed output if there are too many
            limit_msg = f"  ... (showing first 200 of {missing_count})"
            print(limit_msg)
            terminal_output.append(limit_msg)
            break
else:
    no_missing_msg = f"\nNo missing ARNs found. All CloudRadar ARNs are present in the dataset CSV ('{DATASET_CSV_FILE}')."
    print(no_missing_msg)
    terminal_output.append(no_missing_msg)

# --- Optionally Write to Output File ---
try:
    with open(OUTPUT_FILE, 'w') as f:
        f.write(f"Report generated on: {datetime.datetime.now().isoformat()}\n")
        f.write(f"Dataset CSV file used: {DATASET_CSV_FILE}\n")
        f.write("="*40 + "\n")
        f.write("\n".join(terminal_output))
        f.write("\n" + "="*40 + "\n")
    print(f"\nResults also written to file: {OUTPUT_FILE}")
except Exception as e:
    print(f"\nWarning: Could not write results to file '{OUTPUT_FILE}': {e}")

print("\n--- Script Execution Finished ---")
