# COMMAND ----------
# MAGIC %md
# MAGIC ## 11. Tier 1 Supporting Evidence
# MAGIC Display details of machine roles that were not evaluated against the control

# COMMAND ----------
from pyspark.sql.functions import col
import logging
from datetime import datetime, date
import pandas as pd
from pyspark.sql.types import StructType, StructField, StringType, TimestampType

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

try:
    # Load filtered and evaluated roles from PySpark DataFrames
    logger.info("Loading filtered and evaluated roles for supporting evidence")
    if not 'df_filtered' in locals() or df_filtered is None:
        df_filtered = spark.table("filtered_machine_roles")
    if not 'df_evaluated_roles' in locals() or df_evaluated_roles is None:
        df_evaluated_roles = spark.table("evaluated_roles")
    
    # Convert to Python lists
    filtered_data = [(row["RESOURCE_ID"], row["AMAZON_RESOURCE_NAME"], row["ACCOUNT"], 
                     row["BA"], row["CREATE_DATE"], row["ROLE_TYPE"]) for row in df_filtered.collect()]
    evaluated_data = [row["RESOURCE_NAME"] for row in df_evaluated_roles.collect()]
    logger.info(f"Filtered data count: {len(filtered_data)}, type: {type(filtered_data)}")
    logger.info(f"Evaluated data count: {len(evaluated_data)}, type: {type(evaluated_data)}")

    # Identify unevaluated roles in Python
    evaluated_set = set(evaluated_data)
    unevaluated_data = [row for row in filtered_data if row[1] not in evaluated_set]  # Index 1 is AMAZON_RESOURCE_NAME
    logger.info(f"Unevaluated data count: {len(unevaluated_data)}, type: {type(unevaluated_data)}")

    # Prepare supporting evidence data as a Python dictionary for pandas
    if unevaluated_data:
        evidence_data = {
            "RESOURCE_ID": [row[0] for row in unevaluated_data],
            "ARN": [row[1] for row in unevaluated_data],
            "ACCOUNT": [row[2] for row in unevaluated_data],
            "BA": [row[3] for row in unevaluated_data],
            "CREATE_DATE": [row[4] for row in unevaluated_data],
            "ROLE_TYPE": [row[5] for row in unevaluated_data],
            "NOTES": [None] * len(unevaluated_data)
        }
    else:
        evidence_data = {
            "RESOURCE_ID": [None],
            "ARN": [None],
            "ACCOUNT": [None],
            "BA": [None],
            "CREATE_DATE": [None],
            "ROLE_TYPE": [None],
            "NOTES": ["All Roles Evaluated Against Control"]
        }
    logger.info(f"Supporting evidence data: {evidence_data}")
    logger.info(f"Supporting evidence data types: {[type(x[0]) if x else None for x in evidence_data.values()]}")

    # Convert to pandas DataFrame and then to PySpark DataFrame
    pd_evidence = pd.DataFrame(evidence_data)
    df_supporting_evidence = spark.createDataFrame(pd_evidence)
    logger.info("Supporting evidence DataFrame created from pandas DataFrame")
    df_supporting_evidence.printSchema()
    logger.info("Supporting evidence DataFrame content:")
    df_supporting_evidence.show(20, truncate=False)

    # Register as temp view
    df_supporting_evidence.createOrReplaceTempView("tier1_evidence")

except Exception as e:
    logger.error(f"ERROR in Tier 1 supporting evidence: {str(e)}")
    raise

# COMMAND ----------
# MAGIC %md
# MAGIC ## Unit Tests for Tier 1 Supporting Evidence

# COMMAND ----------
def test_tier1_supporting_evidence(spark):
    """Unit test for Tier 1 supporting evidence logic and DataFrame creation."""
    # Mock data
    mock_filtered_data = spark.createDataFrame(
        [("res1", "arn1", "acc1", "ba1", datetime(2023, 1, 1), "MACHINE"),
         ("res2", "arn2", "acc2", "ba2", datetime(2023, 1, 2), "MACHINE"),
         ("res3", "arn3", "acc3", "ba3", datetime(2023, 1, 3), "MACHINE")],
        ["RESOURCE_ID", "AMAZON_RESOURCE_NAME", "ACCOUNT", "BA", "CREATE_DATE", "ROLE_TYPE"]
    )
    mock_evaluated_data = spark.createDataFrame(
        [("arn1",), ("arn3",)],
        ["RESOURCE_NAME"]
    )

    # Collect data
    filtered_data = [(row["RESOURCE_ID"], row["AMAZON_RESOURCE_NAME"], row["ACCOUNT"], 
                     row["BA"], row["CREATE_DATE"], row["ROLE_TYPE"]) for row in mock_filtered_data.collect()]
    evaluated_data = [row["RESOURCE_NAME"] for row in mock_evaluated_data.collect()]

    # Identify unevaluated roles
    evaluated_set = set(evaluated_data)
    unevaluated_data = [row for row in filtered_data if row[1] not in evaluated_set]

    # Prepare supporting evidence data
    if unevaluated_data:
        evidence_data = {
            "RESOURCE_ID": [row[0] for row in unevaluated_data],
            "ARN": [row[1] for row in unevaluated_data],
            "ACCOUNT": [row[2] for row in unevaluated_data],
            "BA": [row[3] for row in unevaluated_data],
            "CREATE_DATE": [row[4] for row in unevaluated_data],
            "ROLE_TYPE": [row[5] for row in unevaluated_data],
            "NOTES": [None] * len(unevaluated_data)
        }
    else:
        evidence_data = {
            "RESOURCE_ID": [None],
            "ARN": [None],
            "ACCOUNT": [None],
            "BA": [None],
            "CREATE_DATE": [None],
            "ROLE_TYPE": [None],
            "NOTES": ["All Roles Evaluated Against Control"]
        }

    # Convert to pandas DataFrame and then to PySpark DataFrame
    pd_evidence = pd.DataFrame(evidence_data)
    df_result = spark.createDataFrame(pd_evidence)

    # Assertions
    collected_result = df_result.collect()
    assert len(collected_result) == 1, f"Expected 1 unevaluated row, got {len(collected_result)}"
    assert collected_result[0]["ARN"] == "arn2", f"Expected ARN arn2, got {collected_result[0]['ARN']}"
    assert collected_result[0]["NOTES"] is None, f"Expected NOTES to be None, got {collected_result[0]['NOTES']}"
    logger.info("Unit test for Tier 1 supporting evidence passed!")

# Run unit test
test_tier1_supporting_evidence(spark)
​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​