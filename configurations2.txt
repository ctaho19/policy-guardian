# Databricks notebook source
# MAGIC %md
# MAGIC # IAM Compliance Monitoring Pipeline
# MAGIC This notebook calculates compliance metrics for IAM machine roles across three tiers: coverage (Tier 1), compliance rate (Tier 2), and SLA compliance (Tier 3).

# COMMAND ----------

# MAGIC %md
# MAGIC ## 4. Query Definitions
# MAGIC Define SQL queries for data extraction from Snowflake

# COMMAND ----------
approved_accounts = ["account1", "account2", "account3"]  # Placeholder; replace with actual account list

# Define queries
all_iam_roles_query = """
SELECT 
    RESOURCE_ID,
    AMAZON_RESOURCE_NAME,
    ACCOUNT,
    CREATE_DATE,
    TYPE,
    FULL_RECORD,
    ROLE_TYPE,
    BA,
    SF_LOAD_TIMESTAMP
FROM EIAM_DB.PHDP_CYBR_IAM.IDENTITY_REPORTS_IAM_RESOURCE_V3
WHERE TYPE = 'role'
  AND NOT REGEXP_LIKE(FULL_RECORD, 'DENY-ALL|QUARANTINEPOLICY')
  AND AMAZON_RESOURCE_NAME LIKE 'arn:aws:iam::%role/%'
QUALIFY ROW_NUMBER() OVER (PARTITION BY RESOURCE_ID ORDER BY CREATE_DATE DESC) = 1
  AND (SF_LOAD_TIMESTAMP > '2025-02-19' OR SF_LOAD_TIMESTAMP IS NULL)
"""

filtered_machine_roles_query = f"""
SELECT 
    RESOURCE_ID,
    AMAZON_RESOURCE_NAME,
    ACCOUNT,
    BA,
    ROLE_TYPE
FROM ({all_iam_roles_query})
WHERE ROLE_TYPE = 'MACHINE'
  AND ACCOUNT IN ({','.join([f"'{acc}'" for acc in approved_accounts])})
"""

evaluated_roles_query = """
SELECT DISTINCT 
    UPPER(RESOURCE_NAME) as RESOURCE_NAME,
    COMPLIANCE_STATUS
FROM EIAM_DB.PHDP_CYBR_IAM.IDENTITY_REPORTS_CONTROLS_VIOLATIONS_STREAM_V2
WHERE CONTROL_ID = 'CM-2.AWS.12.v02'
"""

tier2_threshold_query = """
SELECT 
    ALERT_THRESHOLD,
    WARNING_THRESHOLD
FROM CYBR_DB_COLLAB.LAB_ESRA_TCRD.CYBER_CONTROLS_MONITORING_THRESHOLD
WHERE MONITORING_METRIC_ID = 'MNTR-XXXXX-T2'
"""

tier3_threshold_query = """
SELECT 
    ALERT_THRESHOLD,
    WARNING_THRESHOLD
FROM CYBR_DB_COLLAB.LAB_ESRA_TCRD.CYBER_CONTROLS_MONITORING_THRESHOLD
WHERE MONITORING_METRIC_ID = 'MNTR-XXXXX-T3'
"""

non_compliant_resources_query = """
SELECT 
    RESOURCE_ID,
    CONTROL_RISK,
    OPEN_DATE_UTC_TIMESTAMP
FROM CLCN_DB.PHDP_CLOUD.OZONE_NON_COMPLIANT_RESOURCES_TCRD_VIEW_V01
WHERE CONTROL_ID = 'AC-3.AWS.39.v02'
  AND ID NOT IN (
      SELECT ID 
      FROM CLCN_DB.PHDP_CLOUD.OZONE_CLOSED_NON_COMPLIANT_RESOURCES_V04
  )
"""

# COMMAND ----------

# MAGIC %md
# MAGIC ## 8. Tier 1 Metrics Calculation
# MAGIC Calculate coverage metrics for IAM machine roles

# COMMAND ----------
import logging
from datetime import date
import pandas as pd

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def calculate_metrics(alert_val, warning_val, numerator, denominator):
    """Pure Python function to calculate metrics with corrected status logic."""
    alert = float(alert_val) if alert_val is not None else None
    warning = float(warning_val) if warning_val is not None else None
    numerator = int(numerator)
    denominator = int(denominator)
    
    metric = numerator / denominator * 100 if denominator > 0 else 0.0
    metric = round(metric, 2)
    
    status = "GREEN"
    if alert is not None and metric <= alert:
        status = "RED"
    elif warning is not None and metric <= warning and metric > alert:
        status = "YELLOW"
    logger.info(f"Calculated metric: {metric}, status: {status} with alert={alert}, warning={warning}")
    return {
        "metric": metric,
        "status": status,
        "numerator": numerator,
        "denominator": denominator
    }

try:
    logger.info("Loading all IAM roles from Snowflake")
    df_all_iam_roles = spark.read.format("snowflake") \
        .options(**sfOptions) \
        .option("query", all_iam_roles_query) \
        .load()
    
    logger.info(f"df_all_iam_roles count: {df_all_iam_roles.count()}")
    df_all_iam_roles.printSchema()
    logger.info("Sample df_all_iam_roles data:")
    df_all_iam_roles.show(5)
    
    logger.info("Creating all_iam_roles temp view")
    df_all_iam_roles.createOrReplaceTempView("all_iam_roles")
    
    logger.info("Filtering machine roles in approved accounts")
    df_filtered = spark.sql(filtered_machine_roles_query)
    
    logger.info(f"df_filtered count: {df_filtered.count()}")
    df_filtered.printSchema()
    logger.info("Sample df_filtered data:")
    df_filtered.show(5)
    
    logger.info("Creating filtered_machine_roles temp view")
    df_filtered.createOrReplaceTempView("filtered_machine_roles")
    
    logger.info("Loading evaluated roles from Snowflake")
    df_evaluated_roles = spark.read.format("snowflake") \
        .options(**sfOptions) \
        .option("query", evaluated_roles_query) \
        .load()
    
    logger.info(f"df_evaluated_roles count: {df_evaluated_roles.count()}")
    df_evaluated_roles.printSchema()
    logger.info("Sample df_evaluated_roles data:")
    df_evaluated_roles.show(5)
    
    logger.info("Creating evaluated_roles temp view")
    df_evaluated_roles.createOrReplaceTempView("evaluated_roles")
    
    filtered_data = [(row["RESOURCE_ID"], row["AMAZON_RESOURCE_NAME"], row["ACCOUNT"], 
                     row["BA"], row["ROLE_TYPE"]) for row in df_filtered.collect()]
    evaluated_data = [(row["RESOURCE_NAME"], row["COMPLIANCE_STATUS"]) for row in df_evaluated_roles.collect()]
    logger.info(f"Filtered data count: {len(filtered_data)}, type: {type(filtered_data)}")
    logger.info(f"Evaluated data count: {len(evaluated_data)}, type: {type(evaluated_data)}")
    logger.info(f"Sample evaluated_data: {evaluated_data[:5]}")

    filtered_map = {row[1]: (row[0], row[2], row[3], row[4]) for row in filtered_data}
    evaluated_compliance = {arn: status for arn, status in evaluated_data}
    evaluated_roles = [
        (resource_id, arn, acc, ba, role_type, status) 
        for arn, status in evaluated_compliance.items() 
        if arn in filtered_map 
        for resource_id, acc, ba, role_type in [filtered_map[arn]]
    ]
    logger.info(f"Evaluated roles count: {len(evaluated_roles)}, type: {type(evaluated_roles)}")
    logger.info(f"Sample evaluated_roles: {evaluated_roles[:5]}")

    evaluated_count = len(evaluated_roles)
    total_count = df_filtered.count()
    logger.info(f"Evaluated count: {evaluated_count}, type: {type(evaluated_count)}")
    logger.info(f"Total count: {total_count}, type: {type(total_count)}")

    results = calculate_metrics(None, None, evaluated_count, total_count)
    logger.info(f"Calculation results: {results}, type: {type(results)}")

    current_date_value = date.today()
    logger.info(f"Current date: {current_date_value}, type: {type(current_date_value)}")

    metrics_data = {
        "DATE": [current_date_value],
        "MONITORING_METRIC_NUMBER": ['MNTR-XXXXX'],
        "MONITORING_METRIC": [results["metric"]],
        "COMPLIANCE_STATUS": [results["status"]],
        "NUMERATOR": [results["numerator"]],
        "DENOMINATOR": [results["denominator"]]
    }
    logger.info(f"Metrics data: {metrics_data}")
    logger.info(f"Metrics data types: {[type(x[0]) for x in metrics_data.values()]}")

    pd_df = pd.DataFrame(metrics_data)
    df_result = spark.createDataFrame(pd_df)
    logger.info("DataFrame created from pandas DataFrame")
    df_result.printSchema()
    logger.info("DataFrame content:")
    df_result.show()

    df_result.createOrReplaceTempView("tier1_metrics")
    
    logger.info(f"Setting spark.tier1_numerator to {results['numerator']}")
    spark.conf.set("spark.tier1_numerator", str(results['numerator']))
    logger.info(f"Retrieved spark.tier1_numerator: {spark.conf.get('spark.tier1_numerator')}")

    logger.info(f"Setting spark.tier1_denominator to {results['denominator']}")
    spark.conf.set("spark.tier1_denominator", str(results['denominator']))
    logger.info(f"Retrieved spark.tier1_denominator: {spark.conf.get('spark.tier1_denominator')}")

    logger.info(f"Setting spark.tier1_metric_value to {results['metric']}")
    spark.conf.set("spark.tier1_metric_value", str(results['metric']))
    logger.info(f"Retrieved spark.tier1_metric_value: {spark.conf.get('spark.tier1_metric_value')}")

    logger.info(f"Setting spark.tier1_compliance_status to {results['status']}")
    spark.conf.set("spark.tier1_compliance_status", results['status'])
    logger.info(f"Retrieved spark.tier1_compliance_status: {spark.conf.get('spark.tier1_compliance_status')}")

except Exception as e:
    logger.error(f"ERROR in Tier 1 metrics calculation: {str(e)}")
    raise

# COMMAND ----------

# MAGIC %md
# MAGIC ## 12. Tier 2 Metrics Calculation
# MAGIC Calculate compliance metrics for evaluated roles using real COMPLIANCE_STATUS data

# COMMAND ----------
def calculate_metrics(tier2_alert_threshold_raw, tier2_warning_threshold_raw, compliant_count, evaluated_count):
    """Pure Python function to calculate metrics with corrected status logic."""
    alert = float(tier2_alert_threshold_raw) if tier2_alert_threshold_raw is not None else None
    warning = float(tier2_warning_threshold_raw) if tier2_warning_threshold_raw is not None else None
    compliant = int(compliant_count)
    evaluated = int(evaluated_count)
    
    if compliant > evaluated:
        logger.error(f"Invalid counts: compliant_count ({compliant}) exceeds evaluated_count ({evaluated})")
        raise ValueError("Compliant count cannot exceed evaluated count")
    
    metric = compliant / evaluated * 100 if evaluated > 0 else 0.0
    metric = round(metric, 2)
    
    status = "GREEN"
    if alert is not None and metric <= alert:
        status = "RED"
    elif warning is not None and metric <= warning and metric > alert:
        status = "YELLOW"
    logger.info(f"Calculated metric: {metric}, status: {status} with alert={alert}, warning={warning}")
    return {
        "metric": metric,
        "status": status,
        "numerator": compliant,
        "denominator": evaluated
    }

try:
    logger.info("Loading Tier 2 thresholds from Snowflake")
    tier2_thresholds_df = spark.read.format(SNOWFLAKE_SOURCE_NAME) \
        .options(**sfOptions) \
        .option("query", tier2_threshold_query) \
        .load()
    
    tier2_thresholds_df.show()
    threshold_data = tier2_thresholds_df.collect()
    logger.info(f"Threshold data collected: {threshold_data}, type: {type(threshold_data)}")
    
    tier2_alert_threshold_raw = threshold_data[0]["ALERT_THRESHOLD"] if threshold_data else None
    tier2_warning_threshold_raw = threshold_data[0]["WARNING_THRESHOLD"] if threshold_data else None
    logger.info(f"Tier 2 Alert threshold: {tier2_alert_threshold_raw}, type: {type(tier2_alert_threshold_raw)}")
    logger.info(f"Tier 2 Warning threshold: {tier2_warning_threshold_raw}, type: {type(tier2_warning_threshold_raw)}")

    logger.info("Loading filtered and evaluated roles")
    if not 'df_filtered' in locals() or df_filtered is None:
        df_filtered = spark.table("filtered_machine_roles")
    if not 'df_evaluated_roles' in locals() or df_evaluated_roles is None:
        df_evaluated_roles = spark.table("evaluated_roles")
    
    logger.info(f"df_filtered count: {df_filtered.count()}")
    logger.info("df_filtered schema: ")
    df_filtered.printSchema()
    logger.info("Sample df_filtered data: ")
    df_filtered.show(5)
    
    logger.info(f"df_evaluated_roles count: {df_evaluated_roles.count()}")
    logger.info("df_evaluated_roles schema: ")
    df_evaluated_roles.printSchema()
    logger.info("Sample df_evaluated_roles data: ")
    df_evaluated_roles.show(5)
    
    filtered_data = [(row["RESOURCE_ID"], row["AMAZON_RESOURCE_NAME"], row["ACCOUNT"], 
                     row["BA"], row["ROLE_TYPE"]) for row in df_filtered.collect()]
    evaluated_data = [(row["RESOURCE_NAME"], row["COMPLIANCE_STATUS"]) for row in df_evaluated_roles.collect()]
    logger.info(f"Filtered data count: {len(filtered_data)}, type: {type(filtered_data)}")
    logger.info(f"Evaluated data count: {len(evaluated_data)}, type: {type(evaluated_data)}")
    logger.info(f"Sample evaluated_data: {evaluated_data[:5]}")

    filtered_map = {row[1]: (row[0], row[2], row[3], row[4]) for row in filtered_data}
    evaluated_compliance = {arn: status for arn, status in evaluated_data}
    evaluated_roles = [
        (resource_id, arn, acc, ba, role_type, status) 
        for arn, status in evaluated_compliance.items() 
        if arn in filtered_map 
        for resource_id, acc, ba, role_type in [filtered_map[arn]]
    ]
    logger.info(f"Evaluated roles count: {len(evaluated_roles)}, type: {type(evaluated_roles)}")
    logger.info(f"Sample evaluated_roles: {evaluated_roles[:5]}")

    # Log the distribution of COMPLIANCE_STATUS
    compliance_statuses = [status for _, _, _, _, _, status in evaluated_roles]
    from collections import Counter
    status_counts = Counter(compliance_statuses)
    logger.info(f"COMPLIANCE_STATUS distribution: {dict(status_counts)}")

    compliant_count = len([status for _, _, _, _, _, status in evaluated_roles 
                         if status not in ["NonCompliant"]])
    evaluated_count = len(evaluated_roles)
    logger.info(f"Compliant count: {compliant_count}, type: {type(compliant_count)}")
    logger.info(f"Evaluated count: {evaluated_count}, type: {type(evaluated_count)}")

    results = calculate_metrics(tier2_alert_threshold_raw, tier2_warning_threshold_raw, compliant_count, evaluated_count)
    logger.info(f"Calculation results: {results}, type: {type(results)}")

    current_date_value = date.today()
    logger.info(f"Current date: {current_date_value}, type: {type(current_date_value)}")

    metrics_data = {
        "DATE": [current_date_value],
        "MONITORING_METRIC_NUMBER": ['MNTR-XXXXX-T2'],
        "MONITORING_METRIC": [results["metric"]],
        "COMPLIANCE_STATUS": [results["status"]],
        "NUMERATOR": [results["numerator"]],
        "DENOMINATOR": [results["denominator"]]
    }
    logger.info(f"Metrics data: {metrics_data}")
    logger.info(f"Metrics data types: {[type(x[0]) for x in metrics_data.values()]}")

    pd_df = pd.DataFrame(metrics_data)
    df_result = spark.createDataFrame(pd_df)
    logger.info("DataFrame created from pandas DataFrame")
    df_result.printSchema()
    logger.info("DataFrame content:")
    df_result.show()

    df_result.createOrReplaceTempView("tier2_metrics")
    df_result.createOrReplaceTempView("evaluated_roles_with_compliance")
    
    logger.info(f"Setting spark.tier2_numerator to {results['numerator']}")
    spark.conf.set("spark.tier2_numerator", str(results['numerator']))
    logger.info(f"Retrieved spark.tier2_numerator: {spark.conf.get('spark.tier2_numerator')}")

    logger.info(f"Setting spark.tier2_denominator to {results['denominator']}")
    spark.conf.set("spark.tier2_denominator", str(results['denominator']))
    logger.info(f"Retrieved spark.tier2_denominator: {spark.conf.get('spark.tier2_denominator')}")

    logger.info(f"Setting spark.tier2_metric_value to {results['metric']}")
    spark.conf.set("spark.tier2_metric_value", str(results['metric']))
    logger.info(f"Retrieved spark.tier2_metric_value: {spark.conf.get('spark.tier2_metric_value')}")

    logger.info(f"Setting spark.tier2_compliance_status to {results['status']}")
    spark.conf.set("spark.tier2_compliance_status", results['status'])
    logger.info(f"Retrieved spark.tier2_compliance_status: {spark.conf.get('spark.tier2_compliance_status')}")

except Exception as e:
    logger.error(f"ERROR in Tier 2 metrics calculation: {str(e)}")
    raise

# COMMAND ----------

# MAGIC %md
# MAGIC ## 14. Tier 3 Metrics Calculation
# MAGIC Calculate SLA compliance metrics for non-compliant resources using Tier 2 non-compliant count

# COMMAND ----------
from pyspark.sql.functions import col
import logging
from datetime import datetime, date
import pandas as pd

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def calculate_metrics(alert_val, warning_val, numerator, denominator):
    """Pure Python function to calculate metrics with dynamic status logic."""
    alert = float(alert_val) if alert_val is not None else None
    warning = float(warning_val) if warning_val is not None else None
    numerator = int(numerator)
    denominator = int(denominator)
    
    metric = numerator / denominator * 100 if denominator > 0 else 100.0  # 100% if no non-compliant resources
    metric = round(metric, 2)
    
    status = "GREEN"  # Default to GREEN if no thresholds are defined
    if alert is not None and warning is not None:  # Only evaluate status if both thresholds are present
        if metric < alert:
            status = "RED"
        elif metric < warning and metric >= alert:
            status = "YELLOW"
    logger.info(f"Calculated metric: {metric}, status: {status} with alert={alert}, warning={warning}")
    return {
        "metric": metric,
        "status": status,
        "numerator": numerator,
        "denominator": denominator
    }

try:
    # Load Tier 3 thresholds from Snowflake
    logger.info("Loading Tier 3 thresholds from Snowflake")
    tier3_thresholds_df = spark.read.format(SNOWFLAKE_SOURCE_NAME) \
        .options(**sfOptions) \
        .option("query", tier3_threshold_query) \
        .load()
    
    tier3_thresholds_df.show()
    threshold_data = tier3_thresholds_df.collect()
    logger.info(f"Threshold data collected: {threshold_data}, type: {type(threshold_data)}")
    
    tier3_alert_threshold_raw = threshold_data[0]["ALERT_THRESHOLD"] if threshold_data else None
    tier3_warning_threshold_raw = threshold_data[0]["WARNING_THRESHOLD"] if threshold_data else None
    logger.info(f"Tier 3 Alert threshold: {tier3_alert_threshold_raw}, type: {type(tier3_alert_threshold_raw)}")
    logger.info(f"Tier 3 Warning threshold: {tier3_warning_threshold_raw}, type: {type(tier3_warning_threshold_raw)}")

    # Validate that Tier 2 metrics are available
    required_configs = ["spark.tier2_numerator", "spark.tier2_denominator"]
    for config in required_configs:
        try:
            value = spark.conf.get(config)
            logger.info(f"Successfully retrieved {config}: {value}")
        except Exception as e:
            logger.error(f"Required config {config} not found. Ensure Tier 2 Metrics Calculation ran successfully: {str(e)}")
            raise ValueError(f"Required config {config} not found. Ensure Tier 2 Metrics Calculation ran successfully.")

    # Get Tier 2 metrics for non-compliant count
    tier2_numerator = int(spark.conf.get("spark.tier2_numerator", "0"))
    tier2_denominator = int(spark.conf.get("spark.tier2_denominator", "0"))
    total_non_compliant = tier2_denominator - tier2_numerator
    logger.info(f"Retrieved Tier 2 metrics: numerator={tier2_numerator}, denominator={tier2_denominator}, "
                f"total_non_compliant={total_non_compliant}")

    if total_non_compliant <= 0:
        logger.warning("No non-compliant resources found in Tier 2. Setting metric to 100% and status to GREEN.")
        total_non_compliant = 0
        within_sla_count = 0
        results = {
            "metric": 100.0,
            "status": "GREEN",
            "numerator": 0,
            "denominator": 0
        }
    else:
        # Load evaluated roles with compliance from Tier 2
        logger.info("Loading evaluated roles with compliance from Tier 2")
        if not 'evaluated_roles_with_compliance' in spark.catalog.listTables():
            raise ValueError("evaluated_roles_with_compliance temp view not found. Ensure Tier 2 ran successfully.")
        df_evaluated_roles = spark.table("evaluated_roles_with_compliance")
        
        # Extract non-compliant resources
        non_compliant_resources = [(row["RESOURCE_ID"], row["ARN"]) 
                                  for row in df_evaluated_roles.collect() 
                                  if row["COMPLIANCE_STATUS"] == "NonCompliant"]
        logger.info(f"Non-compliant resources from Tier 2: {len(non_compliant_resources)}, type: {type(non_compliant_resources)}")
        logger.info(f"Sample non_compliant_resources: {non_compliant_resources[:5]}")

        if len(non_compliant_resources) != total_non_compliant:
            logger.warning(f"Discrepancy in non-compliant count: Tier 2 calculated {total_non_compliant}, "
                          f"but found {len(non_compliant_resources)} in evaluated_roles_with_compliance")

        # Prepare list of RESOURCE_IDs for querying
        resource_ids = [resource_id for resource_id, _ in non_compliant_resources]
        if not resource_ids:
            logger.info("No non-compliant resources to query for SLA status.")
            within_sla_count = 0
            past_sla_count = 0
        else:
            # Construct a query to get SLA data for these specific resources
            resource_id_list = ",".join([f"'{rid}'" for rid in resource_ids])
            sla_query = f"""
            SELECT 
                RESOURCE_ID,
                CONTROL_RISK,
                OPEN_DATE_UTC_TIMESTAMP
            FROM CLCN_DB.PHDP_CLOUD.OZONE_NON_COMPLIANT_RESOURCES_TCRD_VIEW_V01
            WHERE CONTROL_ID = 'AC-3.AWS.39.v02'
              AND RESOURCE_ID IN ({resource_id_list})
              AND ID NOT IN (
                  SELECT ID 
                  FROM CLCN_DB.PHDP_CLOUD.OZONE_CLOSED_NON_COMPLIANT_RESOURCES_V04
              )
            """
            logger.info("Loading SLA data for non-compliant resources from Snowflake")
            df_sla_data = spark.read.format(SNOWFLAKE_SOURCE_NAME) \
                .options(**sfOptions) \
                .option("query", sla_query) \
                .load()
            
            logger.info(f"df_sla_data count: {df_sla_data.count()}")
            df_sla_data.printSchema()
            logger.info("Sample df_sla_data data:")
            df_sla_data.show(5)
            
            # Convert to Python list
            sla_data = [(row["RESOURCE_ID"], row["CONTROL_RISK"], row["OPEN_DATE_UTC_TIMESTAMP"]) 
                       for row in df_sla_data.collect()]
            logger.info(f"SLA data count: {len(sla_data)}, type: {type(sla_data)}")
            logger.info(f"Sample sla_data: {sla_data[:5]}")

            # Check for missing resources
            found_resource_ids = set(row[0] for row in sla_data)
            missing_resources = [rid for rid in resource_ids if rid not in found_resource_ids]
            if missing_resources:
                logger.warning(f"Missing SLA data for {len(missing_resources)} resources: {missing_resources[:5]}")

            # Calculate days open and determine past SLA status
            sla_thresholds = {
                "Critical": 0,
                "High": 30,
                "Medium": 60,
                "Low": 90
            }
            current_date = datetime.now()
            past_sla_count = 0
            for resource_id, control_risk, open_date in sla_data:
                if open_date is None or control_risk not in sla_thresholds:
                    logger.warning(f"Skipping invalid entry: resource_id={resource_id}, "
                                  f"control_risk={control_risk}, open_date={open_date}")
                    continue
                days_open = (current_date - open_date).days
                sla_limit = sla_thresholds.get(control_risk, 90)  # Default to 90 if risk level unknown
                if days_open > sla_limit:
                    past_sla_count += 1
            
            logger.info(f"Past SLA count: {past_sla_count}, type: {type(past_sla_count)}")

            # Calculate numerator (resources within SLA)
            within_sla_count = total_non_compliant - past_sla_count if total_non_compliant >= past_sla_count else 0
            logger.info(f"Within SLA count: {within_sla_count}, type: {type(within_sla_count)}")

        # Perform metric calculation
        results = calculate_metrics(tier3_alert_threshold_raw, tier3_warning_threshold_raw, 
                                  within_sla_count, total_non_compliant)
        logger.info(f"Calculation results: {results}, type: {type(results)}")

    # Get current date as a Python object
    current_date_value = date.today()
    logger.info(f"Current date: {current_date_value}, type: {type(current_date_value)}")

    # Prepare metrics data as a Python dictionary for pandas DataFrame
    metrics_data = {
        "DATE": [current_date_value],
        "MONITORING_METRIC_NUMBER": ['MNTR-XXXXX-T3'],
        "MONITORING_METRIC": [results["metric"]],
        "COMPLIANCE_STATUS": [results["status"]],
        "NUMERATOR": [results["numerator"]],
        "DENOMINATOR": [results["denominator"]]
    }
    logger.info(f"Metrics data: {metrics_data}")
    logger.info(f"Metrics data types: {[type(x[0]) for x in metrics_data.values()]}")

    # Convert to pandas DataFrame and then to PySpark DataFrame
    pd_df = pd.DataFrame(metrics_data)
    df_result = spark.createDataFrame(pd_df)
    logger.info("DataFrame created from pandas DataFrame")
    df_result.printSchema()
    logger.info("DataFrame content:")
    df_result.show()

    # Register as temp view and store metrics
    df_result.createOrReplaceTempView("tier3_metrics")
    logger.info(f"Setting spark.tier3_numerator to {results['numerator']}")
    spark.conf.set("spark.tier3_numerator", str(results['numerator']))
    logger.info(f"Setting spark.tier3_denominator to {results['denominator']}")
    spark.conf.set("spark.tier3_denominator", str(results['denominator']))
    logger.info(f"Setting spark.tier3_metric_value to {results['metric']}")
    spark.conf.set("spark.tier3_metric_value", str(results['metric']))
    logger.info(f"Setting spark.tier3_compliance_status to {results['status']}")
    spark.conf.set("spark.tier3_compliance_status", results['status'])

except Exception as e:
    logger.error(f"ERROR in Tier 3 metrics calculation: {str(e)}")
    raise

# COMMAND ----------

# MAGIC %md
# MAGIC ## Unit Tests

# COMMAND ----------

def test_tier1_metrics_calculation(spark):
    """Unit test for Tier 1 metrics calculation and DataFrame creation."""
    # Mock data for all_iam_roles
    mock_all_iam_roles = spark.createDataFrame(
        [("res1", "arn1", "account1", datetime(2025, 1, 1), "role", "policy1", "MACHINE", "BA1", datetime(2025, 2, 1)),
         ("res2", "arn2", "account2", datetime(2025, 1, 2), "role", "policy2", "MACHINE", "BA2", datetime(2025, 2, 2)),
         ("res3", "arn3", "account3", datetime(2025, 1, 3), "role", "policy3", "USER", "BA3", datetime(2025, 2, 3))],
        ["RESOURCE_ID", "AMAZON_RESOURCE_NAME", "ACCOUNT", "CREATE_DATE", "TYPE", "FULL_RECORD", "ROLE_TYPE", "BA", "SF_LOAD_TIMESTAMP"]
    )
    mock_all_iam_roles.createOrReplaceTempView("all_iam_roles")

    # Mock data for evaluated_roles
    mock_evaluated_roles = spark.createDataFrame(
        [("ARN1", "Compliant"), ("ARN2", "NonCompliant")],
        ["RESOURCE_NAME", "COMPLIANCE_STATUS"]
    )
    mock_evaluated_roles.createOrReplaceTempView("evaluated_roles")

    # Mock approved_accounts
    approved_accounts = ["account1", "account2"]

    # Execute filtered_machine_roles query
    df_filtered = spark.sql(filtered_machine_roles_query)

    # Execute evaluated_roles query
    df_evaluated = spark.sql(evaluated_roles_query)

    filtered_data = [(row["RESOURCE_ID"], row["AMAZON_RESOURCE_NAME"], row["ACCOUNT"], 
                     row["BA"], row["ROLE_TYPE"]) for row in df_filtered.collect()]
    evaluated_data = [(row["RESOURCE_NAME"], row["COMPLIANCE_STATUS"]) for row in df_evaluated.collect()]

    filtered_map = {row[1]: (row[0], row[2], row[3], row[4]) for row in filtered_data}
    evaluated_compliance = {arn: status for arn, status in evaluated_data}
    evaluated_roles = [
        (resource_id, arn, acc, ba, role_type, status) 
        for arn, status in evaluated_compliance.items() 
        if arn in filtered_map 
        for resource_id, acc, ba, role_type in [filtered_map[arn]]
    ]

    evaluated_count = len(evaluated_roles)
    total_count = df_filtered.count()
    results = calculate_metrics(None, None, evaluated_count, total_count)

    metrics_data = {
        "DATE": [date.today()],
        "MONITORING_METRIC_NUMBER": ['MNTR-XXXXX'],
        "MONITORING_METRIC": [results["metric"]],
        "COMPLIANCE_STATUS": [results["status"]],
        "NUMERATOR": [results["numerator"]],
        "DENOMINATOR": [results["denominator"]]
    }

    pd_df = pd.DataFrame(metrics_data)
    df_result = spark.createDataFrame(pd_df)

    collected_result = df_result.collect()[0]
    assert collected_result["NUMERATOR"] == 2, f"Expected numerator 2, got {collected_result['NUMERATOR']}"
    assert collected_result["DENOMINATOR"] == 2, f"Expected denominator 2, got {collected_result['DENOMINATOR']}"
    assert round(collected_result["MONITORING_METRIC"], 2) == 100.00, f"Expected metric 100.00%, got {collected_result['MONITORING_METRIC']}"
    assert collected_result["COMPLIANCE_STATUS"] == "GREEN", f"Expected status GREEN, got {collected_result['COMPLIANCE_STATUS']}"
    logger.info("Unit test for Tier 1 metrics calculation and DataFrame creation passed!")

def test_tier2_metrics_calculation(spark):
    """Unit test for Tier 2 metrics calculation and DataFrame creation."""
    # Mock data for filtered_machine_roles
    mock_filtered = spark.createDataFrame(
        [("res1", "arn1", "account1", "BA1", "MACHINE"),
         ("res2", "arn2", "account2", "BA2", "MACHINE")],
        ["RESOURCE_ID", "AMAZON_RESOURCE_NAME", "ACCOUNT", "BA", "ROLE_TYPE"]
    )
    mock_filtered.createOrReplaceTempView("filtered_machine_roles")

    # Mock data for evaluated_roles
    mock_evaluated = spark.createDataFrame(
        [("ARN1", "Compliant"), ("ARN2", "NonCompliant")],
        ["RESOURCE_NAME", "COMPLIANCE_STATUS"]
    )
    mock_evaluated.createOrReplaceTempView("evaluated_roles")

    filtered_data = [(row["RESOURCE_ID"], row["AMAZON_RESOURCE_NAME"], row["ACCOUNT"], 
                     row["BA"], row["ROLE_TYPE"]) for row in mock_filtered.collect()]
    evaluated_data = [(row["RESOURCE_NAME"], row["COMPLIANCE_STATUS"]) for row in mock_evaluated.collect()]

    filtered_map = {row[1]: (row[0], row[2], row[3], row[4]) for row in filtered_data}
    evaluated_compliance = {arn: status for arn, status in evaluated_data}
    evaluated_roles = [
        (resource_id, arn, acc, ba, role_type, status) 
        for arn, status in evaluated_compliance.items() 
        if arn in filtered_map 
        for resource_id, acc, ba, role_type in [filtered_map[arn]]
    ]

    compliant_count = len([status for _, _, _, _, _, status in evaluated_roles 
                         if status not in ["NonCompliant"]])
    evaluated_count = len(evaluated_roles)
    results = calculate_metrics(None, None, compliant_count, evaluated_count)

    metrics_data = {
        "DATE": [date.today()],
        "MONITORING_METRIC_NUMBER": ['MNTR-XXXXX-T2'],
        "MONITORING_METRIC": [results["metric"]],
        "COMPLIANCE_STATUS": [results["status"]],
        "NUMERATOR": [results["numerator"]],
        "DENOMINATOR": [results["denominator"]]
    }

    pd_df = pd.DataFrame(metrics_data)
    df_result = spark.createDataFrame(pd_df)

    collected_result = df_result.collect()[0]
    assert collected_result["NUMERATOR"] == 1, f"Expected numerator 1, got {collected_result['NUMERATOR']}"
    assert collected_result["DENOMINATOR"] == 2, f"Expected denominator 2, got {collected_result['DENOMINATOR']}"
    assert round(collected_result["MONITORING_METRIC"], 2) == 50.00, f"Expected metric 50.00%, got {collected_result['MONITORING_METRIC']}"
    assert collected_result["COMPLIANCE_STATUS"] == "GREEN", f"Expected status GREEN, got {collected_result['COMPLIANCE_STATUS']}"
    logger.info("Unit test for Tier 2 metrics calculation and DataFrame creation passed!")

def test_tier3_metrics_calculation(spark):
    """Unit test for Tier 3 metrics calculation and DataFrame creation."""
    # Mock Tier 2 metrics
    spark.conf.set("spark.tier2_numerator", "2")
    spark.conf.set("spark.tier2_denominator", "5")  # 5 total, 2 compliant, 3 non-compliant
    
    # Mock evaluated_roles_with_compliance from Tier 2
    mock_evaluated_roles = spark.createDataFrame(
        [("res1", "arn1", "acc1", "ba1", "MACHINE", "Compliant"),
         ("res2", "arn2", "acc2", "ba2", "MACHINE", "NonCompliant"),
         ("res3", "arn3", "acc3", "ba3", "MACHINE", "NonCompliant"),
         ("res4", "arn4", "acc4", "ba4", "MACHINE", "Compliant"),
         ("res5", "arn5", "acc5", "ba5", "MACHINE", "NonCompliant")],
        ["RESOURCE_ID", "ARN", "ACCOUNT", "BA", "ROLE_TYPE", "COMPLIANCE_STATUS"]
    )
    mock_evaluated_roles.createOrReplaceTempView("evaluated_roles_with_compliance")

    # Mock SLA data for non-compliant resources
    mock_sla_data = spark.createDataFrame(
        [("res2", "High", datetime(2025, 2, 1)),   # 35 days old, past SLA (High: 30 days)
         ("res3", "Medium", datetime(2025, 2, 1)),  # 35 days old, within SLA (Medium: 60 days)
         ("res5", "Low", datetime(2025, 2, 1))],   # 35 days old, within SLA (Low: 90 days)
        ["RESOURCE_ID", "CONTROL_RISK", "OPEN_DATE_UTC_TIMESTAMP"]
    )

    # Mock thresholds (empty to test default GREEN)
    mock_thresholds = spark.createDataFrame(
        [],  # Empty DataFrame to simulate missing thresholds
        ["ALERT_THRESHOLD", "WARNING_THRESHOLD"]
    )

    # Get non-compliant count from Tier 2
    tier2_numerator = int(spark.conf.get("spark.tier2_numerator", "0"))
    tier2_denominator = int(spark.conf.get("spark.tier2_denominator", "0"))
    total_non_compliant = tier2_denominator - tier2_numerator

    # Get non-compliant resources
    non_compliant_resources = [(row["RESOURCE_ID"], row["ARN"]) 
                              for row in mock_evaluated_roles.collect() 
                              if row["COMPLIANCE_STATUS"] == "NonCompliant"]
    
    # Simulate SLA data lookup (in real code, this is a Snowflake query)
    sla_data = [(row["RESOURCE_ID"], row["CONTROL_RISK"], row["OPEN_DATE_UTC_TIMESTAMP"]) 
                for row in mock_sla_data.collect()]

    # Calculate past SLA count
    sla_thresholds = {
        "Critical": 0,
        "High": 30,
        "Medium": 60,
        "Low": 90
    }
    current_date = datetime.now()
    past_sla_count = 0
    for _, control_risk, open_date in sla_data:
        if open_date is None or control_risk not in sla_thresholds:
            continue
        days_open = (current_date - open_date).days
        sla_limit = sla_thresholds.get(control_risk, 90)
        if days_open > sla_limit:
            past_sla_count += 1

    # Calculate numerator
    within_sla_count = total_non_compliant - past_sla_count if total_non_compliant >= past_sla_count else 0
    threshold_data = mock_thresholds.collect()
    alert_threshold = threshold_data[0]["ALERT_THRESHOLD"] if threshold_data else None
    warning_threshold = threshold_data[0]["WARNING_THRESHOLD"] if threshold_data else None

    results = calculate_metrics(alert_threshold, warning_threshold, within_sla_count, total_non_compliant)
    
    metrics_data = {
        "DATE": [date.today()],
        "MONITORING_METRIC_NUMBER": ['MNTR-XXXXX-T3'],
        "MONITORING_METRIC": [results["metric"]],
        "COMPLIANCE_STATUS": [results["status"]],
        "NUMERATOR": [results["numerator"]],
        "DENOMINATOR": [results["denominator"]]
    }

    pd_df = pd.DataFrame(metrics_data)
    df_result = spark.createDataFrame(pd_df)

    collected_result = df_result.collect()[0]
    assert collected_result["NUMERATOR"] == 2, f"Expected numerator 2, got {collected_result['NUMERATOR']}"  # 2 within SLA
    assert collected_result["DENOMINATOR"] == 3, f"Expected denominator 3, got {collected_result['DENOMINATOR']}"  # 3 non-compliant
    assert round(collected_result["MONITORING_METRIC"], 2) == 66.67, f"Expected metric ~66.67%, got {collected_result['MONITORING_METRIC']}"
    assert collected_result["COMPLIANCE_STATUS"] == "GREEN", f"Expected status GREEN, got {collected_result['COMPLIANCE_STATUS']}"
    logger.info("Unit test for Tier 3 metrics calculation and DataFrame creation passed!")

# Run unit tests
test_tier1_metrics_calculation(spark)
test_tier2_metrics_calculation(spark)
test_tier3_metrics_calculation(spark)

