try:
    # Get approved accounts
    df_approved_accounts = get_approved_accounts()

    # Step 1: Load all IAM roles with their full information
    log_step("Loading IAM roles")
    df_all_iam_roles = spark.read.format(SNOWFLAKE_SOURCE_NAME) \
        .options(**sfOptions) \
        .option("query", all_iam_roles_query) \
        .load()
    
    # Validate account numbers and basic role information
    df_all_iam_roles = df_all_iam_roles.filter(
        (col("ACCOUNT").isNotNull()) &  # Check for null
        (col("ACCOUNT") != "") &        # Check for empty string
        (col("ACCOUNT").rlike("^[0-9]+$"))  # Check for only digits using regex pattern
    )
    
    validate_dataframe(
        df_all_iam_roles,
        expected_cols=['RESOURCE_ID', 'AMAZON_RESOURCE_NAME', 'ACCOUNT'],
        min_count=1
    )
    log_step("IAM roles loaded", f"Total Valid IAM Roles: {df_all_iam_roles.count()}")

    # Step 2: Load violation roles mapping
    log_step("Loading violation roles mapping")
    df_violation_roles = spark.read.format(SNOWFLAKE_SOURCE_NAME) \
        .options(**sfOptions) \
        .option("query", violation_roles_query) \
        .load()
    
    validate_dataframe(
        df_violation_roles,
        expected_cols=['RESOURCE_NAME', 'ROLE_TYPE'],
        min_count=1
    )
    log_step("Violation roles loaded", f"Total Violation Roles: {df_violation_roles.count()}")

    # Step 3: Load evaluated roles
    log_step("Loading evaluated roles")
    df_evaluated_roles = spark.read.format(SNOWFLAKE_SOURCE_NAME) \
        .options(**sfOptions) \
        .option("query", evaluated_roles_query) \
        .load()
    
    validate_dataframe(
        df_evaluated_roles,
        expected_cols=['RESOURCE_NAME'],
        min_count=1
    )
    log_step("Evaluated roles loaded", f"Total Evaluated Roles: {df_evaluated_roles.count()}")

    # Step 4: Create initial mapping with full role information
    log_step("Creating initial role mapping")
    df_initial_mapping = df_all_iam_roles.alias("iam") \
        .join(
            df_violation_roles.alias("viol"),
            df_all_iam_roles.AMAZON_RESOURCE_NAME == df_violation_roles.RESOURCE_NAME,
            "left"
        ) \
        .join(
            df_evaluated_roles.alias("eval"),
            df_all_iam_roles.AMAZON_RESOURCE_NAME == df_evaluated_roles.RESOURCE_NAME,
            "left"
        ) \
        .select(
            col("iam.RESOURCE_ID"),
            col("iam.AMAZON_RESOURCE_NAME"),
            col("iam.BA"),
            col("iam.ACCOUNT"),
            col("iam.CREATE_DATE"),
            col("iam.LOAD_TIMESTAMP"),
            coalesce(col("viol.ROLE_TYPE"), lit("NOT FOUND")).alias("ROLE_TYPE"),
            when(col("eval.RESOURCE_NAME").isNotNull(), 1).otherwise(0).alias("IS_EVALUATED"),
            lit("INITIAL_MAPPING").alias("MAPPING_SOURCE")
        ) \
        .dropDuplicates(["RESOURCE_ID"])  # Ensure no duplicates after joins
    
    initial_mapping_count = df_initial_mapping.count()
    log_step("Initial mapping complete", f"Initially Mapped Roles: {initial_mapping_count}")

    # Check if we need to process unmapped roles
    if initial_mapping_count < df_all_iam_roles.count():
        log_step("Processing unmapped roles", "Found potentially unmapped roles, starting validation")
        
        # Step 5: Find unmapped resources
        log_step("Finding unmapped resources")
        df_unmapped = df_all_iam_roles.alias("iam") \
            .join(
                df_initial_mapping.alias("init"),
                col("iam.RESOURCE_ID") == col("init.RESOURCE_ID"),
                "left_anti"
            )
        unmapped_count = df_unmapped.count()
        log_step("Unmapped resources identified", f"Unmapped Roles: {unmapped_count}")

        if unmapped_count > 0:
            # Step 6: Check unmapped resources in violations dataset
            log_step("Validating unmapped resources")
            df_unmapped_validation = df_unmapped.alias("unmap") \
                .join(
                    df_violation_roles.alias("viol"),
                    col("unmap.AMAZON_RESOURCE_NAME") == col("viol.RESOURCE_NAME"),
                    "left"
                ) \
                .select(
                    col("unmap.RESOURCE_ID"),
                    col("unmap.AMAZON_RESOURCE_NAME"),
                    col("unmap.BA"),
                    col("unmap.ACCOUNT"),
                    col("unmap.CREATE_DATE"),
                    col("unmap.LOAD_TIMESTAMP"),
                    coalesce(col("viol.ROLE_TYPE"), lit("NOT FOUND")).alias("ROLE_TYPE"),
                    lit(0).alias("IS_EVALUATED"),
                    lit("VALIDATION_MAPPING").alias("MAPPING_SOURCE")
                )
            log_step("Unmapped validation complete", 
                    f"Validated Unmapped Roles: {df_unmapped_validation.count()}")

            # Step 7: Combine all roles and deduplicate
            log_step("Combining and deduplicating roles")
            df_all_roles = df_initial_mapping.union(df_unmapped_validation) \
                .dropDuplicates(["RESOURCE_ID"]) \
                .orderBy(col("LOAD_TIMESTAMP").desc())
        else:
            log_step("Skipping unmapped validation", "No unmapped roles found")
            df_all_roles = df_initial_mapping
    else:
        log_step("Skipping unmapped processing", "All roles are already mapped in initial mapping")
        df_all_roles = df_initial_mapping

    # Step 8: Filter for machine roles only
    log_step("Filtering for machine roles")
    df_machine_roles = df_all_roles.filter(col("ROLE_TYPE") == "MACHINE")
    log_step("Machine roles filtered", f"Total Machine Roles Found: {df_machine_roles.count()}")

    # Step 9: Join with approved accounts
    log_step("Filtering for approved accounts")
    df_filtered = df_machine_roles.join(
        df_approved_accounts,
        upper(df_machine_roles.ACCOUNT) == upper(df_approved_accounts.accountNumber),
        "inner"
    )
    log_step("Account filtering complete", 
             f"Machine Roles in Approved Accounts: {df_filtered.count()}")

    # Cache the filtered results for metrics calculation
    df_filtered.cache()

except Exception as e:
    log_step("ERROR in data processing", f"Pipeline execution failed during data processing: {str(e)}")
    raise

# COMMAND ----------
# MAGIC %md
# MAGIC ## 6. Metrics Calculation and Results
# MAGIC Calculate and display compliance metrics
# MAGIC - Create temporary views
# MAGIC - Calculate compliance metrics
# MAGIC - Display detailed results and summaries
# MAGIC - Perform cleanup operations

try:
    # Create temporary view for final metrics calculation
    df_filtered.createOrReplaceTempView("filtered_machine_roles")
    
    # Calculate final compliance metrics
    log_step("Calculating compliance metrics")
    final_metrics_query = """
    WITH Count_Calculations AS (
        SELECT
            COUNT(*) as denominator,
            SUM(CASE WHEN IS_EVALUATED = 1 THEN 1 ELSE 0 END) as numerator
        FROM filtered_machine_roles
    )
    SELECT
        CURRENT_DATE() AS DATE,
        'MNTR-1071824-T1' AS MONITORING_METRIC_NUMBER,
        ROUND(100.0 * numerator / denominator, 2) AS MONITORING_METRIC,
        CASE
            WHEN ROUND(100.0 * numerator / denominator, 2) >= 98 THEN 'GREEN'
            WHEN ROUND(100.0 * numerator / denominator, 2) >= 95 THEN 'YELLOW'
            ELSE 'RED'
        END AS COMPLIANCE_STATUS,
        numerator AS NUMERATOR,
        denominator AS DENOMINATOR
    FROM Count_Calculations
    """
    
    df_result = spark.sql(final_metrics_query)
    
    # Display final results
    log_step("Pipeline Execution Summary")
    print("\nDetailed Pipeline Results:")
    print(f"1. Total Valid IAM Roles: {df_all_iam_roles.count()}")
    print(f"2. Initially Mapped Roles: {df_initial_mapping.count()}")
    print(f"3. Unmapped Roles Found: {df_unmapped.count()}")
    print(f"4. Validated Unmapped Roles: {df_unmapped_validation.count()}")
    print(f"5. Total Combined Roles: {df_all_roles.count()}")
    print(f"6. Total Machine Roles: {df_machine_roles.count()}")
    print(f"7. Machine Roles in Approved Accounts: {df_filtered.count()}")
    
    print("\nRole Type Distribution:")
    df_all_roles.groupBy("ROLE_TYPE", "MAPPING_SOURCE") \
        .count() \
        .orderBy("ROLE_TYPE", "count") \
        .show()
    
    print("\nFinal Compliance Metrics:")
    df_result.show()

    # Validate final results
    if df_result.count() == 0:
        raise ValueError("No compliance metrics generated")
    
    log_step("Pipeline execution completed successfully")

except Exception as e:
    log_step("ERROR in metrics calculation", f"Pipeline execution failed during metrics calculation: {str(e)}")
    raise

finally:
    # Clean up resources
    log_step("Cleanup", "Removing temporary views and cached DataFrames")
    spark.catalog.dropTempView("filtered_machine_roles")
    # Unpersist cached DataFrames if any
    for df in [df_all_roles, df_filtered]:
        try:
            if df and df.is_cached:
                df.unpersist()
        except Exception:
            pass  # Ignore cleanup errors
