def get_summary_count(payload: Dict, timeout: int = 30, max_retries: int = 2) -> Optional[int]:
    """Fetch the total count of resources from the summary-view API."""
    logger.info("Fetching summary count...")
    
    for retry in range(max_retries + 1):
        try:
            response = requests.post(
                CONFIG_URL.replace('search_resource_configurations', 'summary_view'),
                headers=HEADERS,
                json=payload,
                verify=False,
                timeout=timeout
            )
            
            if response.status_code == 200:
                data = response.json()
                level1_list = data.get("level1List", [])
                if not level1_list:
                    logger.warning("Empty level1List in response")
                    return 0
                    
                count = level1_list[0].get("level1ResourceCount", 0)
                logger.info(f"Summary count: {count}")
                return count
            
            elif response.status_code == 429:
                wait_time = min(2 ** retry, 30)
                logger.warning(f"Rate limited (429). Waiting {wait_time}s before retry {retry+1}/{max_retries}.")
                time.sleep(wait_time)
            else:
                logger.error(f"Summary API failed: {response.status_code} - {response.text}")
                if retry < max_retries:
                    time.sleep(min(2 ** retry, 15))
                
        except Exception as e:
            logger.error(f"Summary API request failed: {str(e)}")
            if retry < max_retries:
                time.sleep(min(2 ** retry, 15))
    
    return None

def fetch_resources(timeout: int = 60, max_retries: int = 3, page_size: int = 1000) -> Tuple[List[Dict], Dict[str, int]]:
    """
    Fetch all KMS resources using the pagination API.

    Args:
        timeout: Request timeout in seconds
        max_retries: Maximum number of retry attempts
        page_size: Number of resources to fetch per page

    Returns:
        Tuple containing list of resources and dictionary of origin value counts
    """
    all_resources = []
    origin_counts = {}
    # Configure debug logging
    debug_logger = logging.getLogger('pagination_debug')
    debug_logger.setLevel(logging.DEBUG)
    fh = logging.FileHandler('pagination_debug.log')
    fh.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
    debug_logger.addHandler(fh)
    """
    Fetch all KMS resources and analyze their origin values.
    
    Args:
        timeout: Request timeout in seconds
        max_retries: Maximum number of retry attempts
        page_size: Number of resources to fetch per page
        
    Returns:
        Tuple containing list of resources and dictionary of origin value counts
    """
    all_resources = []
    origin_counts = {}
    next_record_key = ""
    page_count = 0
    unique_resource_ids = set()
    
    debug_logger.info("=== Starting new fetch_resources call ===")
    debug_logger.info(f"Parameters: timeout={timeout}, max_retries={max_retries}, page_size={page_size}")

    # Get expected total count
    payload = {
        "searchParameters": [{
            "resourceType": "AWS::KMS::Key"
        }]
    }
    expected_count = get_summary_count(payload)
    if expected_count is not None:
        debug_logger.info(f"Expected total resources from summary: {expected_count}")
    else:
        debug_logger.warning("Could not get expected count from summary API - pagination validation will be limited")
    
    # Base payload with essential fields
    payload = {
        "searchParameters": [{
            "resourceType": "AWS::KMS::Key"
        }],
        "responseFields": [
            "accountName",
            "accountResourceId", 
            "awsAccountId",
            "awsRegion",
            "environment",
            "resourceCreationTimestamp",
            "resourceId",
            "resourceType",
            "configuration.origin"
        ],
        "limit": page_size
    }
    
    start_time = datetime.now()
    
    while True:  # Continue until no more pages
        current_page_key = next_record_key  # Store current key for comparison
        logger.info(f"Processing page {page_count + 1} with record key: {current_page_key}")
        
        for retry in range(max_retries + 1):
            try:
                logger.info(f"Requesting page {page_count + 1}" + (f" (retry {retry})" if retry > 0 else ""))
                
                # Add nextRecordKey if we have one
                if next_record_key:
                    payload["nextRecordKey"] = next_record_key
                    logger.debug(f"Using nextRecordKey: {next_record_key}")
                
                debug_logger.debug(f"Making API request with payload: {json.dumps(payload, indent=2)}")
                response = requests.post(
                    CONFIG_URL,
                    headers=HEADERS,
                    json=payload,
                    verify=False,
                    timeout=timeout
                )
                debug_logger.debug(f"Response status code: {response.status_code}")
                if response.status_code != 200:
                    debug_logger.debug(f"Response headers: {dict(response.headers)}")
                    debug_logger.debug(f"Response body: {response.text[:1000]}...")
                
                if response.status_code == 200:
                    data = response.json()
                    resources = data.get("resourceConfigurations", [])
                    
                    # Enhanced pagination debugging
                    debug_logger.info(f"=== Page {page_count + 1} Response Analysis ===")
                    debug_logger.info(f"Current nextRecordKey: {current_page_key}")
                    debug_logger.info(f"New nextRecordKey: {data.get('nextRecordKey')}")
                    debug_logger.info(f"Resources in response: {len(resources)}")
                    debug_logger.info(f"Total resources so far: {len(all_resources)}")
                    
                    if len(resources) == 0:
                        debug_logger.warning("Received empty resource list in response")
                        debug_logger.debug(f"Full response structure: {json.dumps(data, indent=2)}")
                    
                    # Analyze response structure
                    debug_logger.debug("Response keys: " + ", ".join(data.keys()))
                    if resources and len(resources) > 0:
                        debug_logger.debug(f"Sample resource structure: {json.dumps(resources[0], indent=2)}")

                    
                    # Process resources
                    new_resources_count = 0
                    for resource in resources:
                        resource_id = resource.get('resourceId')
                        if resource_id in unique_resource_ids:
                            debug_logger.warning(f"Duplicate resource detected - ID: {resource_id}, Page: {page_count + 1}")
                            debug_logger.debug(f"Duplicate resource data: {json.dumps(resource, indent=2)}")
                            continue
                            
                        unique_resource_ids.add(resource_id)
                        new_resources_count += 1
                        
                        # Extract origin value
                        origin = "Unknown"
                        config_list = resource.get("configurationList", [])
                        for config in config_list:
                            if config["configurationName"] == "configuration.origin":
                                origin = config["configurationValue"]
                                break
                        
                        # Update counts
                        origin_counts[origin] = origin_counts.get(origin, 0) + 1
                        
                        # Store resource with origin
                        resource["extracted_origin"] = origin
                        all_resources.append(resource)
                    
                    # Update pagination info and validate
                    next_record_key = data.get("nextRecordKey", "")
                    logger.info(f"Page {page_count + 1}: Added {new_resources_count} new resources (total: {len(all_resources)})")
                    logger.info(f"Next record key: {next_record_key}")
                    
                    # Enhanced pagination loop validation
                    if next_record_key == current_page_key:
                        debug_logger.error("=== Pagination Loop Detection ===")
                        debug_logger.error(f"Detected same nextRecordKey: {next_record_key}")
                        debug_logger.error(f"Current page: {page_count + 1}, Total resources: {len(all_resources)}")
                        debug_logger.error(f"Expected total: {expected_count if expected_count is not None else 'Unknown'}")
                        debug_logger.error("Breaking pagination loop to avoid infinite iteration")
                        next_record_key = ""
                        break
                    
                    # Validate against expected count
                    if expected_count and len(all_resources) >= expected_count:
                        logger.info(f"Reached expected count of {expected_count} resources")
                        break
                    
                    page_count += 1
                    break
                    
                elif response.status_code == 429:
                    wait_time = min(2 ** retry, 30)
                    logger.warning(f"Rate limited (429). Waiting {wait_time}s before retry {retry+1}/{max_retries}.")
                    time.sleep(wait_time)
                    if retry == max_retries:
                        logger.error("Max retries reached for rate limiting.")
                        break
                        
                else:
                    logger.error(f"API error: {response.status_code} - {response.text}")
                    if retry < max_retries:
                        wait_time = min(2 ** retry, 30)
                        logger.info(f"Retrying in {wait_time}s... (Attempt {retry+1}/{max_retries})")
                        time.sleep(wait_time)
                    else:
                        logger.error(f"Failed after {max_retries} retries")
                        break
                        
            except Exception as e:
                logger.error(f"Request failed: {str(e)}")
                if retry < max_retries:
                    wait_time = min(2 ** retry, 30)
                    logger.info(f"Retrying in {wait_time}s... (Attempt {retry+1}/{max_retries})")
                    time.sleep(wait_time)
                else:
                    logger.error(f"Failed after {max_retries} retries")
                    break
        
        # Exit if no more pages or we've reached the limit
        if not next_record_key:
            break
    
    total_time = (datetime.now() - start_time).total_seconds()
    debug_logger.info("=== Fetch Resources Summary ===")
    debug_logger.info(f"Total execution time: {total_time:.1f} seconds")
    debug_logger.info(f"Pages processed: {page_count}")
    debug_logger.info(f"Total resources: {len(all_resources)}")
    debug_logger.info(f"Unique resource IDs: {len(unique_resource_ids)}")
    debug_logger.info(f"Expected resources: {expected_count if expected_count is not None else 'Unknown'}")
    debug_logger.info(f"Origin distribution: {json.dumps(origin_counts, indent=2)}")
    
    logger.info(f"Fetch complete: {len(all_resources)} resources in {page_count} pages, {total_time:.1f} seconds")
    
    return all_resources, origin_counts

def main():
    # Fetch all resources and analyze origins
    resources, origin_counts = fetch_resources()
    
    # Print summary with total count
    print(f"\nTotal Resources Found: {len(resources)}")
    print("\nOrigin Value Distribution:")
    print("-" * 30)
    for origin, count in origin_counts.items():
        percentage = (count / len(resources)) * 100
        print(f"{origin}: {count} ({percentage:.1f}%)")
    
    # Print sample resources
    print("\nSample Resources (first 5):")
    print("-" * 30)
    for resource in resources[:5]:
        print(f"Resource ID: {resource['resourceId']}")
        print(f"Account: {resource['awsAccountId']}")
        print(f"Region: {resource['awsRegion']}")
        print(f"Origin: {resource['extracted_origin']}")
        print("-" * 30)

if __name__ == "__main__":
    main()
