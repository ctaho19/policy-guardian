import requests
import json
from pyspark.sql import functions as F
from pyspark.sql.types import StructType, StructField, StringType, LongType, DoubleType, DateType, Row
import datetime

# --- Configuration ---

# IMPORTANT: Use Databricks Secrets to store the API token
# Example: token = dbutils.secrets.get(scope="your-scope", key="cloudradar-api-token")
# Replace placeholder below with your secret retrieval method
# DO NOT HARDCODE THE TOKEN
try:
    # Replace "your-scope" and "cloudradar-api-token" with your actual secret scope and key
    API_TOKEN = dbutils.secrets.get(scope="your-scope", key="cloudradar-api-token")
    print("Successfully retrieved API Token from Databricks Secrets.")
except Exception as e:
    print("ERROR: Failed to retrieve API Token from Databricks Secrets.")
    print("Please ensure the secret scope and key are configured correctly.")
    # You might want to dbutils.notebook.exit("Failed to get API token") here
    # For demonstration purposes, using a placeholder. THIS IS NOT SECURE FOR PRODUCTION.
    API_TOKEN = "YOUR_BEARER_TOKEN_PLACEHOLDER" # <<<--- REPLACE THIS OR REMOVE IF USING SECRETS
    if API_TOKEN == "YOUR_BEARER_TOKEN_PLACEHOLDER":
       print("WARNING: Using a placeholder API token. Replace with secret retrieval for security.")
       # Consider adding dbutils.notebook.exit("Placeholder token used. Exiting for security.")

SNOWFLAKE_SOURCE_NAME = "net.snowflake.spark.snowflake"
# Define your sfOptions dictionary with Snowflake connection details
# Ensure user/password or other auth methods are secure (e.g., keypair, OAuth)
sfOptions = {
  "sfUrl": "YOUR_ACCOUNT.snowflakecomputing.com",
  "sfUser": "YOUR_USER",
  "sfPassword": "YOUR_PASSWORD", # Or use other auth like sfRole, sfWarehouse, sfDatabase, sfSchema
  "sfDatabase": "CYBR_DB",
  "sfSchema": "PHDP_CYBR", # Default schema for reads
  "sfWarehouse": "YOUR_WAREHOUSE"
}

# CloudRadar API Details
API_URL = "https://api.cloud.capitalone.com/internal-operations/cloud-service/aws-tooling/search-resource-configurations"
API_HEADERS = {
    'Accept': 'application/json;v=1.0',
    'Authorization': f'Bearer {API_TOKEN}',
    'Content-Type': 'application/json'
}
API_LIMIT = 10000 # Match the limit in your payload

# Define schema matching the target table's structure for the final output
# (DATE, CTRL_ID, MONITORING_METRIC_NUMBER, MONITORING_METRIC, COMPLIANCE_STATUS, NUMERATOR, DENOMINATOR)
output_schema = StructType([
    StructField("DATE", DateType(), True),
    StructField("CTRL_ID", StringType(), True),
    StructField("MONITORING_METRIC_NUMBER", StringType(), True),
    StructField("MONITORING_METRIC", DoubleType(), True), # Using Double for percentage
    StructField("COMPLIANCE_STATUS", StringType(), True),
    StructField("NUMERATOR", DoubleType(), True),         # Using Double for counts
    StructField("DENOMINATOR", DoubleType(), True)        # Using Double for counts
])

# --- Helper Function to Fetch CloudRadar ARNs with Pagination ---

def fetch_cloudradar_arns(url, headers, limit):
    """Fetches all AWS::ACM::Certificate ARNs issued by Amazon from CloudRadar, handling pagination."""
    all_arns = set()
    next_key = None
    page_count = 0

    while True:
        page_count += 1
        print(f"Fetching page {page_count} from CloudRadar API...")
        payload = {
            "responseFields": ["amazonResourceName"],
            "searchParameters": [{
                "resourceType": "AWS::ACM::Certificate",
                "configurationList": [{
                    "configurationName": "ConfigurationObject.issuer",
                    "configurationValue": "Amazon"
                }]
            }],
            "limit": limit
        }
        if next_key:
            payload["nextRecordKey"] = next_key

        try:
            response = requests.post(url, headers=headers, data=json.dumps(payload), timeout=120) # Increased timeout
            response.raise_for_status()  # Raise an exception for bad status codes (4xx or 5xx)
            data = response.json()

            resources = data.get("resourceConfigurations", [])
            if resources: # Check if the list exists and is not empty
                 for config in resources:
                    if "amazonResourceName" in config:
                        all_arns.add(config["amazonResourceName"])

            fetched_count = len(resources)
            next_key = data.get("nextRecordKey")
            print(f"Page {page_count} fetched. Found {fetched_count} items. Next key exists: {bool(next_key)}")

            if not next_key:
                break # Exit loop if no more pages

        except requests.exceptions.Timeout:
            print(f"ERROR: CloudRadar API request timed out on page {page_count}.")
            print("Returning ARNs fetched so far (might be incomplete).")
            return list(all_arns), False # Indicate failure due to timeout

        except requests.exceptions.RequestException as e:
            print(f"ERROR: CloudRadar API request failed: {e}")
            # Log response text if available and helpful
            if hasattr(e, 'response') and e.response is not None:
                print(f"Response Status Code: {e.response.status_code}")
                print(f"Response Text: {e.response.text[:500]}...") # Log part of the response
            print("Returning ARNs fetched so far (might be incomplete).")
            return list(all_arns), False # Indicate failure

        except json.JSONDecodeError as e:
            print(f"ERROR: Failed to decode JSON response from CloudRadar API: {e}")
            print(f"Response Text: {response.text[:500]}...") # Log part of the response
            return list(all_arns), False # Indicate failure

    print(f"Finished fetching from CloudRadar. Total distinct ARNs found: {len(all_arns)}")
    return list(all_arns), True # Return True to indicate success

# --- Tier 1 Calculation Steps ---

# 1. Fetch Authoritative List from CloudRadar
print("--- Starting Tier 1 Calculation ---")
cloudradar_arns_list, fetch_success = fetch_cloudradar_arns(API_URL, API_HEADERS, API_LIMIT)

if not fetch_success:
     print("ERROR: Failed to fetch complete data from CloudRadar API. Aborting Tier 1 calculation.")
     dbutils.notebook.exit("CloudRadar API fetch failed.") # Exit notebook execution

# 2. Create CloudRadar ARN DataFrame
cloudradar_schema = StructType([StructField("cloudradar_arn", StringType(), True)]) # Renamed column for clarity
if cloudradar_arns_list:
    # Filter out any potential None values just in case
    valid_arns = [arn for arn in cloudradar_arns_list if arn is not None]
    cloudradar_df = spark.createDataFrame([(arn,) for arn in valid_arns], schema=cloudradar_schema).distinct()
else:
    # Create an empty DataFrame with the correct schema if the list is empty
    cloudradar_df = spark.createDataFrame([], schema=cloudradar_schema)

cloudradar_df.cache() # Cache as it might be used for supporting evidence later
cloudradar_count = cloudradar_df.count()
print(f"Total distinct ARNs from CloudRadar (Denominator): {cloudradar_count}")

# 3. Get Dataset ARNs from Snowflake
print("Fetching distinct ACM ARNs from Certificate Catalog table (Snowflake)...")
dataset_arns_query = """
SELECT DISTINCT CERTIFICATE_ARN
FROM CYBR_DB.PHDP_CYBR.CERTIFICATE_CATALOG_CERTIFICATE_USAGE
WHERE CERTIFICATE_ARN LIKE 'arn:aws:acm:%' -- More specific than just %arn:aws:acm%
  AND CERTIFICATE_ARN IS NOT NULL
"""

try:
    dataset_arns_df = spark.read.format(SNOWFLAKE_SOURCE_NAME) \
        .options(**sfOptions) \
        .option("query", dataset_arns_query) \
        .load()
    dataset_arns_df = dataset_arns_df.withColumnRenamed("CERTIFICATE_ARN", "dataset_arn") # Rename for join clarity
    dataset_arns_df.cache() # Cache for the join and potentially supporting evidence
    dataset_count = dataset_arns_df.count()
    print(f"Distinct ACM ARNs found in dataset: {dataset_count}")
except Exception as e:
    print(f"ERROR: Failed to fetch ARNs from Snowflake table: {e}")
    cloudradar_df.unpersist() # Clean up cache
    dbutils.notebook.exit("Failed to query dataset ARNs.")

# 4. Calculate Tier 1 Metric Values
print("Calculating Tier 1 metric values...")

if cloudradar_count > 0:
    # Find ARNs from CloudRadar that ARE present in the dataset (inner join)
    matched_arns_df = cloudradar_df.join(
        dataset_arns_df,
        cloudradar_df["cloudradar_arn"] == dataset_arns_df["dataset_arn"],
        "inner"
    )
    numerator_t1 = matched_arns_df.count()
    denominator_t1 = float(cloudradar_count) # Ensure float for division

    # Handle potential division by zero, although checked cloudradar_count > 0
    metric_t1 = round((numerator_t1 * 100.0 / denominator_t1), 2) if denominator_t1 > 0 else 0.0
    # Status check: strict equality needed for 100%
    status_t1 = "GREEN" if numerator_t1 == int(denominator_t1) else "RED"

else:
    # Handle case where CloudRadar returned no ARNs
    print("CloudRadar reported 0 in-scope certificates. Setting Tier 1 to 100% GREEN.")
    numerator_t1 = 0
    denominator_t1 = 0.0
    metric_t1 = 100.0 # Defined as 100% if population is zero
    status_t1 = "GREEN"

# 5. Create Tier 1 Result DataFrame
print("Creating Tier 1 output DataFrame...")
current_date = datetime.date.today()

tier1_data = [(
    current_date,
    'CTRL-1077188',
    'MNTR-1077188-T1',
    float(metric_t1),
    status_t1,
    float(numerator_t1),
    float(denominator_t1)
)]

# Create the DataFrame using the predefined schema
df_tier1 = spark.createDataFrame(tier1_data, schema=output_schema)

# --- Display Tier 1 Result ---
print("\n--- Tier 1 Metric Result ---")
df_tier1.show(truncate=False)

print(f"\nCalculation Summary:")
print(f"  CloudRadar ARNs (Denominator): {int(denominator_t1)}")
print(f"  Matched ARNs in Dataset (Numerator): {int(numerator_t1)}")
print(f"  Metric Value: {metric_t1}%")
print(f"  Compliance Status: {status_t1}")

# Keep cached dataframes if needed for Part 2, otherwise unpersist here
# dataset_arns_df.unpersist()
# cloudradar_df.unpersist()

print("\n--- Tier 1 Code Execution Finished ---")
