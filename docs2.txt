Presentation Script for API Pipeline Walkthrough
Opening (1 minute):

"Okay, team, today I’m walking you through our new API pipeline that monitors compliance for AWS KMS keys. The goal here is to ensure our encryption keys meet two levels of standards—Tier 1, where keys have a valid origin, and Tier 2, where that origin is specifically AWS KMS. This pipeline pulls data from an API, processes it, checks compliance, and generates a report. It’s like a quality control system for our keys—think of it as a factory inspector checking products on a conveyor belt. Let’s dive into how it works, step by step."

1. Setup Logging (1 minute):

"First up, we’ve got this block called setup_logging. This is our pipeline’s journal-keeper. It sets up a logging system that tracks everything the pipeline does—every API call, every success, every hiccup. Imagine it’s like a flight recorder on a plane, giving us a detailed log we can review later if something goes wrong. It writes these logs to the console with timestamps, so we always know what’s happening and when. This helps us troubleshoot or confirm it’s running smoothly."

2. Get Summary Count (2 minutes):

"Next, we have get_summary_count. This function is like our scout—it goes out to the API and gets a quick headcount of how many resources, specifically AWS KMS keys, we’re dealing with. It sends a request with a payload—a set of search parameters—and the API responds with a total number. What’s cool here is it’s built to handle hiccups: if the API is slow or says ‘too many requests,’ it waits and retries a couple of times with a clever delay system called exponential backoff. Think of it as patiently knocking on a busy neighbor’s door until they answer. This count becomes our baseline for later calculations."

3. Fetch All Resources (2 minutes):

"Then we’ve got fetch_all_resources. This is the heavy lifter—it grabs all the detailed data about those keys we counted earlier. Since there could be thousands of keys, it uses pagination, like flipping through pages in a book, fetching up to 10,000 records at a time until it’s got everything. It’s smart, too—if the API fails, it retries with timeouts and logs every step. Picture it as a librarian systematically pulling every book off the shelf, checking each one, and stacking them up for us to review. This gives us the raw data we’ll filter next."

4. Filter Tier 1 Resources (2 minutes):

"Now, filter_tier1_resources is where we start checking compliance. For Tier 1, we want every key to have a non-empty origin—like making sure every product has a ‘Made in’ label. This function loops through all the resources we fetched, looks at their configuration data, and counts how many pass this basic test. It also builds a report of the ones that fail, using a table format with Pandas. It’s like sorting a pile of packages and flagging any without a return address—those are our non-compliant ones."

5. Filter Tier 2 Resources (2 minutes):

"Next, filter_tier2_resources takes it up a notch for Tier 2 compliance. Here, we’re stricter—we only want keys where the origin is exactly ‘AWS_KMS.’ It’s similar to Tier 1: it loops through, checks the origin field, and counts the matches, while also flagging the mismatches in a report. Think of it as double-checking that not only do the packages have a return address, but it’s specifically from our trusted supplier. This gives us our second compliance metric."

6. Load Thresholds (1.5 minutes):

"Then we have load_thresholds. This pulls in our compliance benchmarks from a Snowflake database—like getting the rulebook for what ‘good’ looks like. It grabs alert and warning thresholds for both tiers, say 95% for Tier 1 and 99% for Tier 2. If the database is down, it falls back to default values, so we’re never stuck. Imagine it as checking the factory’s quality standards before grading our products—it ensures we’re measuring against the right targets."

7. Get Compliance Status (1.5 minutes):

"After that, get_compliance_status decides if we’re in the green, yellow, or red zone. It takes our compliance percentages—like 92% of keys meeting Tier 1—and compares them to those thresholds. If we’re above the alert threshold, it’s green; between warning and alert, it’s yellow; below warning, it’s red. It’s like a traffic light system telling us if our key compliance is safe, shaky, or needs urgent attention."

8. Main Function (3 minutes):

"Finally, the main function ties it all together—it’s the conductor of this orchestra. Here’s how it flows:

First, it sets up a Spark session for database access and defines our config, like targeting AWS KMS keys and setting timeouts.
Step 1: It calls get_summary_count twice—once for total keys, once for keys already matching AWS KMS—to get our starting numbers.
Step 2: It does a test run with fetch_all_resources on a small sample, validating the data and filtering logic with both tier functions.
Step 3: Once validated, it fetches all resources.
Step 4: It applies both filters to the full dataset.
Step 5: It loads thresholds and calculates our compliance status.
Steps 6 and 7: It builds a final report with metrics and lists of non-compliant keys.
Think of main as the project manager—it runs the show, step by step, ensuring every piece works together to deliver our compliance picture."
